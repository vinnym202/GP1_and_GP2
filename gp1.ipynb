{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3rwy7V72-8YY"
   },
   "source": [
    "# GP1 Trading Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the API Keys Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Z6qlLXY-fA2"
   },
   "outputs": [],
   "source": [
    "from config_private import ALPACA_API_KEY, ALPACA_API_SECRET\n",
    "API_BASE_URL = 'https://paper-api.alpaca.markets'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "--6Kx8I21erH"
   },
   "source": [
    "## Part 1: Imports and Class/Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7I7zsyYfoLJ",
    "outputId": "1812c13b-410f-434c-eb07-29782ba186e6"
   },
   "outputs": [],
   "source": [
    "from config_tickers import GP1_TICKER\n",
    "from config import INDICATORS\n",
    "from config import CDL\n",
    "from data_processor import DataProcessor\n",
    "from plot import backtest_stats, backtest_plot, get_baseline, drop_dup_dates\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from numpy import random as rd\n",
    "\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        gamma=0.985,\n",
    "        turbulence_thresh=99,\n",
    "        max_stock=None,\n",
    "        min_stock_rate=0.2,\n",
    "        initial_capital=1e5,\n",
    "        reward_scaling=2**-11,\n",
    "        sell_cost_pct=1e-2,\n",
    "        initial_stocks=None,\n",
    "    ):\n",
    "        price_ary = config[\"price_array\"]\n",
    "        tech_ary = config[\"tech_array\"]\n",
    "        turbulence_ary = config[\"turbulence_array\"]\n",
    "        date_ary = config[\"date_array\"]\n",
    "        if_train = config[\"if_train\"]\n",
    "        self.price_ary = price_ary.astype(np.float32)\n",
    "        self.tech_ary = tech_ary.astype(np.float32)\n",
    "        self.turbulence_ary = turbulence_ary\n",
    "        self.date_ary = date_ary\n",
    "\n",
    "        self.tech_ary = self.tech_ary * 2**-7\n",
    "        self.turbulence_bool = (turbulence_ary > turbulence_thresh).astype(np.float32)\n",
    "        self.turbulence_ary = (\n",
    "            self.sigmoid_sign(turbulence_ary, turbulence_thresh) * 2**-5\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        stock_dim = self.price_ary.shape[1]\n",
    "        self.gamma = gamma\n",
    "        self.max_stock = max_stock\n",
    "        self.min_stock_rate = min_stock_rate\n",
    "        self.sell_cost_pct = sell_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.initial_capital = initial_capital\n",
    "        self.initial_stocks = (\n",
    "            np.zeros(stock_dim, dtype=np.float32)\n",
    "            if initial_stocks is None\n",
    "            else initial_stocks\n",
    "        )\n",
    "        \n",
    "        # reset()\n",
    "        self.minute = None\n",
    "        self.num_trades = None\n",
    "        self.cash = None\n",
    "        self.stocks = None\n",
    "        self.total_assets = None\n",
    "        self.gamma_reward = None\n",
    "        self.initial_total_assets = None\n",
    "\n",
    "        # environment information\n",
    "        self.env_name = \"StockEnv\"\n",
    "        self.state_dim = 114 # Size of get_state() array \n",
    "        self.action_dim = stock_dim\n",
    "        self.max_step = self.price_ary.shape[0] - 1\n",
    "        self.if_train = if_train\n",
    "        self.if_discrete = False\n",
    "        self.episode_return = 0.0\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.state_dim,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=-1, high=1, shape=(self.action_dim,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.minute = 0\n",
    "        self.num_trades = 0\n",
    "        price = self.price_ary[self.minute]\n",
    "        \n",
    "        if self.if_train:\n",
    "            self.stocks = (\n",
    "                self.initial_stocks + rd.randint(0, 17, size=self.initial_stocks.shape)\n",
    "            ).astype(np.float32)\n",
    "            self.stocks_cool_down = np.zeros_like(self.stocks)\n",
    "            self.cash = (\n",
    "                self.initial_capital * rd.uniform(0.95, 1.05)\n",
    "                - (self.stocks * price).sum()\n",
    "            )\n",
    "        else:\n",
    "            self.stocks = self.initial_stocks.astype(np.float32)\n",
    "            self.stocks_cool_down = np.zeros_like(self.stocks)\n",
    "            self.cash = self.initial_capital\n",
    "        \n",
    "        self.total_assets = self.cash + (self.stocks * price).sum()\n",
    "        self.initial_total_assets = self.total_assets\n",
    "        self.gamma_reward = 0.0\n",
    "        observation = self.get_state(price)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.minute += 1\n",
    "        price = self.price_ary[self.minute]\n",
    "        self.max_stock = np.round(np.floor(50_500 / price)).astype(int)\n",
    "        action = np.round((action * self.max_stock)).astype(int)\n",
    "        min_action = np.round((self.max_stock * self.min_stock_rate)).astype(int)\n",
    "        self.stocks_cool_down += 1\n",
    "        penalty = 0\n",
    "        \n",
    "        if self.turbulence_bool[self.minute] == 0:\n",
    "            \n",
    "            # Sell Logic\n",
    "            for index in np.where((action < -min_action) & (self.stocks_cool_down > 4))[0]:\n",
    "                if price[index] > 0:\n",
    "                    sell_num_shares = min(self.stocks[index], -action[index])\n",
    "                    sell_value = price[index] * sell_num_shares\n",
    "                    self.stocks[index] -= sell_num_shares\n",
    "                    self.cash += sell_value\n",
    "                    penalty += sell_value * self.sell_cost_pct\n",
    "                    self.stocks_cool_down[index] = 0\n",
    "                    self.num_trades += 1\n",
    "\n",
    "            # Buy Logic\n",
    "            for index in np.where((action > min_action) & (self.stocks_cool_down > 4))[0]:\n",
    "                if price[index] > 0:\n",
    "                    buy_num_shares = min(self.cash // price[index], action[index])\n",
    "                    buy_value = price[index] * buy_num_shares\n",
    "                    self.stocks[index] += buy_num_shares\n",
    "                    self.cash -= buy_value\n",
    "                    self.stocks_cool_down[index] = 0\n",
    "                    self.num_trades += 1\n",
    "\n",
    "        # turbulence logic\n",
    "        else:\n",
    "            self.cash += (self.stocks * price).sum()\n",
    "            self.num_trades += np.count_nonzero(self.stocks)\n",
    "            self.stocks[:] = 0\n",
    "            self.stocks_cool_down[:] = 0\n",
    "        \n",
    "        # Reward Calculations\n",
    "        observation = self.get_state(price)\n",
    "        total_assets = self.cash + (self.stocks * price).sum()\n",
    "        reward = ((total_assets - penalty) - self.total_assets) * self.reward_scaling\n",
    "        self.total_assets = total_assets\n",
    "        self.gamma_reward = self.gamma_reward * self.gamma + reward\n",
    "        terminated = self.minute == self.max_step\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        if terminated:\n",
    "            reward = self.gamma_reward\n",
    "            self.episode_return = total_assets / self.initial_total_assets\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def get_state(self, price):\n",
    "        cash = np.array(self.cash * (2**-12), dtype=np.float32)\n",
    "        scale = np.array(2**-6, dtype=np.float32)\n",
    "        observation = np.hstack(\n",
    "            (\n",
    "                cash,\n",
    "                self.turbulence_ary[self.minute],\n",
    "                self.turbulence_bool[self.minute],\n",
    "                price * scale,\n",
    "                self.stocks * scale,\n",
    "                self.stocks_cool_down,\n",
    "                self.tech_ary[self.minute],\n",
    "            )\n",
    "        )\n",
    "        # print(len(observation))\n",
    "        return observation\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_sign(ary, thresh):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x * np.e)) - 0.5\n",
    "\n",
    "        return sigmoid(ary / thresh) * thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EVJIQUR6_fu"
   },
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EYx40S84tzo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from torch import Tensor\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "class ActorPPO(nn.Module):\n",
    "    def __init__(self, dims: [int], state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, action_dim])\n",
    "        layer_init_with_orthogonal(self.net[-1], std=0.1)\n",
    "        self.action_std_log = nn.Parameter(torch.zeros((1, action_dim)), requires_grad=True)  # trainable parameter\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        return self.net(state).tanh()  # action.tanh()\n",
    "\n",
    "    def get_action(self, state: Tensor) -> (Tensor, Tensor):  # for exploration\n",
    "        action_avg = self.net(state)\n",
    "        action_std = self.action_std_log.exp()\n",
    "\n",
    "        dist = Normal(action_avg, action_std)\n",
    "        action = dist.sample()\n",
    "        logprob = dist.log_prob(action).sum(1)\n",
    "        return action, logprob\n",
    "\n",
    "    def get_logprob_entropy(self, state: Tensor, action: Tensor) -> (Tensor, Tensor):\n",
    "        action_avg = self.net(state)\n",
    "        action_std = self.action_std_log.exp()\n",
    "\n",
    "        dist = Normal(action_avg, action_std)\n",
    "        logprob = dist.log_prob(action).sum(1)\n",
    "        entropy = dist.entropy().sum(1)\n",
    "        return logprob, entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_action_for_env(action: Tensor) -> Tensor:\n",
    "        return action.tanh()\n",
    "\n",
    "\n",
    "class CriticPPO(nn.Module):\n",
    "    def __init__(self, dims: [int], state_dim: int, _action_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(dims=[state_dim, *dims, 1])\n",
    "        layer_init_with_orthogonal(self.net[-1], std=0.5)\n",
    "\n",
    "        self.state_avg = nn.Parameter(torch.zeros((state_dim,)), requires_grad=False)\n",
    "        self.state_std = nn.Parameter(torch.ones((state_dim,)), requires_grad=False)\n",
    "        self.value_avg = nn.Parameter(torch.zeros((1,)), requires_grad=False)\n",
    "        self.value_std = nn.Parameter(torch.ones((1,)), requires_grad=False)\n",
    "\n",
    "    def state_norm(self, state: Tensor) -> Tensor:\n",
    "        return (state - self.state_avg) / self.state_std\n",
    "\n",
    "    def value_re_norm(self, value: Tensor) -> Tensor:\n",
    "        return value * self.value_std + self.value_avg\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        state = self.state_norm(state)\n",
    "        value = self.net(state)\n",
    "        value = self.value_re_norm(value)\n",
    "        return value  # state value used to calculate advantage value\n",
    "\n",
    "\n",
    "def build_mlp(dims: [int]) -> nn.Sequential:  # MLP (MultiLayer Perceptron)\n",
    "    net_list = []\n",
    "    for i in range(len(dims) - 1):\n",
    "        net_list.extend([nn.Linear(dims[i], dims[i + 1]), nn.ReLU()])\n",
    "    del net_list[-1]  # remove the activation of output layer\n",
    "    return nn.Sequential(*net_list)\n",
    "\n",
    "def layer_init_with_orthogonal(layer, std=1.0, bias_const=1e-6):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, agent_class=None, env_class=None, env_args=None):\n",
    "        self.env_class = env_class  # env = env_class(**env_args)\n",
    "        self.env_args = env_args  # env = env_class(**env_args)\n",
    "\n",
    "        if env_args is None:  # dummy env_args\n",
    "            env_args = {'env_name': None, 'state_dim': None, 'action_dim': None, 'if_discrete': None}\n",
    "        self.env_name = env_args['env_name']  # the name of environment. Be used to set 'cwd'.\n",
    "        self.state_dim = env_args['state_dim']  # vector dimension (feature number) of state\n",
    "        self.action_dim = env_args['action_dim']  # vector dimension (feature number) of action\n",
    "        self.if_discrete = env_args['if_discrete']  # discrete or continuous action space\n",
    "\n",
    "        self.agent_class = agent_class  # agent = agent_class(...)\n",
    "\n",
    "        '''Arguments for reward shaping'''\n",
    "        self.gamma = 0.99  # discount factor of future rewards\n",
    "        self.reward_scale = 2 ** 0  # an approximate target reward usually be closed to 256\n",
    "\n",
    "        '''Arguments for training'''\n",
    "        self.gpu_id = int(0)  # `int` means the ID of single GPU, -1 means CPU\n",
    "        self.net_dims = (128, 64)  # the middle layer dimension of MLP (MultiLayer Perceptron)\n",
    "        self.learning_rate = 6e-5  # 2 ** -14 ~= 6e-5\n",
    "        self.weight_decay = 1e-2  # Default value for weight_decay\n",
    "        self.lambda_entropy = 1e-2  # Default value for lambda_entropy\n",
    "        self.soft_update_tau = 5e-3  # 2 ** -8 ~= 5e-3\n",
    "        self.batch_size = int(512)  # num of transitions sampled from replay buffer.\n",
    "        self.horizon_len = int(2000)  # collect horizon_len step while exploring, then update network\n",
    "        self.buffer_size = None  # ReplayBuffer size. Empty the ReplayBuffer for on-policy.\n",
    "        self.repeat_times = 8.0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
    "\n",
    "        '''Arguments for evaluate'''\n",
    "        self.cwd = None  # current working directory to save model. None means set automatically\n",
    "        self.break_step = +np.inf  # break training if 'total_step > break_step'\n",
    "        self.eval_times = int(3)  # number of times that get episodic cumulative return\n",
    "        self.eval_per_step = int(2e4)  # evaluate the agent per training steps\n",
    "\n",
    "    def init_before_training(self):\n",
    "        if self.cwd is None:  # set cwd (current working directory) for saving model\n",
    "            self.cwd = f'./{self.env_name}_{self.agent_class.__name__[5:]}'\n",
    "        os.makedirs(self.cwd, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_gym_env_args(env, if_print: bool) -> dict:\n",
    "    if {'unwrapped', 'observation_space', 'action_space', 'spec'}.issubset(dir(env)):  # isinstance(env, gym.Env):\n",
    "        env_name = env.unwrapped.spec.id\n",
    "        state_shape = env.observation_space.shape\n",
    "        state_dim = state_shape[0] if len(state_shape) == 1 else state_shape  # sometimes state_dim is a list\n",
    "\n",
    "        if_discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "        if if_discrete:  # make sure it is discrete action space\n",
    "            action_dim = env.action_space.n\n",
    "        elif isinstance(env.action_space, gym.spaces.Box):  # make sure it is continuous action space\n",
    "            action_dim = env.action_space.shape[0]\n",
    "\n",
    "    env_args = {'env_name': env_name, 'state_dim': state_dim, 'action_dim': action_dim, 'if_discrete': if_discrete}\n",
    "    print(f\"env_args = {repr(env_args)}\") if if_print else None\n",
    "    return env_args\n",
    "\n",
    "\n",
    "def kwargs_filter(function, kwargs: dict) -> dict:\n",
    "    import inspect\n",
    "    sign = inspect.signature(function).parameters.values()\n",
    "    sign = {val.name for val in sign}\n",
    "    common_args = sign.intersection(kwargs.keys())\n",
    "    return {key: kwargs[key] for key in common_args}  # filtered kwargs\n",
    "\n",
    "\n",
    "def build_env(env_class=None, env_args=None):\n",
    "    if env_class.__module__ == 'gym.envs.registration':  # special rule\n",
    "        env = env_class(id=env_args['env_name'])\n",
    "    else:\n",
    "        env = env_class(**kwargs_filter(env_class.__init__, env_args.copy()))\n",
    "    for attr_str in ('env_name', 'state_dim', 'action_dim', 'if_discrete'):\n",
    "        setattr(env, attr_str, env_args[attr_str])\n",
    "    return env\n",
    "\n",
    "\n",
    "class AgentBase:\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.gamma = args.gamma\n",
    "        self.batch_size = args.batch_size\n",
    "        self.repeat_times = args.repeat_times\n",
    "        self.reward_scale = args.reward_scale\n",
    "        self.soft_update_tau = args.soft_update_tau\n",
    "\n",
    "        self.states = None  # assert self.states == (1, state_dim)\n",
    "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
    "\n",
    "        act_class = getattr(self, \"act_class\", None)\n",
    "        cri_class = getattr(self, \"cri_class\", None)\n",
    "        self.act = self.act_target = act_class(net_dims, state_dim, action_dim).to(self.device)\n",
    "        self.cri = self.cri_target = cri_class(net_dims, state_dim, action_dim).to(self.device) \\\n",
    "            if cri_class else self.act\n",
    "\n",
    "        self.act_optimizer = torch.optim.RAdam(self.act.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "        self.cri_optimizer = torch.optim.RAdam(self.cri.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay) \\\n",
    "            if cri_class else self.act_optimizer\n",
    "\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    @staticmethod\n",
    "    def optimizer_update(optimizer, objective: Tensor):\n",
    "        optimizer.zero_grad()\n",
    "        objective.backward()\n",
    "        clip_grad_norm_(parameters=optimizer.param_groups[0][\"params\"], max_norm=3.0)  # Add gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(target_net: torch.nn.Module, current_net: torch.nn.Module, tau: float):\n",
    "        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n",
    "            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))\n",
    "\n",
    "\n",
    "class AgentPPO(AgentBase):\n",
    "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
    "        self.if_off_policy = False\n",
    "        self.act_class = getattr(self, \"act_class\", ActorPPO)\n",
    "        self.cri_class = getattr(self, \"cri_class\", CriticPPO)\n",
    "        AgentBase.__init__(self, net_dims, state_dim, action_dim, gpu_id, args)\n",
    "\n",
    "        self.ratio_clip = getattr(args, \"ratio_clip\", 0.25)  # `ratio.clamp(1 - clip, 1 + clip)`\n",
    "        self.lambda_gae_adv = getattr(args, \"lambda_gae_adv\", 0.95)  # could be 0.80~0.99\n",
    "        self.lambda_entropy = getattr(args, \"lambda_entropy\", 1e-2)  # could be 0.00~0.10\n",
    "        self.lambda_entropy = torch.tensor(self.lambda_entropy, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def explore_env(self, env, horizon_len: int) -> [Tensor]:\n",
    "        states = torch.zeros((horizon_len, self.state_dim), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.zeros((horizon_len, self.action_dim), dtype=torch.float32).to(self.device)\n",
    "        logprobs = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.zeros(horizon_len, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        ary_state = self.states[0]\n",
    "\n",
    "        get_action = self.act.get_action\n",
    "        convert = self.act.convert_action_for_env\n",
    "        for i in range(horizon_len):\n",
    "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device)\n",
    "            action, logprob = [t.squeeze(0) for t in get_action(state.unsqueeze(0))[:2]]\n",
    "\n",
    "            ary_action = convert(action).detach().cpu().numpy()\n",
    "            ary_state, reward, done, _, _ = env.step(ary_action)\n",
    "            if done:\n",
    "                ary_state, _ = env.reset()\n",
    "\n",
    "            states[i] = state\n",
    "            actions[i] = action\n",
    "            logprobs[i] = logprob\n",
    "            rewards[i] = reward\n",
    "            dones[i] = done\n",
    "\n",
    "        self.states[0] = ary_state\n",
    "        rewards = (rewards * self.reward_scale).unsqueeze(1)\n",
    "        undones = (1 - dones.type(torch.float32)).unsqueeze(1)\n",
    "        return states, actions, logprobs, rewards, undones\n",
    "\n",
    "    def update_net(self, buffer) -> [float]:\n",
    "        with torch.no_grad():\n",
    "            states, actions, logprobs, rewards, undones = buffer\n",
    "            buffer_size = states.shape[0]\n",
    "\n",
    "            '''get advantages reward_sums'''\n",
    "            bs = 2 ** 10  # set a smaller 'batch_size' when out of GPU memory.\n",
    "            values = [self.cri(states[i:i + bs]) for i in range(0, buffer_size, bs)]\n",
    "            values = torch.cat(values, dim=0).squeeze(1)  # values.shape == (buffer_size, )\n",
    "\n",
    "            advantages = self.get_advantages(rewards, undones, values)  # advantages.shape == (buffer_size, )\n",
    "            reward_sums = advantages + values  # reward_sums.shape == (buffer_size, )\n",
    "            del rewards, undones, values\n",
    "\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std(dim=0) + 1e-5)\n",
    "        assert logprobs.shape == advantages.shape == reward_sums.shape == (buffer_size,)\n",
    "\n",
    "        '''update network'''\n",
    "        obj_critics = 0.0\n",
    "        obj_actors = 0.0\n",
    "\n",
    "        update_times = int(buffer_size * self.repeat_times / self.batch_size)\n",
    "        assert update_times >= 1\n",
    "        for _ in range(update_times):\n",
    "            indices = torch.randint(buffer_size, size=(self.batch_size,), requires_grad=False)\n",
    "            state = states[indices]\n",
    "            action = actions[indices]\n",
    "            logprob = logprobs[indices]\n",
    "            advantage = advantages[indices]\n",
    "            reward_sum = reward_sums[indices]\n",
    "\n",
    "            value = self.cri(state).squeeze(1)  # critic network predicts the reward_sum (Q value) of state\n",
    "            obj_critic = self.criterion(value, reward_sum)\n",
    "            self.optimizer_update(self.cri_optimizer, obj_critic)\n",
    "\n",
    "            new_logprob, obj_entropy = self.act.get_logprob_entropy(state, action)\n",
    "            ratio = (new_logprob - logprob.detach()).exp()\n",
    "            surrogate1 = advantage * ratio\n",
    "            surrogate2 = advantage * ratio.clamp(1 - self.ratio_clip, 1 + self.ratio_clip)\n",
    "            obj_surrogate = torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            obj_actor = obj_surrogate + obj_entropy.mean() * self.lambda_entropy\n",
    "            self.optimizer_update(self.act_optimizer, -obj_actor)\n",
    "\n",
    "            obj_critics += obj_critic.item()\n",
    "            obj_actors += obj_actor.item()\n",
    "        a_std_log = getattr(self.act, 'a_std_log', torch.zeros(1)).mean()\n",
    "        return obj_critics / update_times, obj_actors / update_times, a_std_log.item()\n",
    "\n",
    "    def get_advantages(self, rewards: Tensor, undones: Tensor, values: Tensor) -> Tensor:\n",
    "        advantages = torch.empty_like(values)  # advantage value\n",
    "\n",
    "        masks = undones * self.gamma\n",
    "        horizon_len = rewards.shape[0]\n",
    "\n",
    "        next_state = torch.tensor(self.states, dtype=torch.float32).to(self.device)\n",
    "        next_value = self.cri(next_state).detach()[0, 0]\n",
    "\n",
    "        advantage = 0  # last_gae_lambda\n",
    "        for t in range(horizon_len - 1, -1, -1):\n",
    "            delta = rewards[t] + masks[t] * next_value - values[t]\n",
    "            advantages[t] = advantage = delta + masks[t] * self.lambda_gae_adv * advantage\n",
    "            next_value = values[t]\n",
    "        return advantages\n",
    "\n",
    "    \n",
    "def train_agent(args: Config):\n",
    "    args.init_before_training()\n",
    "\n",
    "    env = build_env(args.env_class, args.env_args)\n",
    "    agent = args.agent_class(args.net_dims, args.state_dim, args.action_dim, gpu_id=args.gpu_id, args=args)\n",
    "    agent.states, _ = env.reset()\n",
    "    agent.states = agent.states[np.newaxis, :]\n",
    "\n",
    "    evaluator = Evaluator(eval_env=build_env(args.env_class, args.env_args),\n",
    "                          eval_per_step=args.eval_per_step,\n",
    "                          eval_times=args.eval_times,\n",
    "                          cwd=args.cwd)\n",
    "    torch.set_grad_enabled(False)\n",
    "    while True: # start training\n",
    "        buffer_items = agent.explore_env(env, args.horizon_len)\n",
    "\n",
    "        torch.set_grad_enabled(True)\n",
    "        logging_tuple = agent.update_net(buffer_items)\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        evaluator.evaluate_and_save(agent.act, args.horizon_len, logging_tuple)\n",
    "        if (evaluator.total_step > args.break_step) or os.path.exists(f\"{args.cwd}/stop\"):\n",
    "            torch.save(agent.act.state_dict(), args.cwd + '/actor.pth')\n",
    "            break  # stop training when reach `break_step` or `mkdir cwd/stop`\n",
    "\n",
    "\n",
    "def render_agent(env_class, env_args: dict, net_dims: [int], agent_class, actor_path: str, render_times: int = 8):\n",
    "    env = build_env(env_class, env_args)\n",
    "\n",
    "    state_dim = env_args['state_dim']\n",
    "    action_dim = env_args['action_dim']\n",
    "    agent = agent_class(net_dims, state_dim, action_dim, gpu_id=-1)\n",
    "    actor = agent.act\n",
    "\n",
    "    print(f\"| render and load actor from: {actor_path}\")\n",
    "    actor.load_state_dict(torch.load(actor_path, map_location=lambda storage, loc: storage))\n",
    "    for i in range(render_times):\n",
    "        cumulative_reward, episode_step = get_rewards_and_steps(env, actor, if_render=True)\n",
    "        print(f\"|{i:4}  cumulative_reward {cumulative_reward:9.3f}  episode_step {episode_step:5.0f}\")\n",
    "\n",
    "        \n",
    "class Evaluator:\n",
    "    def __init__(self, eval_env, eval_per_step: int = 1e4, eval_times: int = 8, cwd: str = '.'):\n",
    "        self.cwd = cwd\n",
    "        self.env_eval = eval_env\n",
    "        self.eval_step = 0\n",
    "        self.total_step = 0\n",
    "        self.start_time = time.time()\n",
    "        self.eval_times = eval_times  # number of times that get episodic cumulative return\n",
    "        self.eval_per_step = eval_per_step  # evaluate the agent per training steps\n",
    "\n",
    "        self.recorder = []\n",
    "        print(f\"\\n| step: Number of samples, or total training steps, or running times of `env.step()`.\"\n",
    "              f\"\\n| time: Time spent from the start of training to this moment.\"\n",
    "              f\"\\n| avgR: Average value of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              f\"\\n| stdR: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\"\n",
    "              f\"\\n| avgS: Average of steps in an episode.\"\n",
    "              f\"\\n| objC: Objective of Critic network. Or call it loss function of critic network.\"\n",
    "              f\"\\n| objA: Objective of Actor network. It is the average Q value of the critic network.\"\n",
    "              f\"\\n| {'step':>8}  {'time':>8}  | {'avgR':>8}  {'stdR':>6}  {'avgS':>6}  | {'objC':>8}  {'objA':>8}\")\n",
    "            \n",
    "    def evaluate_and_save(self, actor, horizon_len: int, logging_tuple: tuple):\n",
    "        self.total_step += horizon_len\n",
    "        if self.eval_step + self.eval_per_step > self.total_step:\n",
    "            return\n",
    "        self.eval_step = self.total_step\n",
    "\n",
    "        rewards_steps_ary = [get_rewards_and_steps(self.env_eval, actor) for _ in range(self.eval_times)]\n",
    "        rewards_steps_ary = np.array(rewards_steps_ary, dtype=np.float32)\n",
    "        avg_r = rewards_steps_ary[:, 0].mean()  # average of cumulative rewards\n",
    "        std_r = rewards_steps_ary[:, 0].std()  # std of cumulative rewards\n",
    "        avg_s = rewards_steps_ary[:, 1].mean()  # average of steps in an episode\n",
    "\n",
    "        used_time = time.time() - self.start_time\n",
    "        self.recorder.append((self.total_step, used_time, avg_r))\n",
    "        \n",
    "        print(f\"| {self.total_step:8.2e}  {used_time:8.0f}  \"\n",
    "              f\"| {avg_r:8.2f}  {std_r:6.2f}  {avg_s:6.0f}  \"\n",
    "              f\"| {logging_tuple[0]:8.2f}  {logging_tuple[1]:8.2f}\")\n",
    "\n",
    "\n",
    "def get_rewards_and_steps(env, actor, if_render: bool = False) -> (float, int):  # cumulative_rewards and episode_steps\n",
    "    device = next(actor.parameters()).device  # net.parameters() is a Python generator.\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    episode_steps = 0\n",
    "    cumulative_returns = 0.0  # sum of rewards in an episode\n",
    "    for episode_steps in range(100_000):\n",
    "        tensor_state = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        tensor_action = actor(tensor_state)\n",
    "        action = tensor_action.detach().cpu().numpy()[0]  # not need detach(), because using torch.no_grad() outside\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        cumulative_returns += reward\n",
    "\n",
    "        if if_render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "    return cumulative_returns, episode_steps + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9tzAw9k26nAC"
   },
   "source": [
    "## DRL Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwCbbocm6PHM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "MODELS = {\"ppo\": AgentPPO}\n",
    "OFF_POLICY_MODELS = [\"ddpg\", \"td3\", \"sac\"]\n",
    "ON_POLICY_MODELS = [\"ppo\"]\n",
    "\n",
    "\n",
    "\n",
    "class DRLAgent:\n",
    "    \"\"\"Implementations of DRL algorithms\n",
    "    Attributes\n",
    "    ----------\n",
    "        env: gym environment class\n",
    "            user-defined class\n",
    "    Methods\n",
    "    -------\n",
    "        get_model()\n",
    "            setup DRL algorithms\n",
    "        train_model()\n",
    "            train DRL algorithms in a train dataset\n",
    "            and output the trained model\n",
    "        DRL_prediction()\n",
    "            make a prediction in a test dataset and get results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, price_array, tech_array, turbulence_array, date_array):\n",
    "            self.env = env\n",
    "            self.price_array = price_array\n",
    "            self.tech_array = tech_array\n",
    "            self.turbulence_array = turbulence_array\n",
    "            self.date_array = date_array\n",
    "\n",
    "    def get_model(self, model_name, model_kwargs, cwd, model_resume_training=True):\n",
    "        env_config = {\n",
    "            \"price_array\": self.price_array,\n",
    "            \"tech_array\": self.tech_array,\n",
    "            \"turbulence_array\": self.turbulence_array,\n",
    "            \"date_array\": self.date_array,\n",
    "            \"if_train\": True,\n",
    "        }\n",
    "        environment = self.env(config=env_config)\n",
    "        env_args = {'config': env_config,\n",
    "              'env_name': environment.env_name,\n",
    "              'state_dim': environment.state_dim,\n",
    "              'action_dim': environment.action_dim,\n",
    "              'if_discrete': False}\n",
    "        agent = MODELS[model_name]\n",
    "        if model_name not in MODELS:\n",
    "            raise NotImplementedError(\"NotImplementedError\")\n",
    "        model = Config(agent_class=agent, env_class=self.env, env_args=env_args)\n",
    "        model.cwd = cwd\n",
    "        model.if_off_policy = model_name in OFF_POLICY_MODELS\n",
    "        if model_kwargs is not None:\n",
    "            try:\n",
    "                model.learning_rate = model_kwargs[\"learning_rate\"]\n",
    "                model.batch_size = model_kwargs[\"batch_size\"]\n",
    "                model.gamma = model_kwargs[\"gamma\"]\n",
    "                model.seed = model_kwargs[\"seed\"]\n",
    "                model.net_dims = model_kwargs[\"net_dimension\"]\n",
    "                model.target_step = model_kwargs[\"target_step\"]\n",
    "                model.eval_gap = model_kwargs[\"eval_gap\"]\n",
    "                model.eval_times = model_kwargs[\"eval_times\"]\n",
    "                model.weight_decay = model_kwargs[\"weight_decay\"]\n",
    "                model.lambda_entropy = model_kwargs[\"lambda_entropy\"]\n",
    "            except BaseException:\n",
    "                raise ValueError(\n",
    "                    \"Fail to read arguments, please check 'model_kwargs' input.\"\n",
    "                )\n",
    "        # Initialize the agent's act attribute\n",
    "        agent_instance = agent(model_kwargs[\"net_dimension\"], environment.state_dim, environment.action_dim)\n",
    "        model.act = agent_instance.act\n",
    "        # Resume trainining or start from scratch logic\n",
    "        model_path = model.cwd + '/actor.pth'\n",
    "        if model_resume_training and os.path.exists(model_path):\n",
    "            print(f\"Loading existing model from: {model_path}\")\n",
    "            model.act.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))\n",
    "        elif not model_resume_training and os.path.exists(model_path):\n",
    "            print(f\"Deleting existing model and starting training from scratch.\")\n",
    "            os.remove(model_path)\n",
    "        else:\n",
    "            print(\"No existing model found. Starting training from scratch.\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def train_model(self, model, cwd, total_timesteps=5000):\n",
    "        model.cwd = cwd\n",
    "        model.break_step = total_timesteps\n",
    "        train_agent(model)\n",
    "\n",
    "    @staticmethod\n",
    "    def DRL_prediction(model_name, cwd, net_dimension, environment):\n",
    "        if model_name not in MODELS:\n",
    "            raise NotImplementedError(\"NotImplementedError\")\n",
    "        agent_class = MODELS[model_name]\n",
    "        environment.env_num = 1\n",
    "        agent = agent_class(net_dimension, environment.state_dim, environment.action_dim)\n",
    "        actor = agent.act\n",
    "        # load agent\n",
    "        try:  \n",
    "            cwd = cwd + '/actor.pth'\n",
    "            print(f\"| load actor from: {cwd}\")\n",
    "            actor.load_state_dict(torch.load(cwd, map_location=lambda storage, loc: storage))\n",
    "            act = actor\n",
    "            device = agent.device\n",
    "        except BaseException:\n",
    "            raise ValueError(\"Fail to load agent!\")\n",
    "\n",
    "        # test on the testing env\n",
    "        _torch = torch\n",
    "        state, _ = environment.reset()\n",
    "        episode_returns = []  # the cumulative_return / initial_account\n",
    "        episode_total_assets = [environment.initial_total_assets]\n",
    "        episode_dates = []\n",
    "        with _torch.no_grad():\n",
    "            for i in range(environment.max_step):\n",
    "                s_tensor = _torch.as_tensor((state,), device=device)\n",
    "                a_tensor = act(s_tensor)  # action_tanh = act.forward()\n",
    "                action = (\n",
    "                    a_tensor.detach().cpu().numpy()[0]\n",
    "                )  # not need detach(), because with torch.no_grad() outside\n",
    "                state, reward, done, _, _ = environment.step(action)\n",
    "                # episode_dates.append(environment.get_date())\n",
    "                episode_dates.append(environment.date_ary[environment.minute])\n",
    "                total_assets = (\n",
    "                    environment.cash\n",
    "                    + (\n",
    "                        environment.price_ary[environment.minute] * environment.stocks\n",
    "                    ).sum()\n",
    "                )\n",
    "                episode_total_assets.append(total_assets)\n",
    "                episode_return = total_assets / environment.initial_total_assets\n",
    "                episode_returns.append(episode_return)\n",
    "                if done:\n",
    "                    break\n",
    "        num_trades = environment.num_trades\n",
    "        print(\"Test Finished!\")\n",
    "        # return episode total_assets on testing data\n",
    "        print(\"episode_return\", episode_return)\n",
    "        return episode_dates, episode_total_assets, num_trades"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zjLda8No6pvI"
   },
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8-e03ev32oz"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    # Create 'train_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"train_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'train_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "            \n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": True,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # read parameters\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "\n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        break_step = kwargs.get(\"break_step\", 1e6)\n",
    "        erl_params = kwargs.get(\"erl_params\")\n",
    "        agent = DRLAgent_erl(\n",
    "            env=env,\n",
    "            price_array=price_array,\n",
    "            tech_array=tech_array,\n",
    "            turbulence_array=turbulence_array,\n",
    "            date_array=date_array,\n",
    "        )\n",
    "        model = agent.get_model(model_name, model_kwargs=erl_params, cwd=cwd)\n",
    "        trained_model = agent.train_model(\n",
    "            model=model, cwd=cwd, total_timesteps=break_step\n",
    "        )    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Evsg8QtEDHDO"
   },
   "outputs": [],
   "source": [
    "def test(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    # Create 'test_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"test_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'test_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "            \n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": False,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # load elegantrl needs state dim, action dim and net dim\n",
    "    net_dimension = kwargs.get(\"net_dimension\", 2**7)\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "    print(\"price_array: \", len(price_array))\n",
    "\n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        episode_dates, episode_total_assets, num_trades = DRLAgent_erl.DRL_prediction(\n",
    "            model_name=model_name,\n",
    "            cwd=cwd,\n",
    "            net_dimension=net_dimension,\n",
    "            environment=env_instance,\n",
    "        )\n",
    "        \n",
    "        print(f\"Number of trades made: {num_trades}\")\n",
    "        episode_dates = np.unique(np.concatenate(episode_dates))\n",
    "        episode_dates = np.concatenate(([episode_dates[0]], episode_dates))\n",
    "        episode_dates = [timestamp.date() for timestamp in episode_dates]\n",
    "        account_value_df = pd.DataFrame({'date': episode_dates, 'account_value': episode_total_assets})\n",
    "        return account_value_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pf5aVHAU-xF6"
   },
   "source": [
    "## Import Stock Symbols and Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jx25TA_X87F-"
   },
   "outputs": [],
   "source": [
    "ticker_list = GP1_TICKER\n",
    "action_dim = len(GP1_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIV0kO_y-inG",
    "outputId": "bd7b3c21-641e-4eb7-a4af-ae7d156042a6"
   },
   "outputs": [],
   "source": [
    "print(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CnqQ-cC5-rfO",
    "outputId": "29b248c9-ec98-44cd-befb-65192af72ea4"
   },
   "outputs": [],
   "source": [
    "print(INDICATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CDL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rZMkcyjZ-25l"
   },
   "source": [
    "## Calculate the DRL State Dimension Manually for Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLfkTsXK-e90"
   },
   "outputs": [],
   "source": [
    "state_dim = 114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqUkvImG-n66",
    "outputId": "9cb4a3d8-5064-4971-d095-65d3ab12f11a"
   },
   "outputs": [],
   "source": [
    "state_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eW0UDAXI1nEa"
   },
   "source": [
    "# Part 2: Train the Agent\n",
    "\n",
    "Note: Start Date Must Be Monday for backtest plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import random\n",
    "from datetime import datetime\n",
    "from calendar import monthrange\n",
    "\n",
    "\n",
    "env = StockTradingEnv\n",
    "\n",
    "TRAIN_START_DATE = \"2016-01-01\"\n",
    "TRAIN_END_DATE = \"2016-12-31\"\n",
    "\n",
    "def optuna_train(\n",
    "    trial,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Create 'train_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"train_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'train_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "        \n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": True,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # read parameters\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "    \n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        break_step = kwargs.get(\"break_step\", 1e6)\n",
    "        erl_params = kwargs.get(\"erl_params\")\n",
    "        agent = DRLAgent_erl(\n",
    "            env=env,\n",
    "            price_array=price_array,\n",
    "            tech_array=tech_array,\n",
    "            turbulence_array=turbulence_array,\n",
    "            date_array=date_array,\n",
    "        )\n",
    "        model = agent.get_model(model_name, model_kwargs=erl_params, cwd=cwd, model_resume_training=False)\n",
    "        trained_model = agent.train_model(\n",
    "            model=model, cwd=cwd, total_timesteps=break_step\n",
    "        )\n",
    "\n",
    "def optuna_test(\n",
    "    trial,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Create 'test_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"test_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'test_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": False,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # load elegantrl needs state dim, action dim and net dim\n",
    "    net_dimension = kwargs.get(\"net_dimension\")\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "    print(\"price_array: \", len(price_array))\n",
    "\n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        episode_dates, episode_total_assets, num_trades = DRLAgent_erl.DRL_prediction(\n",
    "            model_name=model_name,\n",
    "            cwd=cwd,\n",
    "            net_dimension=net_dimension,\n",
    "            environment=env_instance,\n",
    "        )\n",
    "        \n",
    "        print(f\"Number of trades made: {num_trades}\")\n",
    "        episode_dates = np.unique(np.concatenate(episode_dates))\n",
    "        episode_dates = np.concatenate(([episode_dates[0]], episode_dates))\n",
    "        episode_dates = [timestamp.date() for timestamp in episode_dates]\n",
    "        account_value_df = pd.DataFrame({'date': episode_dates, 'account_value': episode_total_assets})\n",
    "        return account_value_df, num_trades\n",
    "\n",
    "\n",
    "def optuna_calculate_loss(account_value_daily, w1=0.6, w2=0.4):\n",
    "    pnl = account_value_daily['account_value'].iloc[-1] - account_value_daily['account_value'].iloc[0]\n",
    "    neg_pnl = -pnl\n",
    "\n",
    "    running_max = np.maximum.accumulate(account_value_daily['account_value'])\n",
    "    drawdowns = (running_max - account_value_daily['account_value']) / running_max\n",
    "    max_drawdown = np.max(drawdowns)\n",
    "\n",
    "    loss = w1 * neg_pnl + w2 * max_drawdown\n",
    "    return loss\n",
    "\n",
    "\n",
    "def optuna_objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-5, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 0.15, log=True)\n",
    "    lambda_entropy = trial.suggest_float(\"lambda_entropy\", 1e-5, 0.20, log=True)\n",
    "    batch_size = 4096\n",
    "    gamma = 0.985\n",
    "    net_dimension = [128, 1024]\n",
    "    target_step = 5000\n",
    "    eval_gap = 50\n",
    "    eval_times = 1\n",
    "    erl_params = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"lambda_entropy\": lambda_entropy,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gamma\": gamma,\n",
    "        \"seed\": 999,\n",
    "        \"net_dimension\": net_dimension,\n",
    "        \"target_step\": target_step,\n",
    "        \"eval_gap\": eval_gap,\n",
    "        \"eval_times\": eval_times,\n",
    "    }\n",
    "    \n",
    "    random_year = random.randint(2017, 2022)\n",
    "    random_month = random.randint(1, 12)\n",
    "    _, last_day_of_month = monthrange(random_year, random_month)\n",
    "\n",
    "    TEST_START_DATE = datetime(random_year, random_month, 1)\n",
    "    TEST_END_DATE = datetime(random_year, random_month, last_day_of_month)\n",
    "    TEST_START_DATE = TEST_START_DATE.strftime('%Y-%m-%d')\n",
    "    TEST_END_DATE = TEST_END_DATE.strftime('%Y-%m-%d')\n",
    "    \n",
    "    optuna_train(\n",
    "        trial,\n",
    "        start_date = TRAIN_START_DATE, \n",
    "        end_date = TRAIN_END_DATE,\n",
    "        ticker_list = ticker_list, \n",
    "        data_source = 'alpaca',\n",
    "        time_interval = '1Min', \n",
    "        technical_indicator_list = INDICATORS,\n",
    "        drl_lib ='elegantrl', \n",
    "        env = env,\n",
    "        model_name ='ppo',\n",
    "        if_vix = True,\n",
    "        if_cdl = True, \n",
    "        API_KEY = ALPACA_API_KEY, \n",
    "        API_SECRET = ALPACA_API_SECRET, \n",
    "        API_BASE_URL = API_BASE_URL,\n",
    "        cwd = './gp1_testing', #current_working_dir\n",
    "        break_step = 2e5,\n",
    "        erl_params = erl_params,\n",
    "    )\n",
    "\n",
    "    account_value_df, num_trades = optuna_test(\n",
    "        trial,\n",
    "        start_date = TEST_START_DATE, \n",
    "        end_date = TEST_END_DATE,\n",
    "        ticker_list = ticker_list, \n",
    "        data_source = 'alpaca',\n",
    "        time_interval = '1Min', \n",
    "        technical_indicator_list = INDICATORS,\n",
    "        drl_lib = 'elegantrl', \n",
    "        env = env, \n",
    "        model_name = 'ppo',\n",
    "        if_vix = True,\n",
    "        if_cdl = True,\n",
    "        API_KEY = ALPACA_API_KEY, \n",
    "        API_SECRET = ALPACA_API_SECRET, \n",
    "        API_BASE_URL = API_BASE_URL,\n",
    "        cwd = './gp1_testing',\n",
    "        net_dimension = erl_params['net_dimension'],\n",
    "    )\n",
    "\n",
    "    account_value_df['date'] = pd.to_datetime(account_value_df['date']).dt.date\n",
    "    account_value_daily = account_value_df.groupby('date')['account_value'].last().reset_index()\n",
    "\n",
    "    account_value_daily['daily_return'] = account_value_daily['account_value'].pct_change(1)\n",
    "\n",
    "    if account_value_daily['daily_return'].std() > 1e-8:\n",
    "        sharpe_ratio = (252**0.5) * account_value_daily['daily_return'].mean() / account_value_daily['daily_return'].std()\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "    \n",
    "    loss = optuna_calculate_loss(account_value_daily)\n",
    "\n",
    "    return loss, sharpe_ratio, num_trades"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Optuna Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"gp1_trials\"\n",
    "directions = [\"minimize\", \"maximize\", \"minimize\"]\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, directions=directions)\n",
    "\n",
    "# timeout=3600 is 1 hr, 86400 is 24hrs or n_trials=100\n",
    "study.optimize(optuna_objective, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pareto front trials\n",
    "pareto_front_trials = study.best_trials\n",
    "\n",
    "# Print Pareto front trials\n",
    "print(f\"Number of trials on the Pareto front: {len(study.best_trials)}\")\n",
    "for trial in pareto_front_trials:\n",
    "    print(f\"  Trial {trial.number}:\")\n",
    "    print(\"    Value: \", trial.values)\n",
    "    print(\"    Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"      {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_pareto_front(study, target_names=[\"loss\", \"sharpe_ratio\", \"num_trades\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Visualization\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "pareto_front = optuna.visualization.plot_pareto_front(study, target_names=[\"loss\", \"sharpe_ratio\", \"num_trades\"])\n",
    "\n",
    "pio.write_html(pareto_front, file='pareto_front.html', auto_open=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = study.get_trials()\n",
    "\n",
    "def trial_filter(trial):\n",
    "    if trial.values is None:\n",
    "        return False\n",
    "    \n",
    "    # order of objectives:\n",
    "    # 0: loss (minimize)\n",
    "    # 1: sharpe ratio (maximize)\n",
    "    # 2: number of trades (minimize)\n",
    "    \n",
    "    # Set thresholds for each objective\n",
    "    loss_threshold = 0\n",
    "    sharpe_ratio_threshold = 3.0\n",
    "    num_trades_threshold = 100\n",
    "\n",
    "    if (trial.values[0] <= loss_threshold and \n",
    "        trial.values[1] >= sharpe_ratio_threshold and \n",
    "        trial.values[2] >= num_trades_threshold):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "filtered_trials = [t for t in trials if trial_filter(t)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered Trials Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for min, max, and sum\n",
    "min_learning_rate = float('inf')\n",
    "max_learning_rate = float('-inf')\n",
    "sum_learning_rate = 0\n",
    "\n",
    "min_weight_decay = float('inf')\n",
    "max_weight_decay = float('-inf')\n",
    "sum_weight_decay = 0\n",
    "\n",
    "min_lambda_entropy = float('inf')\n",
    "max_lambda_entropy = float('-inf')\n",
    "sum_lambda_entropy = 0\n",
    "\n",
    "# Iterate through the filtered trials and calculate the required statistics\n",
    "num_trials = len(filtered_trials)\n",
    "\n",
    "for trial in filtered_trials:\n",
    "    learning_rate = trial.params['learning_rate']\n",
    "    weight_decay = trial.params['weight_decay']\n",
    "    lambda_entropy = trial.params['lambda_entropy']\n",
    "\n",
    "    min_learning_rate = min(min_learning_rate, learning_rate)\n",
    "    max_learning_rate = max(max_learning_rate, learning_rate)\n",
    "    sum_learning_rate += learning_rate\n",
    "\n",
    "    min_weight_decay = min(min_weight_decay, weight_decay)\n",
    "    max_weight_decay = max(max_weight_decay, weight_decay)\n",
    "    sum_weight_decay += weight_decay\n",
    "\n",
    "    min_lambda_entropy = min(min_lambda_entropy, lambda_entropy)\n",
    "    max_lambda_entropy = max(max_lambda_entropy, lambda_entropy)\n",
    "    sum_lambda_entropy += lambda_entropy\n",
    "\n",
    "# Calculate averages\n",
    "avg_learning_rate = sum_learning_rate / num_trials\n",
    "avg_weight_decay = sum_weight_decay / num_trials\n",
    "avg_lambda_entropy = sum_lambda_entropy / num_trials\n",
    "\n",
    "# Print the results\n",
    "print(\"Learning rate: min={}, max={}, avg={}\".format(min_learning_rate, max_learning_rate, avg_learning_rate))\n",
    "print(\"Weight decay: min={}, max={}, avg={}\".format(min_weight_decay, max_weight_decay, avg_weight_decay))\n",
    "print(\"Lambda entropy: min={}, max={}, avg={}\".format(min_lambda_entropy, max_lambda_entropy, avg_lambda_entropy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gauntlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.rrule import rrule, MONTHLY\n",
    "from dateutil.parser import parse\n",
    "from calendar import monthrange\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "env = StockTradingEnv\n",
    "\n",
    "TRAIN_START_DATE = \"2016-01-01\"\n",
    "TRAIN_END_DATE = \"2016-12-31\"\n",
    "TEST_START_DATE = \"2017-01-01\"\n",
    "TEST_END_DATE = \"2017-01-31\"\n",
    "END_DATE = '2023-03-31'\n",
    "\n",
    "def optuna_train(\n",
    "    trial,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Create 'train_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"train_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'train_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "        \n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": True,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # read parameters\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "\n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        break_step = kwargs.get(\"break_step\", 1e6)\n",
    "        erl_params = kwargs.get(\"erl_params\")\n",
    "        agent = DRLAgent_erl(\n",
    "            env=env,\n",
    "            price_array=price_array,\n",
    "            tech_array=tech_array,\n",
    "            turbulence_array=turbulence_array,\n",
    "            date_array=date_array,\n",
    "        )\n",
    "        model = agent.get_model(model_name, model_kwargs=erl_params, cwd=cwd, model_resume_training=True)\n",
    "        trained_model = agent.train_model(\n",
    "            model=model, cwd=cwd, total_timesteps=break_step\n",
    "        )\n",
    "\n",
    "def optuna_test(\n",
    "    trial,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Create 'test_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"test_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'test_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "            \n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": False,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # load elegantrl needs state dim, action dim and net dim\n",
    "    net_dimension = kwargs.get(\"net_dimension\")\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "    print(\"price_array: \", len(price_array))\n",
    "\n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        episode_dates, episode_total_assets, num_trades = DRLAgent_erl.DRL_prediction(\n",
    "            model_name=model_name,\n",
    "            cwd=cwd,\n",
    "            net_dimension=net_dimension,\n",
    "            environment=env_instance,\n",
    "        )\n",
    "        \n",
    "        print(f\"Number of trades made: {num_trades}\")\n",
    "        episode_dates = np.unique(np.concatenate(episode_dates))\n",
    "        episode_dates = np.concatenate(([episode_dates[0]], episode_dates))\n",
    "        episode_dates = [timestamp.date() for timestamp in episode_dates]\n",
    "        account_value_df = pd.DataFrame({'date': episode_dates, 'account_value': episode_total_assets})\n",
    "        return account_value_df, num_trades\n",
    "\n",
    "\n",
    "def optuna_calculate_loss(account_value_daily, w1=0.6, w2=0.4):\n",
    "    pnl = account_value_daily['account_value'].iloc[-1] - account_value_daily['account_value'].iloc[0]\n",
    "    neg_pnl = -pnl\n",
    "\n",
    "    running_max = np.maximum.accumulate(account_value_daily['account_value'])\n",
    "    drawdowns = (running_max - account_value_daily['account_value']) / running_max\n",
    "    max_drawdown = np.max(drawdowns)\n",
    "\n",
    "    loss = w1 * neg_pnl + w2 * max_drawdown\n",
    "    return loss\n",
    "\n",
    "def optuna_gauntlet_objective(trial):\n",
    "    loss_threshold = 0\n",
    "    sharpe_threshold = 0\n",
    "    min_trades_threshold = 100\n",
    "    max_trades_threshold = 5_000\n",
    "    returns_threshold = 1.0\n",
    "    monthly_returns = []\n",
    "    monthly_loss = []\n",
    "    monthly_sharpe = []\n",
    "    monthly_trades = []\n",
    "    \n",
    "    \n",
    "    train_start_date = datetime.strptime(TRAIN_START_DATE, '%Y-%m-%d').date()\n",
    "    train_end_date = datetime.strptime(TRAIN_END_DATE, '%Y-%m-%d').date()\n",
    "    test_start_date = datetime.strptime(TEST_START_DATE, '%Y-%m-%d').date()\n",
    "    test_end_date = datetime.strptime(TEST_END_DATE, '%Y-%m-%d').date()\n",
    "    end_date = datetime.strptime(END_DATE, '%Y-%m-%d').date()\n",
    "    \n",
    "    while train_end_date <= end_date:\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-5, log=True)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 0.15, log=True)\n",
    "        lambda_entropy = trial.suggest_float(\"lambda_entropy\", 1e-5, 0.20, log=True)\n",
    "        batch_size = 4096\n",
    "        gamma = 0.985\n",
    "        net_dimension = [128, 1024]\n",
    "        target_step = 5000\n",
    "        eval_gap = 50\n",
    "        eval_times = 1\n",
    "        erl_params = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"lambda_entropy\": lambda_entropy,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"gamma\": gamma,\n",
    "            \"seed\": 999,\n",
    "            \"net_dimension\": net_dimension,\n",
    "            \"target_step\": target_step,\n",
    "            \"eval_gap\": eval_gap,\n",
    "            \"eval_times\": eval_times,\n",
    "        }\n",
    "        \n",
    "        optuna_train(\n",
    "            trial,\n",
    "            start_date=train_start_date.strftime('%Y-%m-%d'),\n",
    "            end_date=train_end_date.strftime('%Y-%m-%d'),\n",
    "            ticker_list = ticker_list, \n",
    "            data_source = 'alpaca',\n",
    "            time_interval= '1Min', \n",
    "            technical_indicator_list= INDICATORS,\n",
    "            drl_lib='elegantrl', \n",
    "            env=env,\n",
    "            model_name='ppo',\n",
    "            if_vix=True,\n",
    "            if_cdl=True, \n",
    "            API_KEY = ALPACA_API_KEY, \n",
    "            API_SECRET = ALPACA_API_SECRET, \n",
    "            API_BASE_URL = API_BASE_URL,\n",
    "            cwd='./gp1_testing', #current_working_dir\n",
    "            break_step=2e5,\n",
    "            erl_params=erl_params\n",
    "        )\n",
    "\n",
    "        account_value_df, num_trades = optuna_test(\n",
    "            trial,\n",
    "            start_date=test_start_date.strftime('%Y-%m-%d'),\n",
    "            end_date=test_end_date.strftime('%Y-%m-%d'),\n",
    "            ticker_list = ticker_list, \n",
    "            data_source = 'alpaca',\n",
    "            time_interval= '1Min', \n",
    "            technical_indicator_list= INDICATORS,\n",
    "            drl_lib='elegantrl', \n",
    "            env=env, \n",
    "            model_name='ppo',\n",
    "            if_vix=True,\n",
    "            if_cdl=True,\n",
    "            API_KEY = ALPACA_API_KEY, \n",
    "            API_SECRET = ALPACA_API_SECRET, \n",
    "            API_BASE_URL = API_BASE_URL,\n",
    "            cwd='./gp1_testing',\n",
    "            net_dimension = erl_params['net_dimension'],\n",
    "        )\n",
    "        # Stats\n",
    "        account_value_df['date'] = pd.to_datetime(account_value_df['date']).dt.date\n",
    "        account_value_daily = account_value_df.groupby('date')['account_value'].last().reset_index()\n",
    "\n",
    "        account_value_daily['daily_return'] = account_value_daily['account_value'].pct_change(1)\n",
    "\n",
    "        if account_value_daily['daily_return'].std() > 1e-8:\n",
    "            sharpe_ratio = (252**0.5) * account_value_daily['daily_return'].mean() / account_value_daily['daily_return'].std()\n",
    "        else:\n",
    "            sharpe_ratio = 0\n",
    "        \n",
    "        loss = optuna_calculate_loss(account_value_daily)\n",
    "\n",
    "        initial_account_value = account_value_daily['account_value'].iloc[0]\n",
    "        final_account_value = account_value_daily['account_value'].iloc[-1]\n",
    "\n",
    "        monthly_return = (final_account_value - initial_account_value) / initial_account_value\n",
    "        returns = monthly_return * 100\n",
    "        \n",
    "        # Check if all thresholds are met\n",
    "        if (\n",
    "            loss < loss_threshold\n",
    "            and sharpe_ratio > sharpe_threshold\n",
    "            and min_trades_threshold <= num_trades <= max_trades_threshold\n",
    "            and returns >= returns_threshold\n",
    "        ):\n",
    "            # Record Stats\n",
    "            monthly_returns.append(returns)\n",
    "            monthly_loss.append(loss)\n",
    "            monthly_sharpe.append(sharpe_ratio)\n",
    "            monthly_trades.append(num_trades)\n",
    "            \n",
    "            # Move the date range one month forward\n",
    "            train_start_date = test_start_date\n",
    "            train_end_date = test_end_date\n",
    "            test_start_date = list(rrule(freq=MONTHLY, dtstart=parse(test_start_date.strftime('%Y-%m-%d')), count=2))[-1].date()\n",
    "            _, last_day_of_month = monthrange(test_start_date.year, test_start_date.month)\n",
    "            test_end_date = test_start_date.replace(day=last_day_of_month)\n",
    "        else:\n",
    "            # Delete the model if saved to disk\n",
    "            if os.path.exists(\"./gp1_testing/actor.pth\"):\n",
    "                os.remove(\"./gp1_testing/actor.pth\")\n",
    "            # Cancel Trial\n",
    "            raise optuna.TrialPruned()\n",
    "    avg_monthly_loss = np.mean(monthly_loss)\n",
    "    avg_monthly_sharpe = np.mean(monthly_sharpe)\n",
    "    avg_monthly_trades = np.mean(monthly_trades)\n",
    "    avg_monthly_return = np.mean(monthly_returns)\n",
    "    \n",
    "    # Move and rename the model file\n",
    "    src_file = \"./gp1_testing/actor.pth\"\n",
    "    dst_dir = \"./gp1_gauntlet\"\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    dst_file = os.path.join(dst_dir, f\"actor_trial_{trial.number}_avg_return_{int(round(avg_monthly_return))}.pth\")\n",
    "    shutil.move(src_file, dst_file)\n",
    "    \n",
    "    return avg_monthly_loss, avg_monthly_sharpe, avg_monthly_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"gp1_gauntlet\"\n",
    "directions = [\"minimize\", \"maximize\", \"minimize\"]\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, directions=directions)\n",
    "\n",
    "# timeout=3600 is 1 hr, 86400 is 24hrs or n_trials=100\n",
    "study.optimize(optuna_gauntlet_objective, timeout=(3600 * 22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_pareto_front(study, target_names=[\"avg_monthly_loss\", \"avg_monthly_sharpe\", \"avg_monthly_trades\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Visualization\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "pareto_front = optuna.visualization.plot_pareto_front(study, target_names=[\"avg_monthly_loss\", \"avg_monthly_sharpe\", \"avg_monthly_trades\"])\n",
    "\n",
    "pio.write_html(pareto_front, file='pareto_front.html', auto_open=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERL_PARAMS = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"batch_size\": 2048,\n",
    "    \"gamma\":  0.985,\n",
    "    \"seed\":42,\n",
    "    \"net_dimension\":[128, 256], \n",
    "    \"target_step\":5000, \n",
    "    \"eval_gap\":50,\n",
    "    \"eval_times\":1\n",
    "}\n",
    "env = StockTradingEnv\n",
    "\n",
    "TEST_START_DATE = \"2023-01-02\"\n",
    "TEST_END_DATE = \"2023-03-31\"\n",
    "\n",
    "gp1_account_value_df=test(start_date = TEST_START_DATE, \n",
    "                    end_date = TEST_END_DATE,\n",
    "                    ticker_list = ticker_list, \n",
    "                    data_source = 'alpaca',\n",
    "                    time_interval= '1Min', \n",
    "                    technical_indicator_list= INDICATORS,\n",
    "                    drl_lib='elegantrl', \n",
    "                    env=env, \n",
    "                    model_name='ppo',\n",
    "                    if_vix=True,\n",
    "                    if_cdl=True,\n",
    "                    API_KEY = ALPACA_API_KEY, \n",
    "                    API_SECRET = ALPACA_API_SECRET, \n",
    "                    API_BASE_URL = API_BASE_URL,\n",
    "                    cwd='./gp1_trained',\n",
    "                    net_dimension = ERL_PARAMS['net_dimension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==============Get Baseline Stats===========\")\n",
    "\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^GSPC\", \n",
    "        start = TEST_START_DATE,\n",
    "        end = TEST_END_DATE)\n",
    "\n",
    "stats = backtest_stats(baseline_df, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==============GP1 Stats===========\")\n",
    "\n",
    "stats = backtest_stats(gp1_account_value_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "gp1_account_value_df = drop_dup_dates(gp1_account_value_df)\n",
    "\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "\n",
    "backtest_plot(gp1_account_value_df, \n",
    "             baseline_ticker = '^GSPC', \n",
    "             baseline_start = TEST_START_DATE,\n",
    "             baseline_end = TEST_END_DATE,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sIQN6Ggt7gXY"
   },
   "source": [
    "# Part 4: Deploy the Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UFoxkigg1zXa"
   },
   "source": [
    "## Setup Alpaca Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpkoZpYzfneS"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import threading\n",
    "from processor_alpaca import AlpacaProcessor\n",
    "import alpaca_trade_api as tradeapi\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "\n",
    "class AlpacaPaperTrading():\n",
    "\n",
    "    def __init__(self,ticker_list, time_interval, drl_lib, agent, cwd, net_dim, \n",
    "                 state_dim, action_dim, API_KEY, API_SECRET, \n",
    "                 API_BASE_URL, tech_indicator_list, turbulence_thresh=99, \n",
    "                 max_stock=1e2, latency = None):\n",
    "        #load agent\n",
    "        self.drl_lib = drl_lib\n",
    "        if agent =='ppo':\n",
    "            if drl_lib == 'elegantrl':              \n",
    "                agent_class = AgentPPO\n",
    "                agent = agent_class(net_dim, state_dim, action_dim)\n",
    "                actor = agent.act\n",
    "                # load agent\n",
    "                try:  \n",
    "                    cwd = cwd + '/actor.pth'\n",
    "                    print(f\"| load actor from: {cwd}\")\n",
    "                    actor.load_state_dict(torch.load(cwd, map_location=lambda storage, loc: storage))\n",
    "                    self.act = actor\n",
    "                    self.device = agent.device\n",
    "                except BaseException:\n",
    "                    raise ValueError(\"Fail to load agent!\")\n",
    "                    \n",
    "            else:\n",
    "                raise ValueError('The DRL library input is NOT supported yet. Please check your input.')\n",
    "               \n",
    "        else:\n",
    "            raise ValueError('Agent input is NOT supported yet.')\n",
    "            \n",
    "            \n",
    "            \n",
    "        #connect to Alpaca trading API\n",
    "        try:\n",
    "            self.alpaca = tradeapi.REST(API_KEY,API_SECRET,API_BASE_URL, 'v2')\n",
    "        except:\n",
    "            raise ValueError('Fail to connect Alpaca. Please check account info and internet connection.')\n",
    "        \n",
    "        #read trading time interval\n",
    "        if time_interval == '1s':\n",
    "            self.time_interval = 1\n",
    "        elif time_interval == '5s':\n",
    "            self.time_interval = 5\n",
    "        elif time_interval == '1Min':\n",
    "            self.time_interval = 60\n",
    "        elif time_interval == '5Min':\n",
    "            self.time_interval = 60 * 5\n",
    "        elif time_interval == '15Min':\n",
    "            self.time_interval = 60 * 15\n",
    "        else:\n",
    "            raise ValueError('Time interval input is NOT supported yet.')\n",
    "        \n",
    "        #read trading settings\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.turbulence_thresh = turbulence_thresh\n",
    "        self.max_stock = max_stock \n",
    "        \n",
    "        #initialize account\n",
    "        self.stocks = np.asarray([0] * len(ticker_list)) #stocks holding\n",
    "        self.stocks_cd = np.zeros_like(self.stocks) \n",
    "        self.cash = None #cash record \n",
    "        self.stocks_df = pd.DataFrame(self.stocks, columns=['stocks'], index = ticker_list)\n",
    "        self.assets_list = []\n",
    "        self.price = np.asarray([0] * len(ticker_list))\n",
    "        self.stockUniverse = ticker_list\n",
    "        self.turbulence_bool = 0\n",
    "        self.equities = []\n",
    "        \n",
    "    def test_latency(self, test_times = 10): \n",
    "        total_time = 0\n",
    "        for i in range(0, test_times):\n",
    "            time0 = time.time()\n",
    "            self.get_state()\n",
    "            time1 = time.time()\n",
    "            temp_time = time1 - time0\n",
    "            total_time += temp_time\n",
    "        latency = total_time/test_times\n",
    "        print('latency for data processing: ', latency)\n",
    "        return latency\n",
    "        \n",
    "    def run(self):\n",
    "        orders = self.alpaca.list_orders(status=\"open\")\n",
    "        for order in orders:\n",
    "          self.alpaca.cancel_order(order.id)\n",
    "    \n",
    "        # Wait for market to open.\n",
    "        print(\"Waiting for market to open...\")\n",
    "        self.awaitMarketOpen()\n",
    "        print(\"Market opened.\")\n",
    "\n",
    "        while True:\n",
    "\n",
    "          # Figure out when the market will close so we can prepare to sell beforehand.\n",
    "          clock = self.alpaca.get_clock()\n",
    "          closingTime = clock.next_close.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
    "          currTime = clock.timestamp.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
    "          self.timeToClose = closingTime - currTime\n",
    "    \n",
    "          if(self.timeToClose < (60)):\n",
    "            # Close all positions when 1 minutes til market close.\n",
    "            print(\"Market closing soon. Stop trading.\")\n",
    "            break\n",
    "            \n",
    "            '''# Close all positions when 1 minutes til market close.\n",
    "            print(\"Market closing soon.  Closing positions.\")\n",
    "\n",
    "            threads = []\n",
    "            positions = self.alpaca.list_positions()\n",
    "            for position in positions:\n",
    "              if(position.side == 'long'):\n",
    "                orderSide = 'sell'\n",
    "              else:\n",
    "                orderSide = 'buy'\n",
    "              qty = abs(int(float(position.qty)))\n",
    "              respSO = []\n",
    "              tSubmitOrder = threading.Thread(target=self.submitOrder(qty, position.symbol, orderSide, respSO))\n",
    "              tSubmitOrder.start()\n",
    "              threads.append(tSubmitOrder)    # record thread for joining later\n",
    "\n",
    "            for x in threads:   #  wait for all threads to complete\n",
    "                x.join()     \n",
    "            # Run script again after market close for next trading day.\n",
    "            print(\"Sleeping until market close (15 minutes).\")\n",
    "            time.sleep(60 * 15)'''\n",
    "            \n",
    "          else:\n",
    "            self.trade()\n",
    "            last_equity = float(self.alpaca.get_account().last_equity)\n",
    "            cur_time = time.time()\n",
    "            self.equities.append([cur_time,last_equity])\n",
    "            time.sleep(self.time_interval)\n",
    "            \n",
    "    def awaitMarketOpen(self):\n",
    "        isOpen = self.alpaca.get_clock().is_open\n",
    "        while(not isOpen):\n",
    "          clock = self.alpaca.get_clock()\n",
    "          openingTime = clock.next_open.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
    "          currTime = clock.timestamp.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
    "          timeToOpen = int((openingTime - currTime) / 60)\n",
    "          print(str(timeToOpen) + \" minutes til market open.\")\n",
    "          time.sleep(60)\n",
    "          isOpen = self.alpaca.get_clock().is_open\n",
    "    \n",
    "    def trade(self):\n",
    "        state = self.get_state()\n",
    "        \n",
    "        if self.drl_lib == 'elegantrl':\n",
    "            with torch.no_grad():\n",
    "                s_tensor = torch.as_tensor((state,), device=self.device)\n",
    "                a_tensor = self.act(s_tensor)  \n",
    "                action = a_tensor.detach().cpu().numpy()[0]  \n",
    "            action = (action * self.max_stock).astype(int)\n",
    "            \n",
    "        elif self.drl_lib == 'rllib':\n",
    "            action = self.agent.compute_single_action(state)\n",
    "        \n",
    "        elif self.drl_lib == 'stable_baselines3':\n",
    "            action = self.model.predict(state)[0]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('The DRL library input is NOT supported yet. Please check your input.')\n",
    "        \n",
    "        self.stocks_cd += 1\n",
    "        if self.turbulence_bool == 0:\n",
    "            min_action = 10  # stock_cd\n",
    "            threads = []\n",
    "            for index in np.where(action < -min_action)[0]:  # sell_index:\n",
    "                sell_num_shares = min(self.stocks[index], -action[index])\n",
    "                qty =  abs(int(sell_num_shares))\n",
    "                respSO = []\n",
    "                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, self.stockUniverse[index], 'sell', respSO))\n",
    "                tSubmitOrder.start()\n",
    "                threads.append(tSubmitOrder)    # record thread for joining later\n",
    "                self.cash = float(self.alpaca.get_account().cash)\n",
    "                self.stocks_cd[index] = 0\n",
    "            \n",
    "            for x in threads:   #  wait for all threads to complete\n",
    "                x.join()     \n",
    "\n",
    "            threads = []\n",
    "            for index in np.where(action > min_action)[0]:  # buy_index:\n",
    "                if self.cash < 0:\n",
    "                    tmp_cash = 0\n",
    "                else:\n",
    "                    tmp_cash = self.cash\n",
    "                buy_num_shares = min(tmp_cash // self.price[index], abs(int(action[index])))\n",
    "                if (buy_num_shares != buy_num_shares): # if buy_num_change = nan\n",
    "                    qty = 0 # set to 0 quantity\n",
    "                else:\n",
    "                    qty = abs(int(buy_num_shares))\n",
    "                qty = abs(int(buy_num_shares))\n",
    "                respSO = []\n",
    "                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, self.stockUniverse[index], 'buy', respSO))\n",
    "                tSubmitOrder.start()\n",
    "                threads.append(tSubmitOrder)    # record thread for joining later\n",
    "                self.cash = float(self.alpaca.get_account().cash)\n",
    "                self.stocks_cd[index] = 0\n",
    "\n",
    "            for x in threads:   #  wait for all threads to complete\n",
    "                x.join()     \n",
    "                \n",
    "        else:  # sell all when turbulence\n",
    "            threads = []\n",
    "            positions = self.alpaca.list_positions()\n",
    "            for position in positions:\n",
    "                if(position.side == 'long'):\n",
    "                    orderSide = 'sell'\n",
    "                else:\n",
    "                    orderSide = 'buy'\n",
    "                qty = abs(int(float(position.qty)))\n",
    "                respSO = []\n",
    "                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, position.symbol, orderSide, respSO))\n",
    "                tSubmitOrder.start()\n",
    "                threads.append(tSubmitOrder)    # record thread for joining later\n",
    "\n",
    "            for x in threads:   #  wait for all threads to complete\n",
    "                x.join()     \n",
    "            \n",
    "            self.stocks_cd[:] = 0\n",
    "            \n",
    "    \n",
    "    def get_state(self):\n",
    "        alpaca = AlpacaProcessor(api=self.alpaca)\n",
    "        price, tech, turbulence = alpaca.fetch_latest_data(ticker_list = self.stockUniverse, time_interval='1Min',\n",
    "                                                     tech_indicator_list=self.tech_indicator_list)\n",
    "        turbulence_bool = 1 if turbulence >= self.turbulence_thresh else 0\n",
    "        \n",
    "        turbulence = (self.sigmoid_sign(turbulence, self.turbulence_thresh) * 2 ** -5).astype(np.float32)\n",
    "        \n",
    "        tech = tech * 2 ** -7\n",
    "        positions = self.alpaca.list_positions()\n",
    "        stocks = [0] * len(self.stockUniverse)\n",
    "        for position in positions:\n",
    "            ind = self.stockUniverse.index(position.symbol)\n",
    "            stocks[ind] = ( abs(int(float(position.qty))))\n",
    "        \n",
    "        stocks = np.asarray(stocks, dtype = float)\n",
    "        cash = float(self.alpaca.get_account().cash)\n",
    "        self.cash = cash\n",
    "        self.stocks = stocks\n",
    "        self.turbulence_bool = turbulence_bool \n",
    "        self.price = price\n",
    "        \n",
    "        \n",
    "        \n",
    "        amount = np.array(self.cash * (2 ** -12), dtype=np.float32)\n",
    "        scale = np.array(2 ** -6, dtype=np.float32)\n",
    "        state = np.hstack((amount,\n",
    "                    turbulence,\n",
    "                    self.turbulence_bool,\n",
    "                    price * scale,\n",
    "                    self.stocks * scale,\n",
    "                    self.stocks_cd,\n",
    "                    tech,\n",
    "                    )).astype(np.float32)\n",
    "        state[np.isnan(state)] = 0.0\n",
    "        state[np.isinf(state)] = 0.0\n",
    "        print(len(self.stockUniverse))\n",
    "        return state\n",
    "        \n",
    "    def submitOrder(self, qty, stock, side, resp):\n",
    "        if(qty > 0):\n",
    "          try:\n",
    "            self.alpaca.submit_order(stock, qty, side, \"market\", \"day\")\n",
    "            print(\"Market order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | completed.\")\n",
    "            resp.append(True)\n",
    "          except:\n",
    "            print(\"Order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | did not go through.\")\n",
    "            resp.append(False)\n",
    "        else:\n",
    "          print(\"Quantity is 0, order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | not completed.\")\n",
    "          resp.append(True)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_sign(ary, thresh):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x * np.e)) - 0.5\n",
    "\n",
    "        return sigmoid(ary / thresh) * thresh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "os4C4-4H7ns7"
   },
   "source": [
    "## Run Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nw0i-0UN3-7",
    "outputId": "25729df7-4775-49af-bf5a-38e3970d0056"
   },
   "outputs": [],
   "source": [
    "print(GP1_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YsSBK9ION1t6",
    "outputId": "49a69655-850f-436b-a21c-fffe48528e71"
   },
   "outputs": [],
   "source": [
    "state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYtSv6P1N247",
    "outputId": "174550ce-664a-41fc-bd89-9d3726960c5b"
   },
   "outputs": [],
   "source": [
    "action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERL_PARAMS = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"batch_size\": 2048,\n",
    "    \"gamma\":  0.985,\n",
    "    \"seed\":42,\n",
    "    \"net_dimension\":[128, 256], \n",
    "    \"target_step\":5000, \n",
    "    \"eval_gap\":50,\n",
    "    \"eval_times\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kl9nulnAJtiI"
   },
   "outputs": [],
   "source": [
    "trading_erl = AlpacaPaperTrading(ticker_list = GP1_TICKER, \n",
    "                                       time_interval = '1Min', \n",
    "                                       drl_lib = 'elegantrl', \n",
    "                                       agent = 'ppo', \n",
    "                                       cwd = './gp1_trained', \n",
    "                                       net_dim = ERL_PARAMS['net_dimension'], \n",
    "                                       state_dim = state_dim, \n",
    "                                       action_dim= action_dim, \n",
    "                                       API_KEY = ALPACA_API_KEY, \n",
    "                                       API_SECRET = ALPACA_API_SECRET, \n",
    "                                       API_BASE_URL = API_BASE_URL, \n",
    "                                       tech_indicator_list = INDICATORS,\n",
    "                                       max_stock=1e2, turbulence_thresh=99)\n",
    "trading_erl.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "srzBZfYEUI1O"
   },
   "source": [
    "# Part 5: Check Portfolio Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chovN1UhTAht"
   },
   "outputs": [],
   "source": [
    "import alpaca_trade_api as tradeapi\n",
    "import exchange_calendars as tc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import yfinance as yf\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime as dt\n",
    "from plot import backtest_stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaofxMNCfAR1"
   },
   "outputs": [],
   "source": [
    "def get_trading_days(start, end):\n",
    "    nyse = tc.get_calendar('NYSE')\n",
    "    df = nyse.sessions_in_range(pd.Timestamp(start,tz=pytz.UTC),\n",
    "                                pd.Timestamp(end,tz=pytz.UTC))\n",
    "    trading_days = []\n",
    "    for day in df:\n",
    "        trading_days.append(str(day)[:10])\n",
    "\n",
    "    return trading_days\n",
    "\n",
    "def alpaca_history(key, secret, url, start, end):\n",
    "    api = tradeapi.REST(key, secret, url, 'v2')\n",
    "    trading_days = get_trading_days(start, end)\n",
    "    df = pd.DataFrame()\n",
    "    for day in trading_days:\n",
    "        df = df.append(api.get_portfolio_history(date_start = day,timeframe='5Min').df.iloc[:78])\n",
    "    equities = df.equity.values\n",
    "    cumu_returns = equities/equities[0]\n",
    "    cumu_returns = cumu_returns[~np.isnan(cumu_returns)]\n",
    "    \n",
    "    return df, cumu_returns\n",
    "\n",
    "def SP500_history(start):\n",
    "    data_df = yf.download(['^GSPC'],start=start, interval=\"5m\")\n",
    "    data_df = data_df.iloc[:]\n",
    "    baseline_returns = data_df['Adj Close'].values/data_df['Adj Close'].values[0]\n",
    "    return data_df, baseline_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CHiZRVpURpx"
   },
   "source": [
    "## Get cumulative return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_YT7v-LSdfV"
   },
   "outputs": [],
   "source": [
    "df_erl, cumu_erl = alpaca_history(key=ALPACA_API_KEY, \n",
    "                                  secret=ALPACA_API_SECRET, \n",
    "                                  url=API_BASE_URL, \n",
    "                                  start='2023-03-24', #must be within 1 month\n",
    "                                  end='2023-03-24') #change the date if error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMcQjwHOS6Zb",
    "outputId": "1fb21460-1da9-4998-f0c0-fcbf5b056e66"
   },
   "outputs": [],
   "source": [
    "df_sp500, cumu_sp500 = SP500_history(start='2023-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "PJXPwmx9Ts5o",
    "outputId": "c59014eb-c2f9-4be2-8a87-7892cc0b1094"
   },
   "outputs": [],
   "source": [
    "df_erl.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1Iaw90FTNfU",
    "outputId": "0629dca2-d9dd-4c2a-e363-dc0f01daba41"
   },
   "outputs": [],
   "source": [
    "returns_erl = cumu_erl -1 \n",
    "returns_sp500 = cumu_sp500 - 1\n",
    "returns_sp500 = returns_sp500[:returns_erl.shape[0]]\n",
    "print('len of erl return: ', returns_erl.shape[0])\n",
    "print('len of sp500 return: ', returns_sp500.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IawaMsDwZni"
   },
   "outputs": [],
   "source": [
    "returns_erl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z0LEm7KUZ5W"
   },
   "source": [
    "## plot and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Foqk1wIQTQJ3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(dpi=1000)\n",
    "plt.grid()\n",
    "plt.grid(which='minor', axis='y')\n",
    "plt.title('Stock Trading (Paper trading)', fontsize=20)\n",
    "plt.plot(returns_erl, label = 'ElegantRL Agent', color = 'red')\n",
    "#plt.plot(returns_sb3, label = 'Stable-Baselines3 Agent', color = 'blue' )\n",
    "#plt.plot(returns_rllib, label = 'RLlib Agent', color = 'green')\n",
    "plt.plot(returns_sp500, label = 'S&P 500', color = 'grey')\n",
    "plt.ylabel('Return', fontsize=16)\n",
    "plt.xlabel('Year 2023', fontsize=16)\n",
    "plt.xticks(size = 14)\n",
    "plt.yticks(size = 14)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(78))\n",
    "ax.xaxis.set_minor_locator(ticker.MultipleLocator(6))\n",
    "ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.005))\n",
    "ax.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1, decimals=2))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(['','10-19','','10-20',\n",
    "                                                    '','10-21','','10-22']))\n",
    "plt.legend(fontsize=10.5)\n",
    "plt.savefig('papertrading_stock.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0EVJIQUR6_fu",
    "9tzAw9k26nAC",
    "zjLda8No6pvI",
    "pf5aVHAU-xF6",
    "rZMkcyjZ-25l",
    "3rwy7V72-8YY",
    "J25MuZLiGqCP",
    "eW0UDAXI1nEa",
    "UFoxkigg1zXa"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "005d14239094016f48a03a57365c4ccb734e3f38c20ed0ca595d84f773bc39cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
