{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3rwy7V72-8YY"
   },
   "source": [
    "# GP1 Trading Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the API Keys Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Z6qlLXY-fA2"
   },
   "outputs": [],
   "source": [
    "from config_private import ALPACA_API_KEY, ALPACA_API_SECRET\n",
    "API_BASE_URL = 'https://paper-api.alpaca.markets'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "--6Kx8I21erH"
   },
   "source": [
    "## Part 1: Imports and Class/Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7I7zsyYfoLJ",
    "outputId": "1812c13b-410f-434c-eb07-29782ba186e6"
   },
   "outputs": [],
   "source": [
    "from config_tickers import GP1_TICKER\n",
    "from config import INDICATORS\n",
    "from config import CDL\n",
    "from data_processor import DataProcessor\n",
    "from plot import backtest_stats, backtest_plot, get_baseline, drop_dup_dates\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_stocktrading_np import StockTradingEnv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9tzAw9k26nAC"
   },
   "source": [
    "## DRL Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwCbbocm6PHM"
   },
   "outputs": [],
   "source": [
    "from DRLagent import DRLAgent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zjLda8No6pvI"
   },
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8-e03ev32oz"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    # Create 'train_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"train_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'train_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "            \n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": True,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # read parameters\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "\n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        break_step = kwargs.get(\"break_step\", 1e6)\n",
    "        erl_params = kwargs.get(\"erl_params\")\n",
    "        agent = DRLAgent_erl(\n",
    "            env=env,\n",
    "            price_array=price_array,\n",
    "            tech_array=tech_array,\n",
    "            turbulence_array=turbulence_array,\n",
    "            date_array=date_array,\n",
    "        )\n",
    "        model = agent.get_model(model_name, model_kwargs=erl_params)\n",
    "        trained_model = agent.train_model(\n",
    "            model=model, cwd=cwd, total_timesteps=break_step\n",
    "        )    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Evsg8QtEDHDO"
   },
   "outputs": [],
   "source": [
    "def test(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    # Create 'test_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"test_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'test_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "            \n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": False,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # load elegantrl needs state dim, action dim and net dim\n",
    "    net_dimension = kwargs.get(\"net_dimension\", 2**7)\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "    print(\"price_array: \", len(price_array))\n",
    "\n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        episode_dates, episode_total_assets, _, num_trades = DRLAgent_erl.DRL_prediction(\n",
    "            model_name=model_name,\n",
    "            cwd=cwd,\n",
    "            net_dimension=net_dimension,\n",
    "            environment=env_instance,\n",
    "        )\n",
    "        \n",
    "        print(f\"Number of trades made: {num_trades}\")\n",
    "        episode_dates = np.unique(np.concatenate(episode_dates))\n",
    "        episode_dates = np.concatenate(([episode_dates[0]], episode_dates))\n",
    "        episode_dates = [timestamp.date() for timestamp in episode_dates]\n",
    "        account_value_df = pd.DataFrame({'date': episode_dates, 'account_value': episode_total_assets})\n",
    "        return account_value_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pf5aVHAU-xF6"
   },
   "source": [
    "## Import Stock Symbols and Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jx25TA_X87F-"
   },
   "outputs": [],
   "source": [
    "ticker_list = GP1_TICKER\n",
    "action_dim = len(GP1_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIV0kO_y-inG",
    "outputId": "bd7b3c21-641e-4eb7-a4af-ae7d156042a6"
   },
   "outputs": [],
   "source": [
    "print(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CnqQ-cC5-rfO",
    "outputId": "29b248c9-ec98-44cd-befb-65192af72ea4"
   },
   "outputs": [],
   "source": [
    "print(INDICATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CDL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rZMkcyjZ-25l"
   },
   "source": [
    "## Calculate the DRL State Dimension Manually for Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLfkTsXK-e90"
   },
   "outputs": [],
   "source": [
    "state_dim = 109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqUkvImG-n66",
    "outputId": "9cb4a3d8-5064-4971-d095-65d3ab12f11a"
   },
   "outputs": [],
   "source": [
    "state_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eW0UDAXI1nEa"
   },
   "source": [
    "# Part 2: Train the Agent\n",
    "\n",
    "Note: Start Date Must Be Monday for backtest plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import random\n",
    "from datetime import datetime\n",
    "from calendar import monthrange\n",
    "\n",
    "\n",
    "env = StockTradingEnv\n",
    "\n",
    "TRAIN_START_DATE = \"2016-01-01\"\n",
    "TRAIN_END_DATE = \"2016-01-31\"\n",
    "\n",
    "def optuna_train(\n",
    "    trial,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Create 'train_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"train_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'train_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "        \n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": True,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # read parameters\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "    \n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        break_step = kwargs.get(\"break_step\", 1e6)\n",
    "        erl_params = kwargs.get(\"erl_params\")\n",
    "        agent = DRLAgent_erl(\n",
    "            env=env,\n",
    "            price_array=price_array,\n",
    "            tech_array=tech_array,\n",
    "            turbulence_array=turbulence_array,\n",
    "            date_array=date_array,\n",
    "        )\n",
    "        model = agent.get_model(model_name, model_kwargs=erl_params)\n",
    "        trained_model = agent.train_model(\n",
    "            model=model, cwd=cwd, total_timesteps=break_step\n",
    "        )\n",
    "\n",
    "def optuna_test(\n",
    "    trial,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Create 'test_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"test_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'test_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": False,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # load elegantrl needs state dim, action dim and net dim\n",
    "    net_dimension = kwargs.get(\"net_dimension\")\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "    print(\"price_array: \", len(price_array))\n",
    "\n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        episode_dates, episode_total_assets, _, num_trades = DRLAgent_erl.DRL_prediction(\n",
    "            model_name=model_name,\n",
    "            cwd=cwd,\n",
    "            net_dimension=net_dimension,\n",
    "            environment=env_instance,\n",
    "        )\n",
    "        \n",
    "        print(f\"Number of trades made: {num_trades}\")\n",
    "        episode_dates = np.unique(np.concatenate(episode_dates))\n",
    "        episode_dates = np.concatenate(([episode_dates[0]], episode_dates))\n",
    "        episode_dates = [timestamp.date() for timestamp in episode_dates]\n",
    "        account_value_df = pd.DataFrame({'date': episode_dates, 'account_value': episode_total_assets})\n",
    "        return account_value_df, num_trades\n",
    "\n",
    "\n",
    "def optuna_calculate_loss(account_value_daily, w1=0.6, w2=0.4):\n",
    "    pnl = account_value_daily['account_value'].iloc[-1] - account_value_daily['account_value'].iloc[0]\n",
    "    neg_pnl = -pnl\n",
    "\n",
    "    running_max = np.maximum.accumulate(account_value_daily['account_value'])\n",
    "    drawdowns = (running_max - account_value_daily['account_value']) / running_max\n",
    "    max_drawdown = np.max(drawdowns)\n",
    "\n",
    "    loss = w1 * neg_pnl + w2 * max_drawdown\n",
    "    return loss\n",
    "\n",
    "\n",
    "def optuna_objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-7, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 0.20, log=True)\n",
    "    lambda_entropy = trial.suggest_float(\"lambda_entropy\", 1e-5, 0.20, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [512, 1024, 2048, 4096])\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.985, 0.99, 0.995])\n",
    "    net_dimension = []\n",
    "    n_hidden_layers = trial.suggest_int(\"n_hidden_layers\", 1, 5)\n",
    "    possible_layer_sizes = [8, 16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "    for i in range(n_hidden_layers):\n",
    "        layer_size = trial.suggest_categorical(f\"hidden_layer_{i}\", possible_layer_sizes)\n",
    "        net_dimension.append(layer_size)\n",
    "    target_step = 5000\n",
    "    eval_gap = 30\n",
    "    eval_times = 1\n",
    "    erl_params = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"lambda_entropy\": lambda_entropy,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gamma\": gamma,\n",
    "        \"seed\": 999,\n",
    "        \"net_dimension\": net_dimension,\n",
    "        \"target_step\": target_step,\n",
    "        \"eval_gap\": eval_gap,\n",
    "        \"eval_times\": eval_times,\n",
    "    }\n",
    "    \n",
    "    random_year = random.randint(2017, 2022)\n",
    "    random_month = random.randint(1, 12)\n",
    "    _, last_day_of_month = monthrange(random_year, random_month)\n",
    "\n",
    "    TEST_START_DATE = datetime(random_year, random_month, 1)\n",
    "    TEST_END_DATE = datetime(random_year, random_month, last_day_of_month)\n",
    "    TEST_START_DATE = TEST_START_DATE.strftime('%Y-%m-%d')\n",
    "    TEST_END_DATE = TEST_END_DATE.strftime('%Y-%m-%d')\n",
    "    \n",
    "    optuna_train(\n",
    "        trial,\n",
    "        start_date = TRAIN_START_DATE, \n",
    "        end_date = TRAIN_END_DATE,\n",
    "        ticker_list = ticker_list, \n",
    "        data_source = 'alpaca',\n",
    "        time_interval = \"15Min\", \n",
    "        technical_indicator_list = INDICATORS,\n",
    "        drl_lib ='elegantrl', \n",
    "        env = env,\n",
    "        model_name ='ppo',\n",
    "        if_vix = True,\n",
    "        if_cdl = True, \n",
    "        API_KEY = ALPACA_API_KEY, \n",
    "        API_SECRET = ALPACA_API_SECRET, \n",
    "        API_BASE_URL = API_BASE_URL,\n",
    "        cwd = './gp1_testing', #current_working_dir\n",
    "        break_step = 2e5,\n",
    "        erl_params = erl_params,\n",
    "    )\n",
    "\n",
    "    account_value_df, _ = optuna_test(\n",
    "        trial,\n",
    "        start_date = TEST_START_DATE, \n",
    "        end_date = TEST_END_DATE,\n",
    "        ticker_list = ticker_list, \n",
    "        data_source = 'alpaca',\n",
    "        time_interval = \"15Min\", \n",
    "        technical_indicator_list = INDICATORS,\n",
    "        drl_lib = 'elegantrl', \n",
    "        env = env, \n",
    "        model_name = 'ppo',\n",
    "        if_vix = True,\n",
    "        if_cdl = True,\n",
    "        API_KEY = ALPACA_API_KEY, \n",
    "        API_SECRET = ALPACA_API_SECRET, \n",
    "        API_BASE_URL = API_BASE_URL,\n",
    "        cwd = './gp1_testing',\n",
    "        net_dimension = erl_params['net_dimension'],\n",
    "    )\n",
    "\n",
    "    account_value_df['date'] = pd.to_datetime(account_value_df['date']).dt.date\n",
    "    account_value_daily = account_value_df.groupby('date')['account_value'].last().reset_index()\n",
    "\n",
    "    account_value_daily['daily_return'] = account_value_daily['account_value'].pct_change(1)\n",
    "\n",
    "    if account_value_daily['daily_return'].std() > 1e-8:\n",
    "        sharpe_ratio = (252**0.5) * account_value_daily['daily_return'].mean() / account_value_daily['daily_return'].std()\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "    \n",
    "    loss = optuna_calculate_loss(account_value_daily)\n",
    "\n",
    "    return loss, sharpe_ratio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Optuna Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"gp1_trials\"\n",
    "directions = [\"minimize\", \"maximize\"]\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, directions=directions)\n",
    "\n",
    "# timeout=3600 is 1 hr, 86400 is 24hrs or n_trials=100\n",
    "study.optimize(optuna_objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Pareto front trials\n",
    "pareto_front_trials = study.best_trials\n",
    "\n",
    "# Print Pareto front trials\n",
    "print(f\"Number of trials on the Pareto front: {len(study.best_trials)}\")\n",
    "for trial in pareto_front_trials:\n",
    "    print(f\"  Trial {trial.number}:\")\n",
    "    print(\"    Value: \", trial.values)\n",
    "    print(\"    Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"      {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_pareto_front(study, target_names=[\"loss\", \"sharpe_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Visualization\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "pareto_front = optuna.visualization.plot_pareto_front(study, target_names=[\"loss\", \"sharpe_ratio\"])\n",
    "\n",
    "pio.write_html(pareto_front, file='pareto_front.html', auto_open=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = study.get_trials()\n",
    "\n",
    "def trial_filter(trial):\n",
    "    if trial.values is None:\n",
    "        return False\n",
    "    \n",
    "    # order of objectives:\n",
    "    # 0: loss (minimize)\n",
    "    # 1: sharpe ratio (maximize)\n",
    "    # 2: number of trades (minimize)\n",
    "    \n",
    "    # Set thresholds for each objective\n",
    "    loss_threshold = 0\n",
    "    sharpe_ratio_threshold = 0\n",
    "    # num_trades_threshold = 100\n",
    "\n",
    "    if (trial.values[0] == loss_threshold and \n",
    "        trial.values[1] == sharpe_ratio_threshold):\n",
    "        # trial.values[2] >= num_trades_threshold\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "filtered_trials = [t for t in trials if trial_filter(t)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered Trials Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for min, max, and sum\n",
    "min_learning_rate = float('inf')\n",
    "max_learning_rate = float('-inf')\n",
    "sum_learning_rate = 0\n",
    "\n",
    "min_weight_decay = float('inf')\n",
    "max_weight_decay = float('-inf')\n",
    "sum_weight_decay = 0\n",
    "\n",
    "min_lambda_entropy = float('inf')\n",
    "max_lambda_entropy = float('-inf')\n",
    "sum_lambda_entropy = 0\n",
    "\n",
    "# Iterate through the filtered trials and calculate the required statistics\n",
    "num_trials = len(filtered_trials)\n",
    "\n",
    "for trial in filtered_trials:\n",
    "    learning_rate = trial.params['learning_rate']\n",
    "    weight_decay = trial.params['weight_decay']\n",
    "    lambda_entropy = trial.params['lambda_entropy']\n",
    "\n",
    "    min_learning_rate = min(min_learning_rate, learning_rate)\n",
    "    max_learning_rate = max(max_learning_rate, learning_rate)\n",
    "    sum_learning_rate += learning_rate\n",
    "\n",
    "    min_weight_decay = min(min_weight_decay, weight_decay)\n",
    "    max_weight_decay = max(max_weight_decay, weight_decay)\n",
    "    sum_weight_decay += weight_decay\n",
    "\n",
    "    min_lambda_entropy = min(min_lambda_entropy, lambda_entropy)\n",
    "    max_lambda_entropy = max(max_lambda_entropy, lambda_entropy)\n",
    "    sum_lambda_entropy += lambda_entropy\n",
    "\n",
    "# Calculate averages\n",
    "avg_learning_rate = sum_learning_rate / num_trials\n",
    "avg_weight_decay = sum_weight_decay / num_trials\n",
    "avg_lambda_entropy = sum_lambda_entropy / num_trials\n",
    "\n",
    "# Print the results\n",
    "print(\"Learning rate: min={}, max={}, avg={}\".format(min_learning_rate, max_learning_rate, avg_learning_rate))\n",
    "print(\"Weight decay: min={}, max={}, avg={}\".format(min_weight_decay, max_weight_decay, avg_weight_decay))\n",
    "print(\"Lambda entropy: min={}, max={}, avg={}\".format(min_lambda_entropy, max_lambda_entropy, avg_lambda_entropy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gauntlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.rrule import rrule, MONTHLY\n",
    "from dateutil.parser import parse\n",
    "from calendar import monthrange\n",
    "\n",
    "import optuna\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "env = StockTradingEnv\n",
    "\n",
    "TRAIN_START_DATE = \"2016-01-01\"\n",
    "TRAIN_END_DATE = \"2016-12-31\"\n",
    "TEST_START_DATE = \"2017-01-01\"\n",
    "TEST_END_DATE = \"2017-01-31\"\n",
    "END_DATE = '2023-03-31'\n",
    "\n",
    "def optuna_train(\n",
    "    trial,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Create 'train_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"train_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'train_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "        \n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": True,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # read parameters\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "\n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        break_step = kwargs.get(\"break_step\", 1e6)\n",
    "        erl_params = kwargs.get(\"erl_params\")\n",
    "        agent = DRLAgent_erl(\n",
    "            env=env,\n",
    "            price_array=price_array,\n",
    "            tech_array=tech_array,\n",
    "            turbulence_array=turbulence_array,\n",
    "            date_array=date_array,\n",
    "        )\n",
    "        model = agent.get_model(model_name, model_kwargs=erl_params)\n",
    "        trained_model = agent.train_model(\n",
    "            model=model, cwd=cwd, total_timesteps=break_step\n",
    "        )\n",
    "\n",
    "def optuna_test(\n",
    "    trial,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    drl_lib,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Create 'test_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"test_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'test_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "            \n",
    "    env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": False,\n",
    "    }\n",
    "    env_instance = env(config=env_config)\n",
    "\n",
    "    # load elegantrl needs state dim, action dim and net dim\n",
    "    net_dimension = kwargs.get(\"net_dimension\")\n",
    "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
    "    print(\"price_array: \", len(price_array))\n",
    "\n",
    "    if drl_lib == \"elegantrl\":\n",
    "        DRLAgent_erl = DRLAgent\n",
    "        episode_dates, episode_total_assets, episode_return, num_trades = DRLAgent_erl.DRL_prediction(\n",
    "            model_name=model_name,\n",
    "            cwd=cwd,\n",
    "            net_dimension=net_dimension,\n",
    "            environment=env_instance,\n",
    "        )\n",
    "        \n",
    "        print(f\"Number of trades made: {num_trades}\")\n",
    "        episode_dates = np.unique(np.concatenate(episode_dates))\n",
    "        episode_dates = np.concatenate(([episode_dates[0]], episode_dates))\n",
    "        episode_dates = [timestamp.date() for timestamp in episode_dates]\n",
    "        account_value_df = pd.DataFrame({'date': episode_dates, 'account_value': episode_total_assets})\n",
    "        return account_value_df, episode_return, num_trades\n",
    "\n",
    "\n",
    "def optuna_calculate_loss(account_value_daily, w1=0.6, w2=0.4):\n",
    "    pnl = account_value_daily['account_value'].iloc[-1] - account_value_daily['account_value'].iloc[0]\n",
    "    neg_pnl = -pnl\n",
    "\n",
    "    running_max = np.maximum.accumulate(account_value_daily['account_value'])\n",
    "    drawdowns = (running_max - account_value_daily['account_value']) / running_max\n",
    "    max_drawdown = np.max(drawdowns)\n",
    "\n",
    "    loss = w1 * neg_pnl + w2 * max_drawdown\n",
    "    return loss\n",
    "\n",
    "def optuna_gauntlet_objective(trial):\n",
    "    returns_threshold = 0.9\n",
    "    monthly_returns = []\n",
    "    monthly_loss = []\n",
    "    all_test_daily_returns = pd.DataFrame()\n",
    "    train_start_date = datetime.strptime(TRAIN_START_DATE, '%Y-%m-%d').date()\n",
    "    train_end_date = datetime.strptime(TRAIN_END_DATE, '%Y-%m-%d').date()\n",
    "    test_start_date = datetime.strptime(TEST_START_DATE, '%Y-%m-%d').date()\n",
    "    test_end_date = datetime.strptime(TEST_END_DATE, '%Y-%m-%d').date()\n",
    "    end_date = datetime.strptime(END_DATE, '%Y-%m-%d').date()\n",
    "    \n",
    "    while train_end_date <= end_date:\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-7, 1e-3, log=True)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 0.20, log=True)\n",
    "        lambda_entropy = trial.suggest_float(\"lambda_entropy\", 1e-5, 0.20, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [512, 1024, 2048, 4096])\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [0.985, 0.99, 0.995])\n",
    "        net_dimension = []\n",
    "        n_hidden_layers = trial.suggest_int(\"n_hidden_layers\", 1, 5)\n",
    "        possible_layer_sizes = [8, 16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "        for i in range(n_hidden_layers):\n",
    "            layer_size = trial.suggest_categorical(f\"hidden_layer_{i}\", possible_layer_sizes)\n",
    "            net_dimension.append(layer_size)\n",
    "        target_step = 5000\n",
    "        eval_gap = 30\n",
    "        eval_times = 1\n",
    "        erl_params = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"lambda_entropy\": lambda_entropy,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"gamma\": gamma,\n",
    "            \"seed\": 999,\n",
    "            \"net_dimension\": net_dimension,\n",
    "            \"target_step\": target_step,\n",
    "            \"eval_gap\": eval_gap,\n",
    "            \"eval_times\": eval_times,\n",
    "        }\n",
    "        \n",
    "        optuna_train(\n",
    "            trial,\n",
    "            start_date=train_start_date.strftime('%Y-%m-%d'),\n",
    "            end_date=train_end_date.strftime('%Y-%m-%d'),\n",
    "            ticker_list = ticker_list, \n",
    "            data_source = 'alpaca',\n",
    "            time_interval= \"15Min\", \n",
    "            technical_indicator_list= INDICATORS,\n",
    "            drl_lib='elegantrl', \n",
    "            env=env,\n",
    "            model_name='ppo',\n",
    "            if_vix=True,\n",
    "            if_cdl=True, \n",
    "            API_KEY = ALPACA_API_KEY, \n",
    "            API_SECRET = ALPACA_API_SECRET, \n",
    "            API_BASE_URL = API_BASE_URL,\n",
    "            cwd='./gp1_testing', #current_working_dir\n",
    "            break_step=1e6,\n",
    "            erl_params=erl_params\n",
    "        )\n",
    "\n",
    "        account_value_df, episode_return, _ = optuna_test(\n",
    "            trial,\n",
    "            start_date=test_start_date.strftime('%Y-%m-%d'),\n",
    "            end_date=test_end_date.strftime('%Y-%m-%d'),\n",
    "            ticker_list = ticker_list, \n",
    "            data_source = 'alpaca',\n",
    "            time_interval= \"15Min\", \n",
    "            technical_indicator_list= INDICATORS,\n",
    "            drl_lib='elegantrl', \n",
    "            env=env, \n",
    "            model_name='ppo',\n",
    "            if_vix=True,\n",
    "            if_cdl=True,\n",
    "            API_KEY = ALPACA_API_KEY, \n",
    "            API_SECRET = ALPACA_API_SECRET, \n",
    "            API_BASE_URL = API_BASE_URL,\n",
    "            cwd='./gp1_testing',\n",
    "            net_dimension = erl_params['net_dimension'],\n",
    "        )\n",
    "        # Stats\n",
    "        account_value_df['date'] = pd.to_datetime(account_value_df['date']).dt.date\n",
    "        account_value_daily = account_value_df.groupby('date')['account_value'].last().reset_index()\n",
    "        account_value_daily['daily_return'] = account_value_daily['account_value'].pct_change(1)\n",
    "\n",
    "        monthly_return = episode_return\n",
    "        \n",
    "        # Check if all thresholds are met\n",
    "        if (\n",
    "            monthly_return >= returns_threshold\n",
    "        ):\n",
    "            # Record Stats\n",
    "            loss = optuna_calculate_loss(account_value_daily)\n",
    "            monthly_returns.append(monthly_return)\n",
    "            monthly_loss.append(loss)\n",
    "            all_test_daily_returns = pd.concat([all_test_daily_returns, account_value_daily[['date', 'daily_return']]], ignore_index=True)\n",
    "            \n",
    "            # Move the date range one month forward\n",
    "            train_start_date = test_start_date\n",
    "            train_end_date = test_end_date\n",
    "            test_start_date = list(rrule(freq=MONTHLY, dtstart=parse(test_start_date.strftime('%Y-%m-%d')), count=2))[-1].date()\n",
    "            _, last_day_of_month = monthrange(test_start_date.year, test_start_date.month)\n",
    "            test_end_date = test_start_date.replace(day=last_day_of_month)\n",
    "        else:\n",
    "            # Delete the model if saved to disk\n",
    "            if os.path.exists(\"./gp1_testing/actor.pth\"):\n",
    "                os.remove(\"./gp1_testing/actor.pth\")\n",
    "            # Print the hyperparameters\n",
    "            print(f\"Trial {trial.number} pruned. Hyperparameters: learning_rate={learning_rate}, weight_decay={weight_decay}, lambda_entropy={lambda_entropy}, net_dimension={net_dimension}\")\n",
    "            # Cancel Trial\n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "    total_trading_days = len(all_test_daily_returns)\n",
    "    annualized_factor = np.sqrt(total_trading_days)\n",
    "    sharpe_ratio = annualized_factor * all_test_daily_returns['daily_return'].mean() / all_test_daily_returns['daily_return'].std()\n",
    "    cumulative_monthly_loss = np.sum(monthly_loss)\n",
    "    cumulative_monthly_return = np.prod(monthly_returns)\n",
    "    \n",
    "    # Move and rename the model file\n",
    "    src_file = \"./gp1_testing/actor.pth\"\n",
    "    dst_dir = \"./gp1_gauntlet\"\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    dst_file = os.path.join(dst_dir, f\"actor_trial_{trial.number}_return_{format(round(cumulative_monthly_return, 2), '.2f').replace('.', '_')}.pth\")\n",
    "    shutil.move(src_file, dst_file)\n",
    "    \n",
    "    # Get the list of all actor.pth files in the gp1_gauntlet folder\n",
    "    files = glob.glob(f\"{dst_dir}/actor_trial_*.pth\")\n",
    "\n",
    "    # Extract the average monthly return value from the filenames and sort the files by their return values\n",
    "    files_with_returns = [(f, float(re.findall(r\"_return_([\\d_]+)\", f)[0].replace('_', '.'))) for f in files]\n",
    "    files_with_returns.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Delete any files that are not in the top 10\n",
    "    for i in range(10, len(files_with_returns)):\n",
    "        os.remove(files_with_returns[i][0])\n",
    "    \n",
    "    return cumulative_monthly_loss, sharpe_ratio, cumulative_monthly_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"gp1_gauntlet\"\n",
    "directions = [\"minimize\", \"maximize\", \"maximize\"]\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, directions=directions)\n",
    "\n",
    "# timeout=3600 is 1 hr, 86400 is 24hrs or n_trials=100\n",
    "study.optimize(optuna_gauntlet_objective, timeout=(3600 * 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_pareto_front(study, target_names=[\"cumulative_monthly_loss\", \"sharpe_ratio\", \"cumulative_monthly_return\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Visualization\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "pareto_front = optuna.visualization.plot_pareto_front(study, target_names=[\"cumulative_monthly_loss\", \"sharpe_ratio\", \"cumulative_monthly_return\"])\n",
    "\n",
    "pio.write_html(pareto_front, file='pareto_front.html', auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_number = 79  # Choose a specific trial number.\n",
    "# specific_trial = [trial for trial in study.trials if trial.number == trial_number][0]\n",
    "# specific_hyperparameters = specific_trial.params\n",
    "# print(f\"Hyperparameters of trial number {trial_number}:\", specific_hyperparameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERL_PARAMS = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"batch_size\": 2048,\n",
    "    \"gamma\":  0.985,\n",
    "    \"seed\":42,\n",
    "    \"net_dimension\":[128, 256], \n",
    "    \"target_step\":5000, \n",
    "    \"eval_gap\":50,\n",
    "    \"eval_times\":1\n",
    "}\n",
    "env = StockTradingEnv\n",
    "\n",
    "TEST_START_DATE = \"2023-01-02\"\n",
    "TEST_END_DATE = \"2023-03-31\"\n",
    "\n",
    "gp1_account_value_df=test(start_date = TEST_START_DATE, \n",
    "                    end_date = TEST_END_DATE,\n",
    "                    ticker_list = ticker_list, \n",
    "                    data_source = 'alpaca',\n",
    "                    time_interval= '1Min', \n",
    "                    technical_indicator_list= INDICATORS,\n",
    "                    drl_lib='elegantrl', \n",
    "                    env=env, \n",
    "                    model_name='ppo',\n",
    "                    if_vix=True,\n",
    "                    if_cdl=True,\n",
    "                    API_KEY = ALPACA_API_KEY, \n",
    "                    API_SECRET = ALPACA_API_SECRET, \n",
    "                    API_BASE_URL = API_BASE_URL,\n",
    "                    cwd='./gp1_trained',\n",
    "                    net_dimension = ERL_PARAMS['net_dimension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==============Get Baseline Stats===========\")\n",
    "\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^GSPC\", \n",
    "        start = TEST_START_DATE,\n",
    "        end = TEST_END_DATE)\n",
    "\n",
    "stats = backtest_stats(baseline_df, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==============GP1 Stats===========\")\n",
    "\n",
    "stats = backtest_stats(gp1_account_value_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "gp1_account_value_df = drop_dup_dates(gp1_account_value_df)\n",
    "\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "\n",
    "backtest_plot(gp1_account_value_df, \n",
    "             baseline_ticker = '^GSPC', \n",
    "             baseline_start = TEST_START_DATE,\n",
    "             baseline_end = TEST_END_DATE,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sIQN6Ggt7gXY"
   },
   "source": [
    "# Part 4: Deploy the Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UFoxkigg1zXa"
   },
   "source": [
    "## Setup Alpaca Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpkoZpYzfneS"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import threading\n",
    "from processor_alpaca import AlpacaProcessor\n",
    "import alpaca_trade_api as tradeapi\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "\n",
    "class AlpacaPaperTrading():\n",
    "\n",
    "    def __init__(self,ticker_list, time_interval, drl_lib, agent, cwd, net_dim, \n",
    "                 state_dim, action_dim, API_KEY, API_SECRET, \n",
    "                 API_BASE_URL, tech_indicator_list, turbulence_thresh=99, \n",
    "                 max_stock=1e2, latency = None):\n",
    "        #load agent\n",
    "        self.drl_lib = drl_lib\n",
    "        if agent =='ppo':\n",
    "            if drl_lib == 'elegantrl':              \n",
    "                agent_class = AgentPPO\n",
    "                agent = agent_class(net_dim, state_dim, action_dim)\n",
    "                actor = agent.act\n",
    "                # load agent\n",
    "                try:  \n",
    "                    cwd = cwd + '/actor.pth'\n",
    "                    print(f\"| load actor from: {cwd}\")\n",
    "                    actor.load_state_dict(torch.load(cwd, map_location=lambda storage, loc: storage))\n",
    "                    self.act = actor\n",
    "                    self.device = agent.device\n",
    "                except BaseException:\n",
    "                    raise ValueError(\"Fail to load agent!\")\n",
    "                    \n",
    "            else:\n",
    "                raise ValueError('The DRL library input is NOT supported yet. Please check your input.')\n",
    "               \n",
    "        else:\n",
    "            raise ValueError('Agent input is NOT supported yet.')\n",
    "            \n",
    "            \n",
    "            \n",
    "        #connect to Alpaca trading API\n",
    "        try:\n",
    "            self.alpaca = tradeapi.REST(API_KEY,API_SECRET,API_BASE_URL, 'v2')\n",
    "        except:\n",
    "            raise ValueError('Fail to connect Alpaca. Please check account info and internet connection.')\n",
    "        \n",
    "        #read trading time interval\n",
    "        if time_interval == '1s':\n",
    "            self.time_interval = 1\n",
    "        elif time_interval == '5s':\n",
    "            self.time_interval = 5\n",
    "        elif time_interval == '1Min':\n",
    "            self.time_interval = 60\n",
    "        elif time_interval == '5Min':\n",
    "            self.time_interval = 60 * 5\n",
    "        elif time_interval == '15Min':\n",
    "            self.time_interval = 60 * 15\n",
    "        else:\n",
    "            raise ValueError('Time interval input is NOT supported yet.')\n",
    "        \n",
    "        #read trading settings\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.turbulence_thresh = turbulence_thresh\n",
    "        self.max_stock = max_stock \n",
    "        \n",
    "        #initialize account\n",
    "        self.stocks = np.asarray([0] * len(ticker_list)) #stocks holding\n",
    "        self.stocks_cd = np.zeros_like(self.stocks) \n",
    "        self.cash = None #cash record \n",
    "        self.stocks_df = pd.DataFrame(self.stocks, columns=['stocks'], index = ticker_list)\n",
    "        self.assets_list = []\n",
    "        self.price = np.asarray([0] * len(ticker_list))\n",
    "        self.stockUniverse = ticker_list\n",
    "        self.turbulence_bool = 0\n",
    "        self.equities = []\n",
    "        \n",
    "    def test_latency(self, test_times = 10): \n",
    "        total_time = 0\n",
    "        for i in range(0, test_times):\n",
    "            time0 = time.time()\n",
    "            self.get_state()\n",
    "            time1 = time.time()\n",
    "            temp_time = time1 - time0\n",
    "            total_time += temp_time\n",
    "        latency = total_time/test_times\n",
    "        print('latency for data processing: ', latency)\n",
    "        return latency\n",
    "        \n",
    "    def run(self):\n",
    "        orders = self.alpaca.list_orders(status=\"open\")\n",
    "        for order in orders:\n",
    "          self.alpaca.cancel_order(order.id)\n",
    "    \n",
    "        # Wait for market to open.\n",
    "        print(\"Waiting for market to open...\")\n",
    "        self.awaitMarketOpen()\n",
    "        print(\"Market opened.\")\n",
    "\n",
    "        while True:\n",
    "\n",
    "          # Figure out when the market will close so we can prepare to sell beforehand.\n",
    "          clock = self.alpaca.get_clock()\n",
    "          closingTime = clock.next_close.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
    "          currTime = clock.timestamp.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
    "          self.timeToClose = closingTime - currTime\n",
    "    \n",
    "          if(self.timeToClose < (60)):\n",
    "            # Close all positions when 1 minutes til market close.\n",
    "            print(\"Market closing soon. Stop trading.\")\n",
    "            break\n",
    "            \n",
    "            '''# Close all positions when 1 minutes til market close.\n",
    "            print(\"Market closing soon.  Closing positions.\")\n",
    "\n",
    "            threads = []\n",
    "            positions = self.alpaca.list_positions()\n",
    "            for position in positions:\n",
    "              if(position.side == 'long'):\n",
    "                orderSide = 'sell'\n",
    "              else:\n",
    "                orderSide = 'buy'\n",
    "              qty = abs(int(float(position.qty)))\n",
    "              respSO = []\n",
    "              tSubmitOrder = threading.Thread(target=self.submitOrder(qty, position.symbol, orderSide, respSO))\n",
    "              tSubmitOrder.start()\n",
    "              threads.append(tSubmitOrder)    # record thread for joining later\n",
    "\n",
    "            for x in threads:   #  wait for all threads to complete\n",
    "                x.join()     \n",
    "            # Run script again after market close for next trading day.\n",
    "            print(\"Sleeping until market close (15 minutes).\")\n",
    "            time.sleep(60 * 15)'''\n",
    "            \n",
    "          else:\n",
    "            self.trade()\n",
    "            last_equity = float(self.alpaca.get_account().last_equity)\n",
    "            cur_time = time.time()\n",
    "            self.equities.append([cur_time,last_equity])\n",
    "            time.sleep(self.time_interval)\n",
    "            \n",
    "    def awaitMarketOpen(self):\n",
    "        isOpen = self.alpaca.get_clock().is_open\n",
    "        while(not isOpen):\n",
    "          clock = self.alpaca.get_clock()\n",
    "          openingTime = clock.next_open.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
    "          currTime = clock.timestamp.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
    "          timeToOpen = int((openingTime - currTime) / 60)\n",
    "          print(str(timeToOpen) + \" minutes til market open.\")\n",
    "          time.sleep(60)\n",
    "          isOpen = self.alpaca.get_clock().is_open\n",
    "    \n",
    "    def trade(self):\n",
    "        state = self.get_state()\n",
    "        \n",
    "        if self.drl_lib == 'elegantrl':\n",
    "            with torch.no_grad():\n",
    "                s_tensor = torch.as_tensor((state,), device=self.device)\n",
    "                a_tensor = self.act(s_tensor)  \n",
    "                action = a_tensor.detach().cpu().numpy()[0]  \n",
    "            action = (action * self.max_stock).astype(int)\n",
    "            \n",
    "        elif self.drl_lib == 'rllib':\n",
    "            action = self.agent.compute_single_action(state)\n",
    "        \n",
    "        elif self.drl_lib == 'stable_baselines3':\n",
    "            action = self.model.predict(state)[0]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('The DRL library input is NOT supported yet. Please check your input.')\n",
    "        \n",
    "        self.stocks_cd += 1\n",
    "        if self.turbulence_bool == 0:\n",
    "            min_action = 10  # stock_cd\n",
    "            threads = []\n",
    "            for index in np.where(action < -min_action)[0]:  # sell_index:\n",
    "                sell_num_shares = min(self.stocks[index], -action[index])\n",
    "                qty =  abs(int(sell_num_shares))\n",
    "                respSO = []\n",
    "                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, self.stockUniverse[index], 'sell', respSO))\n",
    "                tSubmitOrder.start()\n",
    "                threads.append(tSubmitOrder)    # record thread for joining later\n",
    "                self.cash = float(self.alpaca.get_account().cash)\n",
    "                self.stocks_cd[index] = 0\n",
    "            \n",
    "            for x in threads:   #  wait for all threads to complete\n",
    "                x.join()     \n",
    "\n",
    "            threads = []\n",
    "            for index in np.where(action > min_action)[0]:  # buy_index:\n",
    "                if self.cash < 0:\n",
    "                    tmp_cash = 0\n",
    "                else:\n",
    "                    tmp_cash = self.cash\n",
    "                buy_num_shares = min(tmp_cash // self.price[index], abs(int(action[index])))\n",
    "                if (buy_num_shares != buy_num_shares): # if buy_num_change = nan\n",
    "                    qty = 0 # set to 0 quantity\n",
    "                else:\n",
    "                    qty = abs(int(buy_num_shares))\n",
    "                qty = abs(int(buy_num_shares))\n",
    "                respSO = []\n",
    "                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, self.stockUniverse[index], 'buy', respSO))\n",
    "                tSubmitOrder.start()\n",
    "                threads.append(tSubmitOrder)    # record thread for joining later\n",
    "                self.cash = float(self.alpaca.get_account().cash)\n",
    "                self.stocks_cd[index] = 0\n",
    "\n",
    "            for x in threads:   #  wait for all threads to complete\n",
    "                x.join()     \n",
    "                \n",
    "        else:  # sell all when turbulence\n",
    "            threads = []\n",
    "            positions = self.alpaca.list_positions()\n",
    "            for position in positions:\n",
    "                if(position.side == 'long'):\n",
    "                    orderSide = 'sell'\n",
    "                else:\n",
    "                    orderSide = 'buy'\n",
    "                qty = abs(int(float(position.qty)))\n",
    "                respSO = []\n",
    "                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, position.symbol, orderSide, respSO))\n",
    "                tSubmitOrder.start()\n",
    "                threads.append(tSubmitOrder)    # record thread for joining later\n",
    "\n",
    "            for x in threads:   #  wait for all threads to complete\n",
    "                x.join()     \n",
    "            \n",
    "            self.stocks_cd[:] = 0\n",
    "            \n",
    "    \n",
    "    def get_state(self):\n",
    "        alpaca = AlpacaProcessor(api=self.alpaca)\n",
    "        price, tech, turbulence = alpaca.fetch_latest_data(ticker_list = self.stockUniverse, time_interval='1Min',\n",
    "                                                     tech_indicator_list=self.tech_indicator_list)\n",
    "        turbulence_bool = 1 if turbulence >= self.turbulence_thresh else 0\n",
    "        \n",
    "        turbulence = (self.sigmoid_sign(turbulence, self.turbulence_thresh) * 2 ** -5).astype(np.float32)\n",
    "        \n",
    "        tech = tech * 2 ** -7\n",
    "        positions = self.alpaca.list_positions()\n",
    "        stocks = [0] * len(self.stockUniverse)\n",
    "        for position in positions:\n",
    "            ind = self.stockUniverse.index(position.symbol)\n",
    "            stocks[ind] = ( abs(int(float(position.qty))))\n",
    "        \n",
    "        stocks = np.asarray(stocks, dtype = float)\n",
    "        cash = float(self.alpaca.get_account().cash)\n",
    "        self.cash = cash\n",
    "        self.stocks = stocks\n",
    "        self.turbulence_bool = turbulence_bool \n",
    "        self.price = price\n",
    "        \n",
    "        \n",
    "        \n",
    "        amount = np.array(self.cash * (2 ** -12), dtype=np.float32)\n",
    "        scale = np.array(2 ** -6, dtype=np.float32)\n",
    "        state = np.hstack((amount,\n",
    "                    turbulence,\n",
    "                    self.turbulence_bool,\n",
    "                    price * scale,\n",
    "                    self.stocks * scale,\n",
    "                    self.stocks_cd,\n",
    "                    tech,\n",
    "                    )).astype(np.float32)\n",
    "        state[np.isnan(state)] = 0.0\n",
    "        state[np.isinf(state)] = 0.0\n",
    "        print(len(self.stockUniverse))\n",
    "        return state\n",
    "        \n",
    "    def submitOrder(self, qty, stock, side, resp):\n",
    "        if(qty > 0):\n",
    "          try:\n",
    "            self.alpaca.submit_order(stock, qty, side, \"market\", \"day\")\n",
    "            print(\"Market order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | completed.\")\n",
    "            resp.append(True)\n",
    "          except:\n",
    "            print(\"Order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | did not go through.\")\n",
    "            resp.append(False)\n",
    "        else:\n",
    "          print(\"Quantity is 0, order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | not completed.\")\n",
    "          resp.append(True)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_sign(ary, thresh):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x * np.e)) - 0.5\n",
    "\n",
    "        return sigmoid(ary / thresh) * thresh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "os4C4-4H7ns7"
   },
   "source": [
    "## Run Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7nw0i-0UN3-7",
    "outputId": "25729df7-4775-49af-bf5a-38e3970d0056"
   },
   "outputs": [],
   "source": [
    "print(GP1_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YsSBK9ION1t6",
    "outputId": "49a69655-850f-436b-a21c-fffe48528e71"
   },
   "outputs": [],
   "source": [
    "state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYtSv6P1N247",
    "outputId": "174550ce-664a-41fc-bd89-9d3726960c5b"
   },
   "outputs": [],
   "source": [
    "action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERL_PARAMS = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"batch_size\": 2048,\n",
    "    \"gamma\":  0.985,\n",
    "    \"seed\":42,\n",
    "    \"net_dimension\":[128, 256], \n",
    "    \"target_step\":5000, \n",
    "    \"eval_gap\":50,\n",
    "    \"eval_times\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kl9nulnAJtiI"
   },
   "outputs": [],
   "source": [
    "trading_erl = AlpacaPaperTrading(ticker_list = GP1_TICKER, \n",
    "                                       time_interval = '1Min', \n",
    "                                       drl_lib = 'elegantrl', \n",
    "                                       agent = 'ppo', \n",
    "                                       cwd = './gp1_trained', \n",
    "                                       net_dim = ERL_PARAMS['net_dimension'], \n",
    "                                       state_dim = state_dim, \n",
    "                                       action_dim= action_dim, \n",
    "                                       API_KEY = ALPACA_API_KEY, \n",
    "                                       API_SECRET = ALPACA_API_SECRET, \n",
    "                                       API_BASE_URL = API_BASE_URL, \n",
    "                                       tech_indicator_list = INDICATORS,\n",
    "                                       max_stock=1e2, turbulence_thresh=99)\n",
    "trading_erl.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "srzBZfYEUI1O"
   },
   "source": [
    "# Part 5: Check Portfolio Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chovN1UhTAht"
   },
   "outputs": [],
   "source": [
    "import alpaca_trade_api as tradeapi\n",
    "import exchange_calendars as tc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import yfinance as yf\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime as dt\n",
    "from plot import backtest_stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaofxMNCfAR1"
   },
   "outputs": [],
   "source": [
    "def get_trading_days(start, end):\n",
    "    nyse = tc.get_calendar('NYSE')\n",
    "    df = nyse.sessions_in_range(pd.Timestamp(start,tz=pytz.UTC),\n",
    "                                pd.Timestamp(end,tz=pytz.UTC))\n",
    "    trading_days = []\n",
    "    for day in df:\n",
    "        trading_days.append(str(day)[:10])\n",
    "\n",
    "    return trading_days\n",
    "\n",
    "def alpaca_history(key, secret, url, start, end):\n",
    "    api = tradeapi.REST(key, secret, url, 'v2')\n",
    "    trading_days = get_trading_days(start, end)\n",
    "    df = pd.DataFrame()\n",
    "    for day in trading_days:\n",
    "        df = df.append(api.get_portfolio_history(date_start = day,timeframe='5Min').df.iloc[:78])\n",
    "    equities = df.equity.values\n",
    "    cumu_returns = equities/equities[0]\n",
    "    cumu_returns = cumu_returns[~np.isnan(cumu_returns)]\n",
    "    \n",
    "    return df, cumu_returns\n",
    "\n",
    "def SP500_history(start):\n",
    "    data_df = yf.download(['^GSPC'],start=start, interval=\"5m\")\n",
    "    data_df = data_df.iloc[:]\n",
    "    baseline_returns = data_df['Adj Close'].values/data_df['Adj Close'].values[0]\n",
    "    return data_df, baseline_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CHiZRVpURpx"
   },
   "source": [
    "## Get cumulative return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_YT7v-LSdfV"
   },
   "outputs": [],
   "source": [
    "df_erl, cumu_erl = alpaca_history(key=ALPACA_API_KEY, \n",
    "                                  secret=ALPACA_API_SECRET, \n",
    "                                  url=API_BASE_URL, \n",
    "                                  start='2023-03-24', #must be within 1 month\n",
    "                                  end='2023-03-24') #change the date if error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMcQjwHOS6Zb",
    "outputId": "1fb21460-1da9-4998-f0c0-fcbf5b056e66"
   },
   "outputs": [],
   "source": [
    "df_sp500, cumu_sp500 = SP500_history(start='2023-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "PJXPwmx9Ts5o",
    "outputId": "c59014eb-c2f9-4be2-8a87-7892cc0b1094"
   },
   "outputs": [],
   "source": [
    "df_erl.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1Iaw90FTNfU",
    "outputId": "0629dca2-d9dd-4c2a-e363-dc0f01daba41"
   },
   "outputs": [],
   "source": [
    "returns_erl = cumu_erl -1 \n",
    "returns_sp500 = cumu_sp500 - 1\n",
    "returns_sp500 = returns_sp500[:returns_erl.shape[0]]\n",
    "print('len of erl return: ', returns_erl.shape[0])\n",
    "print('len of sp500 return: ', returns_sp500.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IawaMsDwZni"
   },
   "outputs": [],
   "source": [
    "returns_erl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z0LEm7KUZ5W"
   },
   "source": [
    "## plot and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Foqk1wIQTQJ3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(dpi=1000)\n",
    "plt.grid()\n",
    "plt.grid(which='minor', axis='y')\n",
    "plt.title('Stock Trading (Paper trading)', fontsize=20)\n",
    "plt.plot(returns_erl, label = 'ElegantRL Agent', color = 'red')\n",
    "#plt.plot(returns_sb3, label = 'Stable-Baselines3 Agent', color = 'blue' )\n",
    "#plt.plot(returns_rllib, label = 'RLlib Agent', color = 'green')\n",
    "plt.plot(returns_sp500, label = 'S&P 500', color = 'grey')\n",
    "plt.ylabel('Return', fontsize=16)\n",
    "plt.xlabel('Year 2023', fontsize=16)\n",
    "plt.xticks(size = 14)\n",
    "plt.yticks(size = 14)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(78))\n",
    "ax.xaxis.set_minor_locator(ticker.MultipleLocator(6))\n",
    "ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.005))\n",
    "ax.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1, decimals=2))\n",
    "ax.xaxis.set_major_formatter(ticker.FixedFormatter(['','10-19','','10-20',\n",
    "                                                    '','10-21','','10-22']))\n",
    "plt.legend(fontsize=10.5)\n",
    "plt.savefig('papertrading_stock.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0EVJIQUR6_fu",
    "9tzAw9k26nAC",
    "zjLda8No6pvI",
    "pf5aVHAU-xF6",
    "rZMkcyjZ-25l",
    "3rwy7V72-8YY",
    "J25MuZLiGqCP",
    "eW0UDAXI1nEa",
    "UFoxkigg1zXa"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "005d14239094016f48a03a57365c4ccb734e3f38c20ed0ca595d84f773bc39cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
