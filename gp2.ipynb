{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP2 Trading Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processor import DataProcessor\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_private import ALPACA_API_KEY, ALPACA_API_SECRET\n",
    "API_BASE_URL = 'https://paper-api.alpaca.markets'\n",
    "from config_tickers import GP2_TICKERS\n",
    "from config import INDICATORS\n",
    "from config import CDL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from numpy import random as rd\n",
    "\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        gamma=0.99,\n",
    "        turbulence_thresh=99,\n",
    "        max_stock=None,\n",
    "        min_stock_rate=0.1,\n",
    "        initial_capital=1e5,\n",
    "        reward_scaling=2**-11,\n",
    "        initial_stocks=None,\n",
    "    ):\n",
    "        price_ary = config[\"price_array\"]\n",
    "        tech_ary = config[\"tech_array\"]\n",
    "        turbulence_ary = config[\"turbulence_array\"]\n",
    "        date_ary = config[\"date_array\"]\n",
    "        if_train = config[\"if_train\"]\n",
    "        self.price_ary = price_ary.astype(np.float32)\n",
    "        self.tech_ary = tech_ary.astype(np.float32)\n",
    "        self.turbulence_ary = turbulence_ary\n",
    "        self.date_ary = date_ary\n",
    "\n",
    "        self.tech_ary = self.tech_ary * 2**-7\n",
    "        self.turbulence_bool = (turbulence_ary > turbulence_thresh).astype(np.float32)\n",
    "        self.turbulence_ary = (\n",
    "            self.sigmoid_sign(turbulence_ary, turbulence_thresh) * 2**-5\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        stock_dim = self.price_ary.shape[1]\n",
    "        self.gamma = gamma\n",
    "        self.max_stock = max_stock\n",
    "        self.min_stock_rate = min_stock_rate\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.initial_capital = initial_capital\n",
    "        self.initial_stocks = (\n",
    "            np.zeros(stock_dim, dtype=np.float32)\n",
    "            if initial_stocks is None\n",
    "            else initial_stocks\n",
    "        )\n",
    "        \n",
    "        # reset()\n",
    "        self.current_step = None\n",
    "        self.num_trades = None\n",
    "        self.cash = None\n",
    "        self.stocks = None\n",
    "        self.total_assets = None\n",
    "        self.gamma_reward = None\n",
    "        self.initial_total_assets = None\n",
    "\n",
    "        # environment information\n",
    "        self.env_name = \"StockEnv\"\n",
    "        self.state_dim = 109 # Size of get_state() array \n",
    "        self.action_dim = stock_dim\n",
    "        self.max_step = self.price_ary.shape[0] - 1\n",
    "        self.if_train = if_train\n",
    "        self.if_discrete = False\n",
    "        self.episode_return = 0.0\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-5000, high=5000, shape=(self.state_dim,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=-1, high=1, shape=(self.action_dim,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.num_trades = 0\n",
    "        price = self.price_ary[self.current_step]\n",
    "        \n",
    "        if self.if_train:\n",
    "            self.stocks = (\n",
    "                self.initial_stocks + rd.randint(0, 17, size=self.initial_stocks.shape)\n",
    "            ).astype(np.float32)\n",
    "            self.stocks_cool_down = np.zeros_like(self.stocks)\n",
    "            self.cash = (\n",
    "                self.initial_capital * rd.uniform(0.95, 1.05)\n",
    "                - (self.stocks * price).sum()\n",
    "            )\n",
    "        else:\n",
    "            self.stocks = self.initial_stocks.astype(np.float32)\n",
    "            self.stocks_cool_down = np.zeros_like(self.stocks)\n",
    "            self.cash = self.initial_capital\n",
    "        \n",
    "        self.total_assets = self.cash + (self.stocks * price).sum()\n",
    "        self.initial_total_assets = self.total_assets\n",
    "        self.gamma_reward = 0.0\n",
    "        observation = self.get_state(price)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        price = self.price_ary[self.current_step]\n",
    "        self.max_stock = np.round(np.floor(100_000 / price)).astype(int)\n",
    "        action = np.round((action * self.max_stock)).astype(int)\n",
    "        min_action = np.round((self.max_stock * self.min_stock_rate)).astype(int)\n",
    "        self.stocks_cool_down += 1\n",
    "        \n",
    "        if self.turbulence_bool[self.current_step] == 0:\n",
    "            \n",
    "            # Sell Logic\n",
    "            for index in np.where((action < -min_action) & (self.stocks_cool_down > 0))[0]:\n",
    "                if price[index] > 0:\n",
    "                    sell_num_shares = min(self.stocks[index], -action[index])\n",
    "                    sell_value = price[index] * sell_num_shares\n",
    "                    self.stocks[index] -= sell_num_shares\n",
    "                    self.cash += sell_value\n",
    "                    self.stocks_cool_down[index] = 0\n",
    "                    self.num_trades += 1\n",
    "\n",
    "            # Buy Logic\n",
    "            for index in np.where((action > min_action) & (self.stocks_cool_down > 0))[0]:\n",
    "                if price[index] > 0:\n",
    "                    buy_num_shares = min(self.cash // price[index], action[index])\n",
    "                    buy_value = price[index] * buy_num_shares\n",
    "                    self.stocks[index] += buy_num_shares\n",
    "                    self.cash -= buy_value\n",
    "                    self.stocks_cool_down[index] = 0\n",
    "                    self.num_trades += 1\n",
    "\n",
    "        # turbulence logic\n",
    "        else:\n",
    "            self.cash += (self.stocks * price).sum()\n",
    "            self.num_trades += np.count_nonzero(self.stocks)\n",
    "            self.stocks[:] = 0\n",
    "            self.stocks_cool_down[:] = 0\n",
    "        \n",
    "        # Reward Calculations\n",
    "        observation = self.get_state(price)\n",
    "        total_assets = self.cash + (self.stocks * price).sum()\n",
    "        reward = (total_assets - self.total_assets) * self.reward_scaling\n",
    "        self.total_assets = total_assets\n",
    "        self.gamma_reward = self.gamma_reward * self.gamma + reward\n",
    "        terminated = self.current_step == self.max_step\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        if terminated:\n",
    "            reward = self.gamma_reward\n",
    "            self.episode_return = total_assets / self.initial_total_assets\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def get_state(self, price):\n",
    "        cash = np.array(self.cash * (2**-12), dtype=np.float32)\n",
    "        scale = np.array(2**-6, dtype=np.float32)\n",
    "        observation = np.hstack(\n",
    "            (\n",
    "                cash,\n",
    "                price * scale,\n",
    "                self.stocks * scale,\n",
    "                self.stocks_cool_down,\n",
    "                self.tech_ary[self.current_step],\n",
    "            )\n",
    "        )\n",
    "        # print(len(observation))\n",
    "        return observation\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_sign(ary, thresh):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x * np.e)) - 0.5\n",
    "\n",
    "        return sigmoid(ary / thresh) * thresh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRL Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers.pb2 import PB2 # Dependencies: pip install GPy sklearn\n",
    "from ray.rllib.algorithms import Algorithm\n",
    "from ray.tune import register_env\n",
    "\n",
    "from ray.air import RunConfig, FailureConfig\n",
    "from ray.tune.tune_config import TuneConfig\n",
    "from ray.air.config import CheckpointConfig\n",
    "from ray.tune.callback import Callback\n",
    "\n",
    "from typing import Dict, Optional, Any, List, Union\n",
    "\n",
    "\n",
    "class DRLlibv2:\n",
    "    \"\"\"\n",
    "    It instantiates RLlib model with Ray tune functionality\n",
    "    Params\n",
    "    -------------------------------------\n",
    "    trainable:\n",
    "        Any Trainable class that takes config as parameter\n",
    "    train_env:\n",
    "        Training environment instance\n",
    "    train_env_name: str\n",
    "        Name of the training environment\n",
    "    params: dict\n",
    "        hyperparameters dictionary\n",
    "    run_name: str\n",
    "        tune run name\n",
    "    framework: str\n",
    "        \"torch\" or \"tf\" for tensorflow\n",
    "    local_dir: str\n",
    "         to save the results and tensorboard plots\n",
    "    num_workers: int\n",
    "        number of workers\n",
    "    num_envs_per_worker: int\n",
    "        number of vectorized environments per worker\n",
    "    restart_failed_sub_environments: bool\n",
    "        try to restart faulty sub-environments\n",
    "    recreate_failed_workers: bool\n",
    "        try to recreate failed workers\n",
    "    num_samples: int\n",
    "         Number of samples of hyperparameters config to run\n",
    "    scheduler:\n",
    "        Stopping suboptimal trials\n",
    "    log_level: str = \"WARN\",\n",
    "        Verbosity: \"DEBUG\"\n",
    "    num_gpus: Union[float, int] = 1\n",
    "        GPUs for trial\n",
    "    num_cpus: Union[float, int] = 20\n",
    "        CPUs for rollout collection\n",
    "    dataframe_save: str\n",
    "        Saving the tune results\n",
    "    metric: str\n",
    "        Metric for hyperparameter optimization in Bayesian Methods\n",
    "    mode: str\n",
    "        Maximize or Minimize the metric\n",
    "    max_failures: int\n",
    "        Number of failures to TuneError\n",
    "    timeout: int\n",
    "        Number of seconds to run the experiment\n",
    "    checkpoint_num_to_keep: int\n",
    "        Number of checkpoints to keep\n",
    "    checkpoint_freq: int\n",
    "        Checkpoint freq wrt training iterations\n",
    "    reuse_actors:bool\n",
    "        Reuse actors for tuning\n",
    "    callbacks:\n",
    "        callbacks integration for ray tune\n",
    "\n",
    "    It has the following methods:\n",
    "    Methods\n",
    "    -------------------------------------\n",
    "        train_tune_model: It takes in the params dictionary and fits in sklearn style to our trainable class\n",
    "        restore_agent: It restores previously errored or stopped trials or experiments\n",
    "        infer_results: It returns the results dataframe and trial informations\n",
    "        get_test_agent: It returns the testing agent for inference\n",
    "\n",
    "    Example\n",
    "    ---------------------------------------\n",
    "    def sample_ppo_params():\n",
    "        return {\n",
    "            \"entropy_coeff\": tune.loguniform(0.00000001, 0.1),\n",
    "            \"lr\": tune.loguniform(5e-5, 0.001),\n",
    "            \"sgd_minibatch_size\": tune.choice([ 32, 64, 128, 256, 512]),\n",
    "            \"lambda\": tune.choice([0.1,0.3,0.5,0.7,0.9,1.0]),\n",
    "        }\n",
    "    drl_agent = DRLlibv2(\n",
    "        trainable=\"PPO\",\n",
    "        train_env=train_env_instance,\n",
    "        train_env_name=\"StockTradingEnv_Train\",\n",
    "        framework=\"torch\",\n",
    "        log_level=\"WARN\",\n",
    "        run_name='gp2_train_mlp',\n",
    "        local_dir=\"gp2_train_mlp\",\n",
    "        params=sample_ppo_params(),\n",
    "        num_workers=20,\n",
    "        num_envs_per_worker=32,\n",
    "        restart_failed_sub_environments=True,\n",
    "        recreate_failed_workers=True,\n",
    "        num_samples=20,\n",
    "        num_gpus=1,\n",
    "        num_cpus=20,\n",
    "        timeout=3600,\n",
    "        checkpoint_num_to_keep=3,\n",
    "        checkpoint_freq=5,\n",
    "        scheduler=pb2,\n",
    "    )\n",
    "    #Tune or train the model\n",
    "    res = drl_agent.train_tune_model()\n",
    "\n",
    "    #Get the tune results\n",
    "    results_df, best_result = drl_agent.infer_results()\n",
    "\n",
    "    #Get the best testing agent\n",
    "    test_agent = drl_agent.get_test_agent(test_env_instance,'StockTrading_testenv')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        trainable: Union[str, Any],\n",
    "        params: Dict,\n",
    "        train_env=None,\n",
    "        train_env_name: str='',\n",
    "        run_name: str = \"tune_run\",\n",
    "        framework: str = \"torch\",\n",
    "        local_dir: str = \"tune_results\",\n",
    "        num_workers: int = 20,\n",
    "        num_envs_per_worker: int = 32,\n",
    "        restart_failed_sub_environments: bool = True,\n",
    "        recreate_failed_workers: bool = True,\n",
    "        num_samples: int = 0,\n",
    "        scheduler=None,\n",
    "        log_level: str = \"WARN\",\n",
    "        num_gpus: Union[float, int] = 1,\n",
    "        num_cpus: Union[float, int] = 20,\n",
    "        dataframe_save: str = \"tune.csv\",\n",
    "        metric: str = \"episode_reward_mean\",\n",
    "        mode: Union[str, List[str]] = \"max\",\n",
    "        max_failures: int = 1,\n",
    "        timeout: int = 3600,\n",
    "        checkpoint_num_to_keep: Union[None, int] = None,\n",
    "        checkpoint_freq: int = 0,\n",
    "        reuse_actors: bool = True,\n",
    "        callbacks:Optional[List[\"Callback\"]]=None\n",
    "    ):\n",
    "\n",
    "        if train_env is not None:register_env(train_env_name, lambda config: train_env(config))\n",
    "        \n",
    "        self.params = params\n",
    "        self.params[\"framework\"] = framework\n",
    "        self.params[\"log_level\"] = log_level\n",
    "        self.params[\"num_gpus\"] = num_gpus\n",
    "        self.params[\"num_workers\"] = num_workers\n",
    "        self.params[\"num_envs_per_worker\"] = num_envs_per_worker\n",
    "        self.params[\"restart_failed_sub_environments\"] = restart_failed_sub_environments\n",
    "        self.params[\"recreate_failed_workers\"] = recreate_failed_workers\n",
    "        self.params[\"env\"] = train_env_name\n",
    "\n",
    "        self.run_name = run_name\n",
    "        self.local_dir = local_dir\n",
    "        self.scheduler = scheduler\n",
    "        self.num_samples = num_samples\n",
    "        self.trainable = trainable\n",
    "        if isinstance(self.trainable, str):\n",
    "            self.trainable.upper()\n",
    "        self.num_cpus = num_cpus\n",
    "        self.num_gpus = num_gpus\n",
    "        self.dataframe_save = dataframe_save\n",
    "        self.metric = metric\n",
    "        self.mode = mode\n",
    "        self.max_failures = max_failures\n",
    "        self.timeout = timeout\n",
    "        self.checkpoint_freq = checkpoint_freq\n",
    "        self.checkpoint_num_to_keep = checkpoint_num_to_keep\n",
    "        self.reuse_actors = reuse_actors\n",
    "        self.callbacks = callbacks\n",
    "\n",
    "    def train_tune_model(self):\n",
    "        \"\"\"\n",
    "        Tuning and training the model\n",
    "        Returns the results object\n",
    "        \"\"\"\n",
    "        ray.init(\n",
    "            num_cpus=self.num_cpus, num_gpus=self.num_gpus, ignore_reinit_error=True\n",
    "        )\n",
    "\n",
    "        tuner = tune.Tuner(\n",
    "            self.trainable,\n",
    "            param_space=self.params,\n",
    "            tune_config=TuneConfig(\n",
    "                num_samples=self.num_samples,\n",
    "                metric=self.metric,\n",
    "                mode=self.mode,\n",
    "                time_budget_s=self.timeout,\n",
    "                reuse_actors=self.reuse_actors,\n",
    "                scheduler=self.scheduler\n",
    "            ),\n",
    "            run_config=RunConfig(\n",
    "                name=self.run_name,\n",
    "                local_dir=self.local_dir,\n",
    "                callbacks=self.callbacks,\n",
    "                failure_config=FailureConfig(\n",
    "                    max_failures=self.max_failures, fail_fast=False\n",
    "                ),\n",
    "                checkpoint_config=CheckpointConfig(\n",
    "                    num_to_keep=self.checkpoint_num_to_keep,\n",
    "                    checkpoint_score_attribute=self.metric,\n",
    "                    checkpoint_score_order=self.mode,\n",
    "                    checkpoint_frequency=self.checkpoint_freq,\n",
    "                    checkpoint_at_end=True,\n",
    "                ),\n",
    "                verbose=3,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.results = tuner.fit()\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "    def infer_results(self, to_dataframe: str = None, mode: str = \"a\"):\n",
    "        \"\"\"\n",
    "        Get tune results in a dataframe and best results object\n",
    "        \"\"\"\n",
    "        results_df = self.results.get_dataframe()\n",
    "\n",
    "        if to_dataframe is None:\n",
    "            to_dataframe = self.dataframe_save\n",
    "\n",
    "        results_df.to_csv(to_dataframe, mode=mode)\n",
    "\n",
    "        best_result = self.results.get_best_result()\n",
    "        print(\"Best hyperparameters found were: \", best_result.config)\n",
    "        \n",
    "        return results_df, best_result\n",
    "\n",
    "    def restore_agent(\n",
    "        self,\n",
    "        checkpoint_path: str = \"\",\n",
    "        resume_unfinished: bool = True,\n",
    "        resume_errored: bool = True,\n",
    "        restart_errored: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Restore errored or stopped trials\n",
    "        \"\"\"\n",
    "        if checkpoint_path == \"\":\n",
    "            checkpoint_path = self.results.get_best_result().checkpoint._local_path\n",
    "\n",
    "        restored_agent = tune.Tuner.restore(\n",
    "            checkpoint_path,\n",
    "            restart_errored=restart_errored,\n",
    "            resume_unfinished=resume_unfinished,\n",
    "            resume_errored=resume_errored,\n",
    "        )\n",
    "        print(restored_agent)\n",
    "        self.results = restored_agent.fit()\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def get_test_agent(self, test_env, test_env_name: str, checkpoint=None):\n",
    "        \"\"\"\n",
    "        Get test agent\n",
    "        \"\"\"\n",
    "        if test_env is not None:register_env(test_env_name, lambda config: test_env(config))\n",
    "\n",
    "        if checkpoint is None:\n",
    "            checkpoint = self.results.get_best_result().checkpoint\n",
    "\n",
    "        testing_agent = Algorithm.from_checkpoint(checkpoint)\n",
    "\n",
    "        return testing_agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Environment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_env(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    # Create 'train_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"train_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'train_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "            \n",
    "    train_env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": True,\n",
    "    }\n",
    "    \n",
    "    return env(train_env_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Environment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_env(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    # Create 'test_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"test_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'test_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "            \n",
    "    test_env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": False,\n",
    "    }\n",
    "    \n",
    "    return env(test_env_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Train Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = \"2016-01-01\"\n",
    "TRAIN_END_DATE = \"2022-12-31\"\n",
    "\n",
    "TEST_START_DATE = '2023-01-01'\n",
    "TEST_END_DATE = '2023-04-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GP2_TICKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(INDICATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = len(GP2_TICKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 109 # length of env.get_state() array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env_instance = get_train_env(\n",
    "    start_date=TRAIN_START_DATE,\n",
    "    end_date=TRAIN_END_DATE,\n",
    "    ticker_list=GP2_TICKERS,\n",
    "    data_source=\"alpaca\",\n",
    "    time_interval=\"15Min\",\n",
    "    technical_indicator_list=INDICATORS,\n",
    "    env=StockTradingEnv,\n",
    "    model_name=\"PPO\",\n",
    "    if_vix = True,\n",
    "    if_cdl = True, \n",
    "    API_KEY = ALPACA_API_KEY, \n",
    "    API_SECRET = ALPACA_API_SECRET, \n",
    "    API_BASE_URL = API_BASE_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "\n",
    "pprint(PPOConfig().to_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ppo_params():\n",
    "    dimensions = [32, 64, 128, 256, 512, 1024]\n",
    "    layers = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    def random_layers():\n",
    "        fully_connected = np.random.choice([True, False])\n",
    "        num_layers = np.random.choice(layers)\n",
    "        if fully_connected:\n",
    "            shared_dim = np.random.choice(dimensions)\n",
    "            return [shared_dim for _ in range(num_layers)]\n",
    "        else:\n",
    "            return [np.random.choice(dimensions) for _ in range(num_layers)]\n",
    "        \n",
    "    return {\n",
    "        \"entropy_coeff\": tune.loguniform(1e-5, 0.1),\n",
    "        \"lr\": tune.loguniform(1e-7, 1e-3),\n",
    "        \"sgd_minibatch_size\": tune.choice([32, 64, 128, 256, 512, 1024]),\n",
    "        \"lambda\": tune.choice([0.5, 0.7, 0.9, 0.95, 0.99, 1.0]),\n",
    "        \"framework\": \"torch\",\n",
    "        \"model\": {\n",
    "            \"fcnet_hiddens\": tune.sample_from(random_layers),\n",
    "        },\n",
    "        \"num_sgd_iter\": tune.choice([20, 30, 40]),\n",
    "        \"train_batch_size\": tune.choice([1024, 2000, 4000, 8000, 16000]),\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"batch_mode\": \"truncate_episodes\",\n",
    "        \"gamma\": tune.choice([0.9, 0.95, 0.99, 0.995]),\n",
    "        \"clip_param\": tune.choice([0.1, 0.2, 0.3, 0.4]),\n",
    "        \"vf_clip_param\": tune.choice([5.0, 10.0, 15.0]),\n",
    "        \"vf_loss_coeff\": tune.choice([0.5, 1.0, 1.5]),\n",
    "        \"kl_coeff\": tune.choice([0.1, 0.2, 0.5, 1.0]),\n",
    "        \"kl_target\": tune.choice([0.001, 0.01, 0.02, 0.05]),\n",
    "        \"grad_clip\": tune.choice([0.5, 1.0, 2.0, 3.0, 4.0, 5.0]),\n",
    "        \"observation_filter\": tune.choice([\"NoFilter\", \"MeanStdFilter\"]),\n",
    "        \"exploration_config\": tune.choice([\n",
    "            {\"type\": \"StochasticSampling\"},\n",
    "            {\"type\": \"EpsilonGreedy\", \"initial_epsilon\": 1.0, \"final_epsilon\": 0.01, \"epsilon_timesteps\": 10000},\n",
    "            {\"type\": \"SoftQ\", \"temperature\": 1.0}\n",
    "        ]),\n",
    "        \"normalize_actions\": tune.choice([True, False]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb2 = PB2(\n",
    "    time_attr='episodes_total',\n",
    "    perturbation_interval=5,\n",
    "    quantile_fraction=0.2,\n",
    "    hyperparam_bounds={\n",
    "        \"entropy_coeff\": (1e-5, 0.1),\n",
    "        \"lr\": (1e-7, 1e-3),\n",
    "        \"sgd_minibatch_size\": (32, 1024),\n",
    "        \"lambda\": (0.5, 1.0),\n",
    "        \"num_sgd_iter\": (20, 40),\n",
    "        \"train_batch_size\": (1024, 16000),\n",
    "        \"gamma\": (0.9, 0.995),\n",
    "        \"clip_param\": (0.1, 0.4),\n",
    "        \"vf_clip_param\": (5.0, 15.0),\n",
    "        \"vf_loss_coeff\": (0.5, 1.5),\n",
    "        \"kl_coeff\": (0.1, 1.0),\n",
    "        \"kl_target\": (0.001, 0.05),\n",
    "        \"grad_clip\": (0.5, 5.0),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drl_agent = DRLlibv2(\n",
    "    trainable=\"PPO\",\n",
    "    train_env=train_env_instance,\n",
    "    train_env_name=\"StockTradingEnv_Train\",\n",
    "    framework=\"torch\",\n",
    "    log_level=\"WARN\",\n",
    "    run_name='gp2_train_mlp',\n",
    "    local_dir=\"gp2_train_mlp\",\n",
    "    params=sample_ppo_params(),\n",
    "    num_workers=20,\n",
    "    num_envs_per_worker=32,\n",
    "    restart_failed_sub_environments=True,\n",
    "    recreate_failed_workers=True,\n",
    "    num_samples=20,\n",
    "    num_gpus=1,\n",
    "    num_cpus=20,\n",
    "    timeout=3600,\n",
    "    checkpoint_num_to_keep=3,\n",
    "    checkpoint_freq=5,\n",
    "    scheduler=pb2,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Tune MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = drl_agent.train_tune_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, best_result = drl_agent.infer_results()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ppo_params():\n",
    "    dimensions = [32, 64, 128, 256, 512, 1024]\n",
    "    layers = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    def random_layers():\n",
    "        fully_connected = np.random.choice([True, False])\n",
    "        num_layers = np.random.choice(layers)\n",
    "        if fully_connected:\n",
    "            shared_dim = np.random.choice(dimensions)\n",
    "            return [shared_dim for _ in range(num_layers)]\n",
    "        else:\n",
    "            return [np.random.choice(dimensions) for _ in range(num_layers)]\n",
    "            \n",
    "    return {\n",
    "        \"entropy_coeff\": tune.loguniform(1e-5, 0.1),\n",
    "        \"lr\": tune.loguniform(1e-7, 1e-3),\n",
    "        \"sgd_minibatch_size\": tune.choice([32, 64, 128, 256, 512, 1024]),\n",
    "        \"lambda\": tune.choice([0.5, 0.7, 0.9, 0.95, 0.99, 1.0]),\n",
    "        \"framework\": \"torch\",\n",
    "        \"model\": {\n",
    "            \"fcnet_hiddens\": tune.sample_from(random_layers),\n",
    "            \"use_lstm\": True,\n",
    "            \"lstm_cell_size\": tune.choice([128, 256, 512]),\n",
    "            \"max_seq_len\": tune.choice([10, 20, 50]),\n",
    "            \"lstm_use_prev_action\": tune.choice([True, False]),\n",
    "            \"lstm_use_prev_reward\": tune.choice([True, False]),\n",
    "            \"vf_share_layers\": False,\n",
    "        },\n",
    "        \"num_sgd_iter\": tune.choice([20, 30, 40]),\n",
    "        \"train_batch_size\": tune.choice([1024, 2000, 4000, 8000, 16000]),\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"batch_mode\": \"truncate_episodes\",\n",
    "        \"gamma\": tune.choice([0.9, 0.95, 0.99, 0.995]),\n",
    "        \"clip_param\": tune.choice([0.1, 0.2, 0.3, 0.4]),\n",
    "        \"vf_clip_param\": tune.choice([5.0, 10.0, 15.0]),\n",
    "        \"vf_loss_coeff\": tune.choice([0.5, 1.0, 1.5]),\n",
    "        \"kl_coeff\": tune.choice([0.1, 0.2, 0.5, 1.0]),\n",
    "        \"kl_target\": tune.choice([0.001, 0.01, 0.02, 0.05]),\n",
    "        \"grad_clip\": tune.choice([0.5, 1.0, 2.0, 3.0, 4.0, 5.0]),\n",
    "        \"observation_filter\": tune.choice([\"NoFilter\", \"MeanStdFilter\"]),\n",
    "        \"exploration_config\": tune.choice([\n",
    "            {\"type\": \"StochasticSampling\"},\n",
    "            {\"type\": \"EpsilonGreedy\", \"initial_epsilon\": 1.0, \"final_epsilon\": 0.01, \"epsilon_timesteps\": 10000},\n",
    "            {\"type\": \"SoftQ\", \"temperature\": 1.0}\n",
    "        ]),\n",
    "        \"normalize_actions\": tune.choice([True, False]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb2 = PB2(\n",
    "    time_attr='episodes_total',\n",
    "    perturbation_interval=5,\n",
    "    quantile_fraction=0.2,\n",
    "    hyperparam_bounds={\n",
    "        \"entropy_coeff\": (1e-5, 0.1),\n",
    "        \"lr\": (1e-7, 1e-3),\n",
    "        \"sgd_minibatch_size\": (32, 1024),\n",
    "        \"lambda\": (0.5, 1.0),\n",
    "        \"num_sgd_iter\": (20, 40),\n",
    "        \"train_batch_size\": (1024, 16000),\n",
    "        \"gamma\": (0.9, 0.995),\n",
    "        \"clip_param\": (0.1, 0.4),\n",
    "        \"vf_clip_param\": (5.0, 15.0),\n",
    "        \"vf_loss_coeff\": (0.5, 1.5),\n",
    "        \"kl_coeff\": (0.1, 1.0),\n",
    "        \"kl_target\": (0.001, 0.05),\n",
    "        \"grad_clip\": (0.5, 5.0),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drl_agent = DRLlibv2(\n",
    "    trainable=\"PPO\",\n",
    "    train_env=train_env_instance,\n",
    "    train_env_name=\"StockTradingEnv_Train\",\n",
    "    framework=\"torch\",\n",
    "    log_level=\"WARN\",\n",
    "    run_name='gp2_train_lstm',\n",
    "    local_dir=\"gp2_train_lstm\",\n",
    "    params=sample_ppo_params(),\n",
    "    num_workers=20,\n",
    "    num_envs_per_worker=32,\n",
    "    restart_failed_sub_environments=True,\n",
    "    recreate_failed_workers=True,\n",
    "    num_samples=20,\n",
    "    num_gpus=1,\n",
    "    num_cpus=20,\n",
    "    timeout=3600,\n",
    "    checkpoint_num_to_keep=3,\n",
    "    checkpoint_freq=5,\n",
    "    scheduler=pb2,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Tune LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = drl_agent.train_tune_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, best_result = drl_agent.infer_results()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention (GTrXL) Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ppo_params():\n",
    "    dimensions = [32, 64, 128, 256, 512, 1024]\n",
    "    layers = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    def random_layers():\n",
    "        fully_connected = np.random.choice([True, False])\n",
    "        num_layers = np.random.choice(layers)\n",
    "        if fully_connected:\n",
    "            shared_dim = np.random.choice(dimensions)\n",
    "            return [shared_dim for _ in range(num_layers)]\n",
    "        else:\n",
    "            return [np.random.choice(dimensions) for _ in range(num_layers)]\n",
    "            \n",
    "    return {\n",
    "        \"entropy_coeff\": tune.loguniform(1e-5, 0.1),\n",
    "        \"lr\": tune.loguniform(1e-7, 1e-3),\n",
    "        \"sgd_minibatch_size\": tune.choice([32, 64, 128, 256, 512, 1024]),\n",
    "        \"lambda\": tune.choice([0.5, 0.7, 0.9, 0.95, 0.99, 1.0]),\n",
    "        \"framework\": \"torch\",\n",
    "        \"model\": {\n",
    "            \"fcnet_hiddens\": tune.sample_from(random_layers),\n",
    "            \"use_attention\": True,\n",
    "            \"max_seq_len\": tune.choice([10, 20, 50]),\n",
    "            \"attention_num_transformer_units\": tune.choice([1, 2, 3]),\n",
    "            \"attention_dim\": tune.choice([64, 128, 256]),\n",
    "            \"attention_num_heads\": tune.choice([1, 2, 4]),\n",
    "            \"attention_head_dim\": tune.choice([32, 64]),\n",
    "            \"attention_memory_inference\": tune.choice([25, 50, 75]),\n",
    "            \"attention_memory_training\": tune.choice([25, 50, 75]),\n",
    "            \"attention_position_wise_mlp_dim\": tune.choice([32, 64, 128]),\n",
    "            \"attention_init_gru_gate_bias\": tune.choice([1.0, 2.0, 3.0]),\n",
    "            \"attention_use_n_prev_actions\": tune.choice([0, 1, 2, 3, 4, 5]),\n",
    "            \"attention_use_n_prev_rewards\": tune.choice([0, 1, 2, 3, 4, 5]),\n",
    "        },\n",
    "        \"num_sgd_iter\": tune.choice([20, 30, 40]),\n",
    "        \"train_batch_size\": tune.choice([1024, 2000, 4000, 8000, 16000]),\n",
    "        \"rollout_fragment_length\": \"auto\",\n",
    "        \"batch_mode\": \"truncate_episodes\",\n",
    "        \"gamma\": tune.choice([0.9, 0.95, 0.99, 0.995]),\n",
    "        \"clip_param\": tune.choice([0.1, 0.2, 0.3, 0.4]),\n",
    "        \"vf_clip_param\": tune.choice([5.0, 10.0, 15.0]),\n",
    "        \"vf_loss_coeff\": tune.choice([0.5, 1.0, 1.5]),\n",
    "        \"kl_coeff\": tune.choice([0.1, 0.2, 0.5, 1.0]),\n",
    "        \"kl_target\": tune.choice([0.001, 0.01, 0.02, 0.05]),\n",
    "        \"grad_clip\": tune.choice([0.5, 1.0, 2.0, 3.0, 4.0, 5.0]),\n",
    "        \"observation_filter\": tune.choice([\"NoFilter\", \"MeanStdFilter\"]),\n",
    "        \"exploration_config\": tune.choice([\n",
    "            {\"type\": \"StochasticSampling\"},\n",
    "            {\"type\": \"EpsilonGreedy\", \"initial_epsilon\": 1.0, \"final_epsilon\": 0.01, \"epsilon_timesteps\": 10000},\n",
    "            {\"type\": \"SoftQ\", \"temperature\": 1.0}\n",
    "        ]),\n",
    "        \"normalize_actions\": tune.choice([True, False]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb2 = PB2(\n",
    "    time_attr='episodes_total',\n",
    "    perturbation_interval=5,\n",
    "    quantile_fraction=0.2,\n",
    "    hyperparam_bounds={\n",
    "        \"entropy_coeff\": (1e-5, 0.1),\n",
    "        \"lr\": (1e-7, 1e-3),\n",
    "        \"sgd_minibatch_size\": (32, 1024),\n",
    "        \"lambda\": (0.5, 1.0),\n",
    "        \"num_sgd_iter\": (20, 40),\n",
    "        \"train_batch_size\": (1024, 16000),\n",
    "        \"gamma\": (0.9, 0.995),\n",
    "        \"clip_param\": (0.1, 0.4),\n",
    "        \"vf_clip_param\": (5.0, 15.0),\n",
    "        \"vf_loss_coeff\": (0.5, 1.5),\n",
    "        \"kl_coeff\": (0.1, 1.0),\n",
    "        \"kl_target\": (0.001, 0.05),\n",
    "        \"grad_clip\": (0.5, 5.0),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drl_agent = DRLlibv2(\n",
    "    trainable=\"PPO\",\n",
    "    train_env=train_env_instance,\n",
    "    train_env_name=\"StockTradingEnv_Train\",\n",
    "    framework=\"torch\",\n",
    "    log_level=\"WARN\",\n",
    "    run_name='gp2_train_attn',\n",
    "    local_dir=\"gp2_train_attn\",\n",
    "    params=sample_ppo_params(),\n",
    "    num_workers=20,\n",
    "    num_envs_per_worker=32,\n",
    "    restart_failed_sub_environments=True,\n",
    "    recreate_failed_workers=True,\n",
    "    num_samples=20,\n",
    "    num_gpus=1,\n",
    "    num_cpus=20,\n",
    "    timeout=3600,\n",
    "    checkpoint_num_to_keep=3,\n",
    "    checkpoint_freq=5,\n",
    "    scheduler=pb2,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Tune Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = drl_agent.train_tune_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, best_result = drl_agent.infer_results()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env_instance = get_test_env(\n",
    "    start_date=TEST_START_DATE,\n",
    "    end_date=TRAIN_END_DATE,\n",
    "    ticker_list=GP2_TICKERS,\n",
    "    data_source=\"alpaca\",\n",
    "    time_interval=\"15Min\",\n",
    "    technical_indicator_list=INDICATORS,\n",
    "    env=StockTradingEnv,\n",
    "    model_name=\"PPO\",\n",
    "    if_vix = True,\n",
    "    if_cdl = True, \n",
    "    API_KEY = ALPACA_API_KEY, \n",
    "    API_SECRET = ALPACA_API_SECRET, \n",
    "    API_BASE_URL = API_BASE_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent = drl_agent.get_test_agent(test_env_instance, \"StockTradingEnv_Test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Agent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, _ = test_env_instance.reset()\n",
    "episode_returns = list()\n",
    "episode_total_assets = list()\n",
    "episode_total_assets.append(test_env_instance.initial_total_assets)\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action = test_agent.compute_single_action(observation=observation)\n",
    "    observation, reward, terminated, _, _ = test_env_instance.step(action)\n",
    "    total_assets = (\n",
    "        test_env_instance.cash\n",
    "        + (test_env_instance.price_ary[test_env_instance.current_step] * test_env_instance.stocks).sum()\n",
    "    )\n",
    "    episode_total_assets.append(total_assets)\n",
    "    episode_return = total_assets / test_env_instance.initial_total_assets\n",
    "    episode_returns.append(episode_return)\n",
    "print(\"Test Finished!\")\n",
    "print(f\"Episode Return: {episode_return}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Agent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, _ = test_env_instance.reset()\n",
    "policy = test_agent.get_policy()\n",
    "init_state = state = policy.get_initial_state()\n",
    "episode_returns = list()\n",
    "episode_total_assets = list()\n",
    "episode_total_assets.append(test_env_instance.initial_total_assets)\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action, state, _  = test_agent.compute_single_action(observation=observation,state=state)\n",
    "    observation, reward, terminated, _, _ = test_env_instance.step(action)\n",
    "    total_assets = (\n",
    "        test_env_instance.cash\n",
    "        + (test_env_instance.price_ary[test_env_instance.current_step] * test_env_instance.stocks).sum()\n",
    "    )\n",
    "    episode_total_assets.append(total_assets)\n",
    "    episode_return = total_assets / test_env_instance.initial_total_assets\n",
    "    episode_returns.append(episode_return)\n",
    "print(\"Test Finished!\")\n",
    "print(f\"Episode Return: {episode_return}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Agent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, _ = test_env_instance.reset()\n",
    "policy = test_agent.get_policy()\n",
    "init_state = state = policy.get_initial_state()\n",
    "num_transformers = results.get_best_result().config[\"model\"][\"attention_num_transformer_units\"]\n",
    "episode_returns = list()\n",
    "episode_total_assets = list()\n",
    "episode_total_assets.append(test_env_instance.initial_total_assets)\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action, state_out, _  = test_agent.compute_single_action(observation=observation,state=state)\n",
    "    observation, reward, terminated, _, _ = test_env_instance.step(action)\n",
    "    total_assets = (\n",
    "        test_env_instance.cash\n",
    "        + (test_env_instance.price_ary[test_env_instance.current_step] * test_env_instance.stocks).sum()\n",
    "    )\n",
    "    episode_total_assets.append(total_assets)\n",
    "    episode_return = total_assets / test_env_instance.initial_total_assets\n",
    "    episode_returns.append(episode_return)\n",
    "    state = [\n",
    "        np.concatenate([state[i], [state_out[i]]], axis=0)[1:]\n",
    "        for i in range(num_transformers)\n",
    "    ]\n",
    "print(\"Test Finished!\")\n",
    "print(f\"Episode Return: {episode_return}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
