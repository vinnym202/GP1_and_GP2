{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP2 Trading Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinny/Code/Python/Projects/Algo_trade/GP1/.venv/lib/python3.10/site-packages/pyfolio/pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from data_processor import DataProcessor\n",
    "from plot import backtest_stats, backtest_plot, get_baseline, get_daily_return, drop_dup_dates\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_private import ALPACA_API_KEY, ALPACA_API_SECRET\n",
    "API_BASE_URL = 'https://paper-api.alpaca.markets'\n",
    "from config_tickers import GP2_TICKERS\n",
    "from config import INDICATORS\n",
    "from config import CDL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from numpy import random as rd\n",
    "\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        gamma=0.99,\n",
    "        turbulence_thresh=99,\n",
    "        max_stock=None,\n",
    "        min_stock_rate=0.1,\n",
    "        initial_capital=1e5,\n",
    "        reward_scaling=2**-11,\n",
    "        initial_stocks=None,\n",
    "    ):\n",
    "        price_ary = config[\"price_array\"]\n",
    "        tech_ary = config[\"tech_array\"]\n",
    "        turbulence_ary = config[\"turbulence_array\"]\n",
    "        date_ary = config[\"date_array\"]\n",
    "        if_train = config[\"if_train\"]\n",
    "        self.price_ary = price_ary.astype(np.float32)\n",
    "        self.tech_ary = tech_ary.astype(np.float32)\n",
    "        self.turbulence_ary = turbulence_ary\n",
    "        self.date_ary = date_ary\n",
    "\n",
    "        self.tech_ary = self.tech_ary * 2**-7\n",
    "        self.turbulence_bool = (turbulence_ary > turbulence_thresh).astype(np.float32)\n",
    "        self.turbulence_ary = (\n",
    "            self.sigmoid_sign(turbulence_ary, turbulence_thresh) * 2**-5\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        stock_dim = self.price_ary.shape[1]\n",
    "        self.gamma = gamma\n",
    "        self.max_stock = max_stock\n",
    "        self.min_stock_rate = min_stock_rate\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.initial_capital = initial_capital\n",
    "        self.initial_stocks = (\n",
    "            np.zeros(stock_dim, dtype=np.float32)\n",
    "            if initial_stocks is None\n",
    "            else initial_stocks\n",
    "        )\n",
    "        \n",
    "        # reset()\n",
    "        self.current_step = None\n",
    "        self.num_trades = None\n",
    "        self.cash = None\n",
    "        self.stocks = None\n",
    "        self.total_assets = None\n",
    "        self.gamma_reward = None\n",
    "        self.initial_total_assets = None\n",
    "\n",
    "        # environment information\n",
    "        self.env_name = \"StockEnv\"\n",
    "        self.state_dim = 109 # Size of get_state() array \n",
    "        self.action_dim = stock_dim\n",
    "        self.max_step = self.price_ary.shape[0] - 1\n",
    "        self.if_train = if_train\n",
    "        self.if_discrete = False\n",
    "        self.episode_return = 0.0\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-5000, high=5000, shape=(self.state_dim,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=-1, high=1, shape=(self.action_dim,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.num_trades = 0\n",
    "        price = self.price_ary[self.current_step]\n",
    "        \n",
    "        if self.if_train:\n",
    "            self.stocks = (\n",
    "                self.initial_stocks + rd.randint(0, 17, size=self.initial_stocks.shape)\n",
    "            ).astype(np.float32)\n",
    "            self.stocks_cool_down = np.zeros_like(self.stocks)\n",
    "            self.cash = (\n",
    "                self.initial_capital * rd.uniform(0.95, 1.05)\n",
    "                - (self.stocks * price).sum()\n",
    "            )\n",
    "        else:\n",
    "            self.stocks = self.initial_stocks.astype(np.float32)\n",
    "            self.stocks_cool_down = np.zeros_like(self.stocks)\n",
    "            self.cash = self.initial_capital\n",
    "        \n",
    "        self.total_assets = self.cash + (self.stocks * price).sum()\n",
    "        self.initial_total_assets = self.total_assets\n",
    "        self.gamma_reward = 0.0\n",
    "        observation = self.get_state(price)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        price = self.price_ary[self.current_step]\n",
    "        self.max_stock = np.round(np.floor(100_000 / price)).astype(int)\n",
    "        action = np.round((action * self.max_stock)).astype(int)\n",
    "        min_action = np.round((self.max_stock * self.min_stock_rate)).astype(int)\n",
    "        self.stocks_cool_down += 1\n",
    "        \n",
    "        if self.turbulence_bool[self.current_step] == 0:\n",
    "            \n",
    "            # Sell Logic\n",
    "            for index in np.where((action < -min_action) & (self.stocks_cool_down > 0))[0]:\n",
    "                if price[index] > 0:\n",
    "                    sell_num_shares = min(self.stocks[index], -action[index])\n",
    "                    sell_value = price[index] * sell_num_shares\n",
    "                    self.stocks[index] -= sell_num_shares\n",
    "                    self.cash += sell_value\n",
    "                    self.stocks_cool_down[index] = 0\n",
    "                    self.num_trades += 1\n",
    "\n",
    "            # Buy Logic\n",
    "            for index in np.where((action > min_action) & (self.stocks_cool_down > 0))[0]:\n",
    "                if price[index] > 0:\n",
    "                    buy_num_shares = min(self.cash // price[index], action[index])\n",
    "                    buy_value = price[index] * buy_num_shares\n",
    "                    self.stocks[index] += buy_num_shares\n",
    "                    self.cash -= buy_value\n",
    "                    self.stocks_cool_down[index] = 0\n",
    "                    self.num_trades += 1\n",
    "\n",
    "        # turbulence logic\n",
    "        else:\n",
    "            self.cash += (self.stocks * price).sum()\n",
    "            self.num_trades += np.count_nonzero(self.stocks)\n",
    "            self.stocks[:] = 0\n",
    "            self.stocks_cool_down[:] = 0\n",
    "        \n",
    "        # Reward Calculations\n",
    "        observation = self.get_state(price)\n",
    "        total_assets = self.cash + (self.stocks * price).sum()\n",
    "        reward = (total_assets - self.total_assets) * self.reward_scaling\n",
    "        self.total_assets = total_assets\n",
    "        self.gamma_reward = self.gamma_reward * self.gamma + reward\n",
    "        terminated = self.current_step == self.max_step\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        if terminated:\n",
    "            reward = self.gamma_reward\n",
    "            self.episode_return = total_assets / self.initial_total_assets\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def get_state(self, price):\n",
    "        cash = np.array(self.cash * (2**-12), dtype=np.float32)\n",
    "        scale = np.array(2**-6, dtype=np.float32)\n",
    "        observation = np.hstack(\n",
    "            (\n",
    "                cash,\n",
    "                price * scale,\n",
    "                self.stocks * scale,\n",
    "                self.stocks_cool_down,\n",
    "                self.tech_ary[self.current_step],\n",
    "            )\n",
    "        )\n",
    "        # print(len(observation))\n",
    "        return observation\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_sign(ary, thresh):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x * np.e)) - 0.5\n",
    "\n",
    "        return sigmoid(ary / thresh) * thresh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRL Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers.pb2 import PB2 # Dependencies: pip install GPy sklearn\n",
    "from ray.rllib.algorithms import Algorithm\n",
    "from ray.tune import register_env\n",
    "\n",
    "from ray.air import RunConfig, FailureConfig\n",
    "from ray.tune.tune_config import TuneConfig\n",
    "from ray.air.config import CheckpointConfig\n",
    "from ray.tune.callback import Callback\n",
    "\n",
    "from typing import Dict, Optional, Any, List, Union\n",
    "\n",
    "\n",
    "class DRLlibv2:\n",
    "    \"\"\"\n",
    "    It instantiates RLlib model with Ray tune functionality\n",
    "    Params\n",
    "    -------------------------------------\n",
    "    trainable:\n",
    "        Any Trainable class that takes config as parameter\n",
    "    train_env:\n",
    "        Training environment instance\n",
    "    train_env_name: str\n",
    "        Name of the training environment\n",
    "    params: dict\n",
    "        hyperparameters dictionary\n",
    "    run_name: str\n",
    "        tune run name\n",
    "    framework: str\n",
    "        \"torch\" or \"tf\" for tensorflow\n",
    "    local_dir: str\n",
    "         to save the results and tensorboard plots\n",
    "    num_workers: int\n",
    "        number of workers\n",
    "    num_envs_per_worker: int\n",
    "        number of vectorized environments per worker\n",
    "    restart_failed_sub_environments: bool\n",
    "        try to restart faulty sub-environments\n",
    "    recreate_failed_workers: bool\n",
    "        try to recreate failed workers\n",
    "    num_samples: int\n",
    "         Number of samples of hyperparameters config to run\n",
    "    scheduler:\n",
    "        Stopping suboptimal trials\n",
    "    log_level: str = \"WARN\",\n",
    "        Verbosity: \"DEBUG\"\n",
    "    num_gpus: Union[float, int] = 1\n",
    "        GPUs for trial\n",
    "    num_cpus: Union[float, int] = 20\n",
    "        CPUs for rollout collection\n",
    "    dataframe_save: str\n",
    "        Saving the tune results\n",
    "    metric: str\n",
    "        Metric for hyperparameter optimization in Bayesian Methods\n",
    "    mode: str\n",
    "        Maximize or Minimize the metric\n",
    "    max_failures: int\n",
    "        Number of failures to TuneError\n",
    "    timeout: int\n",
    "        Number of seconds to run the experiment\n",
    "    checkpoint_num_to_keep: int\n",
    "        Number of checkpoints to keep\n",
    "    checkpoint_freq: int\n",
    "        Checkpoint freq wrt training iterations\n",
    "    reuse_actors:bool\n",
    "        Reuse actors for tuning\n",
    "    callbacks:\n",
    "        callbacks integration for ray tune\n",
    "\n",
    "    It has the following methods:\n",
    "    Methods\n",
    "    -------------------------------------\n",
    "        train_tune_model: It takes in the params dictionary and fits in sklearn style to our trainable class\n",
    "        restore_agent: It restores previously errored or stopped trials or experiments\n",
    "        infer_results: It returns the results dataframe and trial informations\n",
    "        get_test_agent: It returns the testing agent for inference\n",
    "\n",
    "    Example\n",
    "    ---------------------------------------\n",
    "    def sample_ppo_params():\n",
    "        return {\n",
    "            \"entropy_coeff\": tune.loguniform(0.00000001, 0.1),\n",
    "            \"lr\": tune.loguniform(5e-5, 0.001),\n",
    "            \"sgd_minibatch_size\": tune.choice([ 32, 64, 128, 256, 512]),\n",
    "            \"lambda\": tune.choice([0.1,0.3,0.5,0.7,0.9,1.0]),\n",
    "        }\n",
    "    drl_agent = DRLlibv2(\n",
    "        trainable=\"PPO\",\n",
    "        train_env=train_env_instance,\n",
    "        train_env_name=\"StockTradingEnv\",\n",
    "        framework=\"torch\",\n",
    "        log_level=\"WARN\",\n",
    "        run_name='gp2_train_mlp',\n",
    "        local_dir=\"gp2_train_mlp\",\n",
    "        params=sample_ppo_params(),\n",
    "        num_workers=16,\n",
    "        num_envs_per_worker=32,\n",
    "        restart_failed_sub_environments=True,\n",
    "        recreate_failed_workers=True,\n",
    "        num_samples=16,\n",
    "        num_gpus=1,\n",
    "        num_cpus=20,\n",
    "        timeout=3600,\n",
    "        checkpoint_num_to_keep=16,\n",
    "        checkpoint_freq=5,\n",
    "        scheduler=pb2,\n",
    "    )\n",
    "    #Tune or train the model\n",
    "    res = drl_agent.train_tune_model()\n",
    "\n",
    "    #Get the tune results\n",
    "    results_df, best_result = drl_agent.infer_results()\n",
    "\n",
    "    #Get the best testing agent\n",
    "    test_agent = drl_agent.get_test_agent(test_env_instance,'StockTrading_testenv')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        trainable: Union[str, Any],\n",
    "        params: Dict,\n",
    "        train_env=None,\n",
    "        train_env_name: str='',\n",
    "        run_name: str = \"tune_run\",\n",
    "        framework: str = \"torch\",\n",
    "        local_dir: str = \"tune_results\",\n",
    "        num_workers: int = 16,\n",
    "        num_envs_per_worker: int = 32,\n",
    "        restart_failed_sub_environments: bool = True,\n",
    "        recreate_failed_workers: bool = True,\n",
    "        num_samples: int = 0,\n",
    "        scheduler=None,\n",
    "        log_level: str = \"WARN\",\n",
    "        num_gpus: Union[float, int] = 1,\n",
    "        num_cpus: Union[float, int] = 20,\n",
    "        dataframe_save: str = \"tune.csv\",\n",
    "        metric: str = \"episode_reward_mean\",\n",
    "        mode: Union[str, List[str]] = \"max\",\n",
    "        max_failures: int = 1,\n",
    "        timeout: int = 3600,\n",
    "        checkpoint_num_to_keep: Union[None, int] = None,\n",
    "        checkpoint_freq: int = 0,\n",
    "        reuse_actors: bool = True,\n",
    "        callbacks:Optional[List[\"Callback\"]]=None\n",
    "    ):\n",
    "\n",
    "        if train_env is not None: register_env(train_env_name, lambda config: train_env(config))\n",
    "        \n",
    "        self.params = params\n",
    "        self.params[\"framework\"] = framework\n",
    "        self.params[\"log_level\"] = log_level\n",
    "        self.params[\"num_gpus\"] = num_gpus\n",
    "        self.params[\"num_workers\"] = num_workers\n",
    "        self.params[\"num_envs_per_worker\"] = num_envs_per_worker\n",
    "        self.params[\"restart_failed_sub_environments\"] = restart_failed_sub_environments\n",
    "        self.params[\"recreate_failed_workers\"] = recreate_failed_workers\n",
    "        self.params[\"env\"] = train_env_name\n",
    "\n",
    "        self.run_name = run_name\n",
    "        self.local_dir = local_dir\n",
    "        self.scheduler = scheduler\n",
    "        self.num_samples = num_samples\n",
    "        self.trainable = trainable\n",
    "        if isinstance(self.trainable, str):\n",
    "            self.trainable.upper()\n",
    "        self.num_cpus = num_cpus\n",
    "        self.num_gpus = num_gpus\n",
    "        self.dataframe_save = dataframe_save\n",
    "        self.metric = metric\n",
    "        self.mode = mode\n",
    "        self.max_failures = max_failures\n",
    "        self.timeout = timeout\n",
    "        self.checkpoint_freq = checkpoint_freq\n",
    "        self.checkpoint_num_to_keep = checkpoint_num_to_keep\n",
    "        self.reuse_actors = reuse_actors\n",
    "        self.callbacks = callbacks\n",
    "\n",
    "    def train_tune_model(self):\n",
    "        \"\"\"\n",
    "        Tuning and training the model\n",
    "        Returns the results object\n",
    "        \"\"\"\n",
    "        ray.init(\n",
    "            num_cpus=self.num_cpus, num_gpus=self.num_gpus, ignore_reinit_error=True\n",
    "        )\n",
    "\n",
    "        tuner = tune.Tuner(\n",
    "            self.trainable,\n",
    "            param_space=self.params,\n",
    "            tune_config=TuneConfig(\n",
    "                num_samples=self.num_samples,\n",
    "                metric=self.metric,\n",
    "                mode=self.mode,\n",
    "                time_budget_s=self.timeout,\n",
    "                reuse_actors=self.reuse_actors,\n",
    "                scheduler=self.scheduler\n",
    "            ),\n",
    "            run_config=RunConfig(\n",
    "                name=self.run_name,\n",
    "                local_dir=self.local_dir,\n",
    "                callbacks=self.callbacks,\n",
    "                failure_config=FailureConfig(\n",
    "                    max_failures=self.max_failures, fail_fast=False\n",
    "                ),\n",
    "                checkpoint_config=CheckpointConfig(\n",
    "                    num_to_keep=self.checkpoint_num_to_keep,\n",
    "                    checkpoint_score_attribute=self.metric,\n",
    "                    checkpoint_score_order=self.mode,\n",
    "                    checkpoint_frequency=self.checkpoint_freq,\n",
    "                    checkpoint_at_end=True,\n",
    "                ),\n",
    "                verbose=3,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.results = tuner.fit()\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "    def infer_results(self, to_dataframe: str = None, mode: str = \"a\"):\n",
    "        \"\"\"\n",
    "        Get tune results in a dataframe and best results object\n",
    "        \"\"\"\n",
    "        results_df = self.results.get_dataframe()\n",
    "\n",
    "        if to_dataframe is None:\n",
    "            to_dataframe = self.dataframe_save\n",
    "\n",
    "        results_df.to_csv(to_dataframe, mode=mode)\n",
    "\n",
    "        best_result = self.results.get_best_result()\n",
    "        print(\"Best hyperparameters found were: \", best_result.config)\n",
    "        \n",
    "        return results_df, best_result\n",
    "\n",
    "    def restore_agent(\n",
    "        self,\n",
    "        checkpoint_path: str = \"\",\n",
    "        resume_unfinished: bool = True,\n",
    "        resume_errored: bool = True,\n",
    "        restart_errored: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Restore errored or stopped trials\n",
    "        \"\"\"\n",
    "        if checkpoint_path == \"\":\n",
    "            checkpoint_path = self.results.get_best_result().checkpoint._local_path\n",
    "\n",
    "        restored_agent = tune.Tuner.restore(\n",
    "            checkpoint_path,\n",
    "            restart_errored=restart_errored,\n",
    "            resume_unfinished=resume_unfinished,\n",
    "            resume_errored=resume_errored,\n",
    "        )\n",
    "        print(restored_agent)\n",
    "        self.results = restored_agent.fit()\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def get_test_agent(self, test_env, test_env_name: str, checkpoint=None):\n",
    "        \"\"\"\n",
    "        Get test agent\n",
    "        \"\"\"\n",
    "        if test_env is not None:register_env(test_env_name, lambda config: test_env)\n",
    "\n",
    "        if checkpoint is None:\n",
    "            checkpoint = self.results.get_best_result().checkpoint\n",
    "\n",
    "        testing_agent = Algorithm.from_checkpoint(checkpoint)\n",
    "\n",
    "        return testing_agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Environment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_env(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    # Create 'train_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"train_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'train_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "            \n",
    "    train_env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": True,\n",
    "    }\n",
    "    \n",
    "    return env(train_env_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Environment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_env(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    ticker_list,\n",
    "    data_source,\n",
    "    time_interval,\n",
    "    technical_indicator_list,\n",
    "    env,\n",
    "    model_name,\n",
    "    if_vix=True,\n",
    "    if_cdl=True,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    # Create 'test_data' folder in the current working directory if it doesn't exist\n",
    "    folder_path = os.path.join(os.getcwd(), \"test_data\")\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Set the file paths within the 'test_data' folder\n",
    "    data_file = os.path.join(folder_path, f\"data_{start_date}_{end_date}.pkl\")\n",
    "    arrays_file = os.path.join(folder_path, f\"arrays_{start_date}_{end_date}.pkl\")\n",
    "\n",
    "    if os.path.exists(data_file) and os.path.exists(arrays_file):\n",
    "        # Load the saved data\n",
    "        with open(data_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        with open(arrays_file, \"rb\") as f:\n",
    "            price_array, tech_array, turbulence_array, date_array = pickle.load(f)\n",
    "    \n",
    "    else:\n",
    "        # download data\n",
    "        dp = DataProcessor(data_source, **kwargs)\n",
    "        data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
    "        data = dp.clean_data(data)\n",
    "        data = dp.add_technical_indicator(data, technical_indicator_list)\n",
    "        if if_cdl:\n",
    "            data = dp.add_cdl(data)\n",
    "        if if_vix:\n",
    "            data = dp.add_vix(data)\n",
    "        else:\n",
    "            data = dp.add_turbulence(data)\n",
    "        price_array, tech_array, turbulence_array, date_array = dp.df_to_array(data, if_vix, if_cdl)\n",
    "        \n",
    "        # Save the data and arrays\n",
    "        with open(data_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        with open(arrays_file, \"wb\") as f:\n",
    "            pickle.dump((price_array, tech_array, turbulence_array, date_array), f)\n",
    "            \n",
    "    test_env_config = {\n",
    "        \"price_array\": price_array,\n",
    "        \"tech_array\": tech_array,\n",
    "        \"turbulence_array\": turbulence_array,\n",
    "        \"date_array\": date_array,\n",
    "        \"if_train\": False,\n",
    "    }\n",
    "    \n",
    "    return env(test_env_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = \"2016-01-01\"\n",
    "TRAIN_END_DATE = \"2022-12-31\"\n",
    "\n",
    "TEST_START_DATE = '2023-01-01'\n",
    "TEST_END_DATE = '2023-04-30'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GP2_TICKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(INDICATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = len(GP2_TICKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 109 # length of env.get_state() array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env_instance = get_train_env(\n",
    "    start_date=TEST_START_DATE,\n",
    "    end_date=TRAIN_END_DATE,\n",
    "    ticker_list=GP2_TICKERS,\n",
    "    data_source=\"alpaca\",\n",
    "    time_interval=\"15Min\",\n",
    "    technical_indicator_list=INDICATORS,\n",
    "    env=StockTradingEnv,\n",
    "    model_name=\"PPO\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_disable_action_flattening': False,\n",
      " '_disable_execution_plan_api': True,\n",
      " '_disable_preprocessor_api': False,\n",
      " '_enable_learner_api': False,\n",
      " '_enable_rl_module_api': False,\n",
      " '_fake_gpus': False,\n",
      " '_learner_hps': PPOLearnerHPs(kl_coeff=0.2,\n",
      "                               kl_target=0.01,\n",
      "                               use_critic=True,\n",
      "                               clip_param=0.3,\n",
      "                               vf_clip_param=10.0,\n",
      "                               entropy_coeff=0.0,\n",
      "                               vf_loss_coeff=1.0,\n",
      "                               lr_schedule=None,\n",
      "                               entropy_coeff_schedule=None),\n",
      " '_tf_policy_handles_more_than_one_loss': False,\n",
      " '_validate_exploration_conf_and_rl_modules': True,\n",
      " 'action_space': None,\n",
      " 'actions_in_input_normalized': False,\n",
      " 'always_attach_evaluation_results': False,\n",
      " 'auto_wrap_old_gym_envs': True,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>,\n",
      " 'checkpoint_trainable_policies_only': False,\n",
      " 'clip_actions': False,\n",
      " 'clip_param': 0.3,\n",
      " 'clip_rewards': None,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'delay_between_worker_restarts_s': 60.0,\n",
      " 'disable_env_checking': False,\n",
      " 'eager_max_retraces': 20,\n",
      " 'eager_tracing': False,\n",
      " 'enable_async_evaluation': False,\n",
      " 'enable_connectors': True,\n",
      " 'enable_tf1_exec_eagerly': False,\n",
      " 'entropy_coeff': 0.0,\n",
      " 'entropy_coeff_schedule': None,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': None,\n",
      " 'evaluation_duration': 10,\n",
      " 'evaluation_duration_unit': 'episodes',\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'evaluation_sample_timeout_s': 180.0,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'export_native_model_files': False,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'torch',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': None,\n",
      " 'horizon': -1,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_config': {},\n",
      " 'is_atari': None,\n",
      " 'keep_per_episode_custom_metrics': False,\n",
      " 'kl_coeff': 0.2,\n",
      " 'kl_target': 0.01,\n",
      " 'lambda': 1.0,\n",
      " 'learner_class': None,\n",
      " 'local_gpu_idx': 0,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'logger_creator': None,\n",
      " 'lr': 5e-05,\n",
      " 'lr_schedule': None,\n",
      " 'max_num_worker_restarts': 1000,\n",
      " 'max_requests_in_flight_per_sampler_worker': 2,\n",
      " 'metrics_episode_collection_timeout_s': 60.0,\n",
      " 'metrics_num_episodes_for_smoothing': 100,\n",
      " 'min_sample_timesteps_per_iteration': 0,\n",
      " 'min_time_s_per_iteration': None,\n",
      " 'min_train_timesteps_per_iteration': 0,\n",
      " 'model': {'_disable_action_flattening': False,\n",
      "           '_disable_preprocessor_api': False,\n",
      "           '_time_major': False,\n",
      "           '_use_default_native_models': -1,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'encoder_latent_dim': None,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': False,\n",
      "           'zero_mean': True},\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {'default_policy': (None, None, None, None)},\n",
      "                'policies_to_train': None,\n",
      "                'policy_map_cache': -1,\n",
      "                'policy_map_capacity': 100,\n",
      "                'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fe176ce9d80>},\n",
      " 'no_done_at_end': -1,\n",
      " 'normalize_actions': True,\n",
      " 'num_consecutive_worker_failures_tolerance': 100,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_learner_worker': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_learner_worker': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_learner_workers': 0,\n",
      " 'num_sgd_iter': 30,\n",
      " 'num_workers': 2,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'observation_space': None,\n",
      " 'off_policy_estimation_methods': {},\n",
      " 'offline_sampling': False,\n",
      " 'ope_split_batch_by_episode': True,\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_config': {},\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'policy_states_are_swappable': False,\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'recreate_failed_workers': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'replay_sequence_length': None,\n",
      " 'restart_failed_sub_environments': False,\n",
      " 'rl_module_spec': None,\n",
      " 'rollout_fragment_length': 'auto',\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'sampler_perf_stats_ema_coef': None,\n",
      " 'seed': None,\n",
      " 'sgd_minibatch_size': 128,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'shuffle_sequences': True,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': -1,\n",
      " 'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'train_batch_size': 4000,\n",
      " 'use_critic': True,\n",
      " 'use_gae': True,\n",
      " 'validate_workers_after_construction': True,\n",
      " 'vf_clip_param': 10.0,\n",
      " 'vf_loss_coeff': 1.0,\n",
      " 'vf_share_layers': -1,\n",
      " 'worker_cls': None,\n",
      " 'worker_health_probe_timeout_s': 60,\n",
      " 'worker_restore_timeout_s': 1800}\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "\n",
    "pprint(PPOConfig().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ppo_params():\n",
    "    return {\n",
    "        \"entropy_coeff\": tune.loguniform(0.00000001, 0.1),\n",
    "        \"lr\": tune.loguniform(5e-5, 0.001),\n",
    "        \"sgd_minibatch_size\": tune.choice([ 32, 64, 128, 256, 512]),\n",
    "        \"lambda\": tune.choice([0.1,0.3,0.5,0.7,0.9,1.0]),\n",
    "        \"framework\":'torch',\n",
    "        'model':{\n",
    "            'fcnet_hiddens': [256, 256]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb2 = PB2(\n",
    "    time_attr='episodes_total',\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    perturbation_interval=5,\n",
    "    quantile_fraction=0.25,\n",
    "    hyperparam_bounds={\n",
    "        \n",
    "    }\n",
    ")\n",
    "# https://docs.ray.io/en/latest/tune/api/doc/ray.tune.schedulers.pb2.PB2.html#ray-tune-schedulers-pb2-pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drl_agent = DRLlibv2(\n",
    "    trainable=\"PPO\",\n",
    "    train_env=train_env_instance,\n",
    "    train_env_name=\"StockTradingEnv\",\n",
    "    framework=\"torch\",\n",
    "    log_level=\"WARN\",\n",
    "    run_name='gp2_train_mlp',\n",
    "    local_dir=\"gp2_train_mlp\",\n",
    "    params=sample_ppo_params(),\n",
    "    num_workers=16,\n",
    "    num_envs_per_worker=32,\n",
    "    restart_failed_sub_environments=True,\n",
    "    recreate_failed_workers=True,\n",
    "    num_samples=16,\n",
    "    num_gpus=1,\n",
    "    num_cpus=20,\n",
    "    timeout=3600,\n",
    "    checkpoint_num_to_keep=16,\n",
    "    checkpoint_freq=5,\n",
    "    scheduler=pb2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
