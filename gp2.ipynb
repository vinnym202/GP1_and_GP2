{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP2 Trading Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processor import DataProcessor\n",
    "from plot import backtest_stats, backtest_plot, get_baseline, get_daily_return, drop_dup_dates\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_private import ALPACA_API_KEY, ALPACA_API_SECRET\n",
    "API_BASE_URL = 'https://paper-api.alpaca.markets'\n",
    "from config_tickers import GP2_TICKER\n",
    "from config import INDICATORS\n",
    "from config import CDL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from numpy import random as rd\n",
    "\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        gamma=0.99,\n",
    "        turbulence_thresh=99,\n",
    "        max_stock=None,\n",
    "        min_stock_rate=0.1,\n",
    "        initial_capital=1e5,\n",
    "        reward_scaling=2**-11,\n",
    "        initial_stocks=None,\n",
    "    ):\n",
    "        price_ary = config[\"price_array\"]\n",
    "        tech_ary = config[\"tech_array\"]\n",
    "        turbulence_ary = config[\"turbulence_array\"]\n",
    "        date_ary = config[\"date_array\"]\n",
    "        if_train = config[\"if_train\"]\n",
    "        self.price_ary = price_ary.astype(np.float32)\n",
    "        self.tech_ary = tech_ary.astype(np.float32)\n",
    "        self.turbulence_ary = turbulence_ary\n",
    "        self.date_ary = date_ary\n",
    "\n",
    "        self.tech_ary = self.tech_ary * 2**-7\n",
    "        self.turbulence_bool = (turbulence_ary > turbulence_thresh).astype(np.float32)\n",
    "        self.turbulence_ary = (\n",
    "            self.sigmoid_sign(turbulence_ary, turbulence_thresh) * 2**-5\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        stock_dim = self.price_ary.shape[1]\n",
    "        self.gamma = gamma\n",
    "        self.max_stock = max_stock\n",
    "        self.min_stock_rate = min_stock_rate\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.initial_capital = initial_capital\n",
    "        self.initial_stocks = (\n",
    "            np.zeros(stock_dim, dtype=np.float32)\n",
    "            if initial_stocks is None\n",
    "            else initial_stocks\n",
    "        )\n",
    "        \n",
    "        # reset()\n",
    "        self.current_step = None\n",
    "        self.num_trades = None\n",
    "        self.cash = None\n",
    "        self.stocks = None\n",
    "        self.total_assets = None\n",
    "        self.gamma_reward = None\n",
    "        self.initial_total_assets = None\n",
    "\n",
    "        # environment information\n",
    "        self.env_name = \"StockEnv\"\n",
    "        self.state_dim = 109 # Size of get_state() array \n",
    "        self.action_dim = stock_dim\n",
    "        self.max_step = self.price_ary.shape[0] - 1\n",
    "        self.if_train = if_train\n",
    "        self.if_discrete = False\n",
    "        self.episode_return = 0.0\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-5000, high=5000, shape=(self.state_dim,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=-1, high=1, shape=(self.action_dim,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.num_trades = 0\n",
    "        price = self.price_ary[self.current_step]\n",
    "        \n",
    "        if self.if_train:\n",
    "            self.stocks = (\n",
    "                self.initial_stocks + rd.randint(0, 17, size=self.initial_stocks.shape)\n",
    "            ).astype(np.float32)\n",
    "            self.stocks_cool_down = np.zeros_like(self.stocks)\n",
    "            self.cash = (\n",
    "                self.initial_capital * rd.uniform(0.95, 1.05)\n",
    "                - (self.stocks * price).sum()\n",
    "            )\n",
    "        else:\n",
    "            self.stocks = self.initial_stocks.astype(np.float32)\n",
    "            self.stocks_cool_down = np.zeros_like(self.stocks)\n",
    "            self.cash = self.initial_capital\n",
    "        \n",
    "        self.total_assets = self.cash + (self.stocks * price).sum()\n",
    "        self.initial_total_assets = self.total_assets\n",
    "        self.gamma_reward = 0.0\n",
    "        observation = self.get_state(price)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        price = self.price_ary[self.current_step]\n",
    "        self.max_stock = np.round(np.floor(100_000 / price)).astype(int)\n",
    "        action = np.round((action * self.max_stock)).astype(int)\n",
    "        min_action = np.round((self.max_stock * self.min_stock_rate)).astype(int)\n",
    "        self.stocks_cool_down += 1\n",
    "        \n",
    "        if self.turbulence_bool[self.current_step] == 0:\n",
    "            \n",
    "            # Sell Logic\n",
    "            for index in np.where((action < -min_action) & (self.stocks_cool_down > 0))[0]:\n",
    "                if price[index] > 0:\n",
    "                    sell_num_shares = min(self.stocks[index], -action[index])\n",
    "                    sell_value = price[index] * sell_num_shares\n",
    "                    self.stocks[index] -= sell_num_shares\n",
    "                    self.cash += sell_value\n",
    "                    self.stocks_cool_down[index] = 0\n",
    "                    self.num_trades += 1\n",
    "\n",
    "            # Buy Logic\n",
    "            for index in np.where((action > min_action) & (self.stocks_cool_down > 0))[0]:\n",
    "                if price[index] > 0:\n",
    "                    buy_num_shares = min(self.cash // price[index], action[index])\n",
    "                    buy_value = price[index] * buy_num_shares\n",
    "                    self.stocks[index] += buy_num_shares\n",
    "                    self.cash -= buy_value\n",
    "                    self.stocks_cool_down[index] = 0\n",
    "                    self.num_trades += 1\n",
    "\n",
    "        # turbulence logic\n",
    "        else:\n",
    "            self.cash += (self.stocks * price).sum()\n",
    "            self.num_trades += np.count_nonzero(self.stocks)\n",
    "            self.stocks[:] = 0\n",
    "            self.stocks_cool_down[:] = 0\n",
    "        \n",
    "        # Reward Calculations\n",
    "        observation = self.get_state(price)\n",
    "        total_assets = self.cash + (self.stocks * price).sum()\n",
    "        reward = (total_assets - self.total_assets) * self.reward_scaling\n",
    "        self.total_assets = total_assets\n",
    "        self.gamma_reward = self.gamma_reward * self.gamma + reward\n",
    "        terminated = self.current_step == self.max_step\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        if terminated:\n",
    "            reward = self.gamma_reward\n",
    "            self.episode_return = total_assets / self.initial_total_assets\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def get_state(self, price):\n",
    "        cash = np.array(self.cash * (2**-12), dtype=np.float32)\n",
    "        scale = np.array(2**-6, dtype=np.float32)\n",
    "        observation = np.hstack(\n",
    "            (\n",
    "                cash,\n",
    "                price * scale,\n",
    "                self.stocks * scale,\n",
    "                self.stocks_cool_down,\n",
    "                self.tech_ary[self.current_step],\n",
    "            )\n",
    "        )\n",
    "        # print(len(observation))\n",
    "        return observation\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_sign(ary, thresh):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x * np.e)) - 0.5\n",
    "\n",
    "        return sigmoid(ary / thresh) * thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray.tune.schedulers.pb2 import PB2 # Dependencies: pip install GPy sklearn\n",
    "from ray.tune import sample_from\n",
    "\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "import YahooDownloader\n",
    "import FeatureEngineer, data_split\n",
    "\n",
    "\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRL Agent Class\n",
    "\n",
    "note: 16 workers, 64 vectorized enviroments instances per worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from ray import tune\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray.tune.schedulers.pb2 import PB2 # Dependencies: pip install GPy sklearn\n",
    "from ray.rllib.algorithms import Algorithm\n",
    "from ray.tune import register_env\n",
    "\n",
    "from ray.air import RunConfig, FailureConfig\n",
    "from ray.tune.tune_config import TuneConfig\n",
    "from ray.air.config import CheckpointConfig\n",
    "from ray.tune.callback import Callback\n",
    "\n",
    "from typing import Dict, Optional, Any, List, Union\n",
    "\n",
    "\n",
    "class DRLlibv2:\n",
    "    \"\"\"\n",
    "    It instantiates RLlib model with Ray tune functionality\n",
    "    Params\n",
    "    -------------------------------------\n",
    "    trainable:\n",
    "        Any Trainable class that takes config as parameter\n",
    "    train_env:\n",
    "        Training environment instance\n",
    "    train_env_name: str\n",
    "        Name of the training environment\n",
    "    params: dict\n",
    "        hyperparameters dictionary\n",
    "    run_name: str\n",
    "        tune run name\n",
    "    framework: str\n",
    "        \"torch\" or \"tf\" for tensorflow\n",
    "    local_dir: str\n",
    "         to save the results and tensorboard plots\n",
    "    num_workers: int\n",
    "        number of workers\n",
    "    num_samples: int\n",
    "         Number of samples of hyperparameters config to run\n",
    "    scheduler:\n",
    "        Stopping suboptimal trials\n",
    "    log_level: str = \"WARN\",\n",
    "        Verbosity: \"DEBUG\"\n",
    "    num_gpus: Union[float, int] = 1\n",
    "        GPUs for trial\n",
    "    num_cpus: Union[float, int] = 20\n",
    "        CPUs for rollout collection\n",
    "    dataframe_save: str\n",
    "        Saving the tune results\n",
    "    metric: str\n",
    "        Metric for hyperparameter optimization in Bayesian Methods\n",
    "    mode: str\n",
    "        Maximize or Minimize the metric\n",
    "    max_failures: int\n",
    "        Number of failures to TuneError\n",
    "    timeout: int\n",
    "        Number of seconds to run the experiment\n",
    "    checkpoint_num_to_keep: int\n",
    "        Number of checkpoints to keep\n",
    "    checkpoint_freq: int\n",
    "        Checkpoint freq wrt training iterations\n",
    "    reuse_actors:bool\n",
    "        Reuse actors for tuning\n",
    "    callbacks:\n",
    "        callbacks integration for ray tune\n",
    "\n",
    "    It has the following methods:\n",
    "    Methods\n",
    "    -------------------------------------\n",
    "        train_tune_model: It takes in the params dictionary and fits in sklearn style to our trainable class\n",
    "        restore_agent: It restores previously errored or stopped trials or experiments\n",
    "        infer_results: It returns the results dataframe and trial informations\n",
    "        get_test_agent: It returns the testing agent for inference\n",
    "\n",
    "    Example\n",
    "    ---------------------------------------\n",
    "    def sample_ppo_params():\n",
    "        return {\n",
    "            \"entropy_coeff\": tune.loguniform(0.00000001, 0.1),\n",
    "            \"lr\": tune.loguniform(5e-5, 0.001),\n",
    "            \"sgd_minibatch_size\": tune.choice([ 32, 64, 128, 256, 512]),\n",
    "            \"lambda\": tune.choice([0.1,0.3,0.5,0.7,0.9,1.0]),\n",
    "        }\n",
    "    drl_agent = DRLlibv2(\n",
    "        trainable=\"PPO\",\n",
    "        train_env=env(train_env_config),\n",
    "        train_env_name=\"StockTrading_train\",\n",
    "        framework=\"torch\",\n",
    "        log_level=\"DEBUG\",\n",
    "        run_name = 'test',\n",
    "        local_dir = \"test\",\n",
    "        params = sample_ppo_params(),\n",
    "        num_samples = 16,\n",
    "        timeout=3600\n",
    "        checkpoint_freq=5\n",
    "    )\n",
    "    #Tune or train the model\n",
    "    res = drl_agent.train_tune_model()\n",
    "\n",
    "    #Get the tune results\n",
    "    results_df, best_result = drl_agent.infer_results()\n",
    "\n",
    "    #Get the best testing agent\n",
    "    test_agent = drl_agent.get_test_agent(test_env_instance,'StockTrading_testenv')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        trainable: Union[str, Any],\n",
    "        params: dict,\n",
    "        train_env=None,\n",
    "        train_env_name: str='',\n",
    "        run_name: str = \"tune_run\",\n",
    "        framework: str = \"torch\",\n",
    "        local_dir: str = \"tune_results\",\n",
    "        num_workers: int = 16,\n",
    "        num_samples: int = 0,\n",
    "        scheduler=None,\n",
    "        log_level: str = \"WARN\",\n",
    "        num_gpus: Union[float, int] = 1,\n",
    "        num_cpus: Union[float, int] = 20,\n",
    "        dataframe_save: str = \"tune.csv\",\n",
    "        metric: str = \"episode_reward_mean\",\n",
    "        mode: Union[str, List[str]] = \"max\",\n",
    "        max_failures: int = 0,\n",
    "        timeout: int = 3600,\n",
    "        checkpoint_num_to_keep: Union[None, int] = None,\n",
    "        checkpoint_freq: int = 0,\n",
    "        reuse_actors: bool = True,\n",
    "        callbacks:Optional[List[\"Callback\"]]=None\n",
    "    ):\n",
    "\n",
    "        if train_env is not None:register_env(train_env_name, lambda config: train_env)\n",
    "\n",
    "        self.params = params\n",
    "        self.params[\"framework\"] = framework\n",
    "        self.params[\"log_level\"] = log_level\n",
    "        self.params[\"num_gpus\"] = num_gpus\n",
    "        self.params[\"num_workers\"] = num_workers\n",
    "        self.params[\"env\"] = train_env_name\n",
    "\n",
    "        self.run_name = run_name\n",
    "        self.local_dir = local_dir\n",
    "        self.scheduler = scheduler\n",
    "        self.num_samples = num_samples\n",
    "        self.trainable = trainable\n",
    "        if isinstance(self.trainable, str):\n",
    "            self.trainable.upper()\n",
    "        self.num_cpus = num_cpus\n",
    "        self.num_gpus = num_gpus\n",
    "        self.dataframe_save = dataframe_save\n",
    "        self.metric = metric\n",
    "        self.mode = mode\n",
    "        self.max_failures = max_failures\n",
    "        self.timeout = timeout\n",
    "        self.checkpoint_freq = checkpoint_freq\n",
    "        self.checkpoint_num_to_keep = checkpoint_num_to_keep\n",
    "        self.reuse_actors = reuse_actors\n",
    "        self.callbacks = callbacks\n",
    "\n",
    "    def train_tune_model(self):\n",
    "        \"\"\"\n",
    "        Tuning and training the model\n",
    "        Returns the results object\n",
    "        \"\"\"\n",
    "        ray.init(\n",
    "            num_cpus=self.num_cpus, num_gpus=self.num_gpus, ignore_reinit_error=True\n",
    "        )\n",
    "\n",
    "        tuner = tune.Tuner(\n",
    "            self.trainable,\n",
    "            param_space=self.params,\n",
    "            tune_config=TuneConfig(\n",
    "                num_samples=self.num_samples,\n",
    "                metric=self.metric,\n",
    "                mode=self.mode,\n",
    "                time_budget_s=self.timeout,\n",
    "                reuse_actors=self.reuse_actors,\n",
    "            ),\n",
    "            run_config=RunConfig(\n",
    "                name=self.run_name,\n",
    "                local_dir=self.local_dir,\n",
    "                callbacks=self.callbacks,\n",
    "                failure_config=FailureConfig(\n",
    "                    max_failures=self.max_failures, fail_fast=False\n",
    "                ),\n",
    "                checkpoint_config=CheckpointConfig(\n",
    "                    num_to_keep=self.checkpoint_num_to_keep,\n",
    "                    checkpoint_score_attribute=self.metric,\n",
    "                    checkpoint_score_order=self.mode,\n",
    "                    checkpoint_frequency=self.checkpoint_freq,\n",
    "                    checkpoint_at_end=True,\n",
    "                ),\n",
    "                verbose=3,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.results = tuner.fit()\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "    def infer_results(self, to_dataframe: str = None, mode: str = \"a\"):\n",
    "        \"\"\"\n",
    "        Get tune results in a dataframe and best results object\n",
    "        \"\"\"\n",
    "        results_df = self.results.get_dataframe()\n",
    "\n",
    "        if to_dataframe is None:\n",
    "            to_dataframe = self.dataframe_save\n",
    "\n",
    "        results_df.to_csv(to_dataframe, mode=mode)\n",
    "\n",
    "        best_result = self.results.get_best_result()\n",
    "\n",
    "        return results_df, best_result\n",
    "\n",
    "    def restore_agent(\n",
    "        self,\n",
    "        checkpoint_path: str = \"\",\n",
    "        resume_unfinished: bool = True,\n",
    "        resume_errored: bool = False,\n",
    "        restart_errored: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Restore errored or stopped trials\n",
    "        \"\"\"\n",
    "        if checkpoint_path == \"\":\n",
    "            checkpoint_path = self.results.get_best_result().checkpoint._local_path\n",
    "\n",
    "        restored_agent = tune.Tuner.restore(\n",
    "            checkpoint_path,\n",
    "            restart_errored=restart_errored,\n",
    "            resume_unfinished=resume_unfinished,\n",
    "            resume_errored=resume_errored,\n",
    "        )\n",
    "        print(restored_agent)\n",
    "        self.results = restored_agent.fit()\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def get_test_agent(self, test_env, test_env_name: str, checkpoint=None):\n",
    "        \"\"\"\n",
    "        Get test agent\n",
    "        \"\"\"\n",
    "        if test_env is not None:register_env(test_env_name, lambda config: test_env)\n",
    "\n",
    "        if checkpoint is None:\n",
    "            checkpoint = self.results.get_best_result().checkpoint\n",
    "\n",
    "        testing_agent = Algorithm.from_checkpoint(checkpoint)\n",
    "\n",
    "        return testing_agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = GP2_TICKER\n",
    "action_dim = len(GP2_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(INDICATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 109"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
